{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Ordering\n",
    "\n",
    "When prompts contain events, conversations, or facts with temporal relationships, presenting them in chronological order helps language models better understand causality and event sequences. Models process narratives more effectively when events flow from past to present, making it easier to identify cause-and-effect relationships and understand \"what happened when and why.\"\n",
    "\n",
    "Temporal ordering is simply organizing time-dependent information chronologically. In practice, this usually means sorting events or messages by their timestamp. This straightforward technique dramatically improves model comprehension of event sequences and causal relationships.\n",
    "\n",
    "This notebook demonstrates the practical application of temporal ordering in AI agent context engineering, showing the problem, the simple solution, and real-world use cases.\n",
    "\n",
    "**Common use cases:**\n",
    "1. **Conversation history**: Order messages chronologically for natural flow.\n",
    "2. **Event logs**: Sort incidents/alerts by time for root cause analysis.\n",
    "3. **Document versions**: Present history from oldest to newest.\n",
    "4. **Multi-turn interactions**: Maintain temporal context across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(), temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating the problem: chronological vs non-ncronological\n",
    "Let's empirically show how chronological ordering affects LLM comprehension. We will present the same incident timeline in two ways - shuffled and chronological - and compare the model's ability to identify root cause and event sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUFFLED ORDER - Model must reconstruct timeline:\n",
      "============================================================\n",
      "Based on the incident timeline provided, the root cause and sequence of events can be summarized as follows:\n",
      "\n",
      "### Root Cause:\n",
      "The root cause of the incident appears to be a corrupted database index, which led to high CPU usage and ultimately caused the primary database to become unresponsive. This corruption likely resulted from the load generated by the marketing campaign email sent to 500,000 us ...\n",
      "\n",
      "\n",
      "CHRONOLOGICAL ORDER - Clear causal chain:\n",
      "============================================================\n",
      "The root cause of the incident appears to be related to the high database CPU usage, which likely led to the database becoming unresponsive. Hereâ€™s the sequence of events that contributed to the incident:\n",
      "\n",
      "1. **13:25** - A marketing campaign email was sent to 500,000 users. This likely increased traffic to the application as users began to engage with the content.\n",
      "   \n",
      "2. **13:30** - An automated b ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same events, different orders\n",
    "shuffled_timeline = \"\"\"\n",
    "Incident Timeline:\n",
    "\n",
    "15:45 - System fully restored\n",
    "14:20 - Database failover initiated\n",
    "15:15 - Engineers identified corrupted index\n",
    "13:45 - Customer complaints about slow page loads\n",
    "14:00 - Monitoring alerts: High database CPU (95%)\n",
    "14:45 - Primary database became unresponsive\n",
    "15:30 - Index rebuild completed\n",
    "13:30 - Automated backup process started\n",
    "13:25 - Marketing campaign email sent to 500,000 users\n",
    "14:30 - Application servers returning 500 errors\n",
    "\n",
    "Question: What was the root cause and sequence of events?\n",
    "\"\"\"\n",
    "\n",
    "chronological_timeline = \"\"\"\n",
    "Incident Timeline (Chronological):\n",
    "\n",
    "13:25 - Marketing campaign email sent to 500,000 users\n",
    "13:30 - Automated backup process started\n",
    "13:45 - Customer complaints about slow page loads\n",
    "14:00 - Monitoring alerts: High database CPU (95%)\n",
    "14:20 - Database failover initiated\n",
    "14:30 - Application servers returning 500 errors\n",
    "14:45 - Primary database became unresponsive\n",
    "15:15 - Engineers identified corrupted index\n",
    "15:30 - Index rebuild completed\n",
    "15:45 - System fully restored\n",
    "\n",
    "Question: What was the root cause and sequence of events?\n",
    "\"\"\"\n",
    "\n",
    "# Test both versions\n",
    "print(\"SHUFFLED ORDER - Model must reconstruct timeline:\")\n",
    "print(\"=\" * 60)\n",
    "response_shuffled = llm.invoke([HumanMessage(content=shuffled_timeline)])\n",
    "print(response_shuffled.content[:400], \"...\\n\")\n",
    "\n",
    "print(\"\\nCHRONOLOGICAL ORDER - Clear causal chain:\")\n",
    "print(\"=\" * 60)\n",
    "response_chrono = llm.invoke([HumanMessage(content=chronological_timeline)])\n",
    "print(response_chrono.content[:400], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact:\n",
    "- Chronological ordering improves root cause identification.\n",
    "- Reduces event sequence errors.\n",
    "- Makes causal relationships immediately apparent.\n",
    "\n",
    "## The solution: timestamp sorting\n",
    "In production AI agent systems, temporal ordering is almost always just sorting by timestamp. Just organize our context chronologically before sending to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events in chronological order:\n",
      "============================================================\n",
      "13:25 - Marketing email sent\n",
      "14:00 - High CPU alert\n",
      "14:20 - Failover initiated\n",
      "15:45 - System restored\n"
     ]
    }
   ],
   "source": [
    "# This is what temporal ordering looks like in practice\n",
    "from typing import List, Dict\n",
    "\n",
    "def order_events_chronologically(events: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Sort events by timestamp. This is 95% of temporal ordering in production.\n",
    "    \n",
    "    Args:\n",
    "        events: List of event dictionaries with 'timestamp' and 'description' keys\n",
    "        \n",
    "    Returns:\n",
    "        Events sorted chronologically (oldest to newest)\n",
    "    \"\"\"\n",
    "    return sorted(events, key=lambda e: e['timestamp'])\n",
    "\n",
    "\n",
    "# Example usage with real events\n",
    "events = [\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 15, 45), \"description\": \"System restored\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 14, 20), \"description\": \"Failover initiated\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 13, 25), \"description\": \"Marketing email sent\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 14, 00), \"description\": \"High CPU alert\"},\n",
    "]\n",
    "\n",
    "# Sort chronologically\n",
    "sorted_events = order_events_chronologically(events)\n",
    "\n",
    "print(\"Events in chronological order:\")\n",
    "print(\"=\" * 60)\n",
    "for event in sorted_events:\n",
    "    time_str = event['timestamp'].strftime(\"%H:%M\")\n",
    "    print(f\"{time_str} - {event['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world use case: conversation history in AI agents\n",
    "The most common application of temporal ordering in AI agents is organizing conversation history. When building an agent with memory, we want to present messages in chronological order so the LLM can follow the conversation flow naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chronologically ordered conversation context:\n",
      "============================================================\n",
      "Previous conversation:\n",
      "\n",
      "[10:15] user: Can you start the deployment?\n",
      "[10:16] assistant: Deployment started. I'll monitor progress.\n",
      "[10:30] user: What's the status of the deployment?\n",
      "[10:32] assistant: The deployment completed at 10:15 AM\n",
      "[10:35] user: Were there any errors?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulating a conversation history from a database or memory store\n",
    "# In practice, these might come from different retrieval queries and be out of order\n",
    "conversation_history = [\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 10, 30), \"role\": \"user\", \"content\": \"What's the status of the deployment?\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 10, 32), \"role\": \"assistant\", \"content\": \"The deployment completed at 10:15 AM\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 10, 15), \"role\": \"user\", \"content\": \"Can you start the deployment?\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 10, 16), \"role\": \"assistant\", \"content\": \"Deployment started. I'll monitor progress.\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 10, 35), \"role\": \"user\", \"content\": \"Were there any errors?\"},\n",
    "]\n",
    "\n",
    "# Sort chronologically (oldest to newest is standard for conversation history)\n",
    "ordered_conversation = sorted(conversation_history, key=lambda msg: msg['timestamp'])\n",
    "\n",
    "# Build context for LLM\n",
    "context = \"Previous conversation:\\n\\n\"\n",
    "for msg in ordered_conversation:\n",
    "    time_str = msg['timestamp'].strftime(\"%H:%M\")\n",
    "    context += f\"[{time_str}] {msg['role']}: {msg['content']}\\n\"\n",
    "\n",
    "print(\"Chronologically ordered conversation context:\")\n",
    "print(\"=\" * 60)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this matters:\n",
    "- LLM can follow the natural conversation flow.\n",
    "- User's questions make sense in context (\"Were there any errors?\" comes after deployment).\n",
    "- Agent can provide consistent, contextually appropriate responses.\n",
    "\n",
    "## Handling edge cases: missing timestamps\n",
    "Sometimes events don't have explicit timestamps. Here is a practical approach for handling this common scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events ordered with fallback handling:\n",
      "============================================================\n",
      "14:15 - Load spike began\n",
      "14:30 - Error detected\n",
      "[No timestamp] - User reported slow performance\n",
      "[No timestamp] - Team investigating\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def order_events_with_fallback(events: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Sort events by timestamp, using insertion order for events without timestamps.\n",
    "    This handles the common case where some events lack explicit time markers.\n",
    "    \n",
    "    Args:\n",
    "        events: List of event dicts with optional 'timestamp' key\n",
    "        \n",
    "    Returns:\n",
    "        Events sorted chronologically where possible\n",
    "    \"\"\"\n",
    "    # Separate events with and without timestamps\n",
    "    timestamped = [e for e in events if 'timestamp' in e and e['timestamp'] is not None]\n",
    "    no_timestamp = [e for e in events if 'timestamp' not in e or e['timestamp'] is None]\n",
    "    \n",
    "    # Sort timestamped events\n",
    "    timestamped.sort(key=lambda e: e['timestamp'])\n",
    "    \n",
    "    # Append non-timestamped events at the end (maintain insertion order)\n",
    "    return timestamped + no_timestamp\n",
    "\n",
    "\n",
    "# Example with mixed timestamp availability\n",
    "mixed_events = [\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 14, 30), \"description\": \"Error detected\"},\n",
    "    {\"description\": \"User reported slow performance\"},  # No timestamp\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 14, 15), \"description\": \"Load spike began\"},\n",
    "    {\"description\": \"Team investigating\"},  # No timestamp\n",
    "]\n",
    "\n",
    "ordered = order_events_with_fallback(mixed_events)\n",
    "\n",
    "print(\"Events ordered with fallback handling:\")\n",
    "print(\"=\" * 60)\n",
    "for event in ordered:\n",
    "    if 'timestamp' in event and event['timestamp']:\n",
    "        time_str = event['timestamp'].strftime(\"%H:%M\")\n",
    "        print(f\"{time_str} - {event['description']}\")\n",
    "    else:\n",
    "        print(f\"[No timestamp] - {event['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "- Events with timestamps come first, in chronological order.\n",
    "- Events without timestamps maintain insertion order.\n",
    "- Simple, predictable behavior for edge cases.\n",
    "\n",
    "## Choosing order direction: oldest-first vs newest-first\n",
    "Depending on our use case, we might want oldest-first (default) or newest-first (recency bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLDEST-FIRST (default for narratives):\n",
      "============================================================\n",
      "10:00 - System started\n",
      "10:30 - First user login\n",
      "11:00 - Peak traffic reached\n",
      "\n",
      "NEWEST-FIRST (for recency bias):\n",
      "============================================================\n",
      "11:00 - Peak traffic reached\n",
      "10:30 - First user login\n",
      "10:00 - System started\n"
     ]
    }
   ],
   "source": [
    "def order_events(events: List[Dict], newest_first: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Sort events chronologically with configurable direction.\n",
    "    \n",
    "    Args:\n",
    "        events: List of event dictionaries with 'timestamp' key\n",
    "        newest_first: If True, sort newest to oldest. If False, oldest to newest.\n",
    "        \n",
    "    Returns:\n",
    "        Chronologically sorted events\n",
    "    \"\"\"\n",
    "    return sorted(events, key=lambda e: e['timestamp'], reverse=newest_first)\n",
    "\n",
    "\n",
    "# Same events, different ordering strategies\n",
    "events = [\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 10, 00), \"description\": \"System started\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 10, 30), \"description\": \"First user login\"},\n",
    "    {\"timestamp\": datetime(2024, 1, 15, 11, 00), \"description\": \"Peak traffic reached\"},\n",
    "]\n",
    "\n",
    "print(\"OLDEST-FIRST (default for narratives):\")\n",
    "print(\"=\" * 60)\n",
    "for event in order_events(events, newest_first=False):\n",
    "    print(f\"{event['timestamp'].strftime('%H:%M')} - {event['description']}\")\n",
    "\n",
    "print(\"\\nNEWEST-FIRST (for recency bias):\")\n",
    "print(\"=\" * 60)\n",
    "for event in order_events(events, newest_first=True):\n",
    "    print(f\"{event['timestamp'].strftime('%H:%M')} - {event['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use each:\n",
    "- Oldest-first: Conversations, incident timelines, narratives.\n",
    "- Newest-first: Recent activity summaries, recency-weighted context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
