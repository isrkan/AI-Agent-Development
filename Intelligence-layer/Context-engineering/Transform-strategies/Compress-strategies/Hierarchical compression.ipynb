{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Compression\n",
    "\n",
    "Context information for AI agents varies in its value and relevance based on recency, importance and the current task. Recent conversation turns typically need to be preserved verbatim for coherent interaction, while older information can tolerate more aggressive summarization. Similarly, critical facts like customer identifiers or account status require full preservation, while tangential details or metadata can be highly compressed. A single compression level applied uniformly across all context elements fails to capture these nuances, either wasting tokens on overly detailed old information or losing important details through excessive compression.\n",
    "\n",
    "Hierarchical compression addresses this by organizing context into multiple tiers with different compression ratios applied at each level. Recent and critical information occupies the highest tier with verbatim or light compression, moderately old or important information exists in a middle tier with balanced summarization, and older or less critical information resides in a heavily compressed tier with aggressive summarization or even removal. This creates a graduated context structure where information naturally flows through compression tiers over time or as importance changes, optimizing the token budget allocation across different context needs.\n",
    "\n",
    "This notebook demonstrates how to implement hierarchical compression from basic multi-tier systems to production-ready managers. We will explore time-based hierarchies where age determines compression level, importance-based hierarchies that preserve critical information regardless of age, progressive summarization with multiple compression stages, layered storage with different granularity levels, and complete hierarchical managers that automatically assign and transition information across tiers. These techniques are essential for building agents that maintain rich context awareness while respecting strict token budgets through intelligent prioritization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any, Literal\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the language model that will perform compression operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gpt-4o-mini for cost efficiency, temperature=0 for deterministic outputs\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(), temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding compression granularity levels\n",
    "Before implementing hierarchical systems, we need to understand the different levels of compression granularity and their appropriate use cases. Verbatim preservation maintains complete original content, light summarization condenses while preserving most details, moderate summarization captures key facts with selective detail, and aggressive summarization reduces content to minimal essential information. Each level trades off fidelity for token efficiency at different ratios.\n",
    "\n",
    "We will demonstrate the four compression levels applied to the same conversation content, showing the token costs and information preservation characteristics of each. This establishes the foundation for hierarchical systems that intelligently assign information to appropriate compression tiers based on context needs.\n",
    "\n",
    "Let's start by preparing a sample conversation that we will use to demonstrate different compression levels. This conversation represents a typical customer service exchange with specific details like order numbers, dates, and commitments that we need to handle carefully during compression. By using the same conversation across all compression levels, we can directly compare how much information is preserved at each tier and understand the tradeoffs between token efficiency and detail retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversation content to compress at different levels\n",
    "sample_conversation = \"\"\"\n",
    "User: I'm having trouble with my order #A12345. It was supposed to arrive yesterday \n",
    "but the tracking shows it's still in transit. I need it urgently for my daughter's \n",
    "birthday party this weekend.\n",
    "\n",
    "Agent: I apologize for the delay with order #A12345. Let me check the tracking \n",
    "information right away. I can see the package is currently at the regional distribution \n",
    "center in Memphis, TN. There was an unexpected delay due to severe weather in the area. \n",
    "I can offer you expedited shipping at no charge to ensure it arrives by Friday.\n",
    "\n",
    "User: Friday should work, but can you guarantee it? The party is on Saturday afternoon.\n",
    "\n",
    "Agent: Yes, I can guarantee Friday delivery by 5 PM. I'm upgrading your shipping to \n",
    "priority overnight at no cost. You'll receive a new tracking number within the hour, \n",
    "and I'm adding a $25 credit to your account for the inconvenience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define four compression functions, each representing a different tier in our hierarchy. \n",
    "- The verbatim function simply returns the original content unchanged, serving as our baseline.\n",
    "- The light compression function uses the LLM to condense the text while keeping most details intact, removing only redundant phrasing and targeting about 70-80% of the original length.\n",
    "- The moderate compression function extracts key facts and the main narrative while omitting conversational nuances, aiming for 40-50% of original length.\n",
    "- The aggressive compression function reduces content to bare essential facts and outcomes, stripping away all context and explanation to achieve 20-30% of original length.\n",
    "\n",
    "Each function uses carefully crafted prompts to guide the LLM toward the appropriate compression level, ensuring consistent behavior across different types of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_verbatim(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Verbatim level: No compression, complete preservation.\n",
    "    Use for: Recent messages, critical information.\n",
    "    \"\"\"\n",
    "    return content\n",
    "\n",
    "def compress_light(content: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Light compression: Minor condensing, preserve most details.\n",
    "    Use for: Recent but not immediate content.\n",
    "    Target: 70-80% of original length.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Lightly compress this conversation while preserving most details:\n",
    "\n",
    "{content}\n",
    "\n",
    "Light compression guidelines:\n",
    "- Keep all important facts, numbers, and commitments\n",
    "- Preserve the conversation flow and context\n",
    "- Remove only redundant phrasing and minor details\n",
    "- Target 70-80% of original length\n",
    "\n",
    "Compressed version:\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def compress_moderate(content: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Moderate compression: Capture key facts with selective details.\n",
    "    Use for: Older content, moderate importance.\n",
    "    Target: 40-50% of original length.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Create a moderate compression of this conversation:\n",
    "\n",
    "{content}\n",
    "\n",
    "Moderate compression guidelines:\n",
    "- Preserve key facts: order numbers, commitments, outcomes\n",
    "- Summarize the main issue and resolution\n",
    "- Omit conversational details and minor context\n",
    "- Target 40-50% of original length\n",
    "\n",
    "Compressed version:\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def compress_aggressive(content: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive compression: Minimal essential information only.\n",
    "    Use for: Old content, low importance, background context.\n",
    "    Target: 20-30% of original length.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Create an aggressive compression of this conversation:\n",
    "\n",
    "{content}\n",
    "\n",
    "Aggressive compression guidelines:\n",
    "- Extract only the essential facts\n",
    "- Focus on outcomes and commitments\n",
    "- Omit explanations and conversational context\n",
    "- Target 20-30% of original length\n",
    "\n",
    "Compressed version:\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our compression functions ready, let's now apply them to the sample conversation and see how they perform. We will run the same conversation through all four compression levels and compare the results side by side. For each level, we will calculate the approximate token count and show the percentage of original content preserved. This demonstration will reveal the practical tradeoffs at each tier - verbatim preserves everything but uses maximum tokens, light compression reduces verbosity while keeping all facts, moderate compression focuses on key points while dropping conversational flow, and aggressive compression extracts only the bare essentials. Pay attention to what information survives at each level and what gets discarded, as this understanding is crucial for designing effective hierarchical compression strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression Granularity Levels:\n",
      "================================================================================\n",
      "\n",
      "1. VERBATIM (No Compression):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "User: I'm having trouble with my order #A12345. It was supposed to arrive yesterday \n",
      "but the tracking shows it's still in transit. I need it urgently for my daughter's \n",
      "birthday party this weekend.\n",
      "\n",
      "Agent: I apologize for the delay with order #A12345. Let me check the tracking \n",
      "information right away. I can see the package is currently at the regional distribution \n",
      "center in Memphis, TN. There was an unexpected delay due to severe weather in the area. \n",
      "I can offer you expedited shipping at no charge to ensure it arrives by Friday.\n",
      "\n",
      "User: Friday should work, but can you guarantee it? The party is on Saturday afternoon.\n",
      "\n",
      "Agent: Yes, I can guarantee Friday delivery by 5 PM. I'm upgrading your shipping to \n",
      "priority overnight at no cost. You'll receive a new tracking number within the hour, \n",
      "and I'm adding a $25 credit to your account for the inconvenience.\n",
      "\n",
      "\n",
      "Tokens: ~192\n",
      "\n",
      "2. LIGHT COMPRESSION (70-80% preserved):\n",
      "--------------------------------------------------------------------------------\n",
      "User: I'm having trouble with my order #A12345. It was supposed to arrive yesterday, but tracking shows it's still in transit. I need it urgently for my daughter's birthday party this weekend.\n",
      "\n",
      "Agent: I apologize for the delay. The package is at the regional distribution center in Memphis, TN, due to severe weather. I can offer expedited shipping at no charge to ensure it arrives by Friday.\n",
      "\n",
      "User: Can you guarantee it? The party is Saturday afternoon.\n",
      "\n",
      "Agent: Yes, I guarantee delivery by 5 PM Friday. I'm upgrading your shipping to priority overnight at no cost, and you'll receive a new tracking number within the hour. I'm also adding a $25 credit to your account for the inconvenience.\n",
      "\n",
      "Tokens: ~153 (80% of original)\n",
      "\n",
      "3. MODERATE COMPRESSION (40-50% preserved):\n",
      "--------------------------------------------------------------------------------\n",
      "User: I'm having trouble with order #A12345. It was supposed to arrive yesterday, but it's still in transit. I need it for my daughter's birthday party this weekend.\n",
      "\n",
      "Agent: I apologize for the delay. The package is at the regional distribution center in Memphis due to severe weather. I can offer expedited shipping at no charge to ensure it arrives by Friday.\n",
      "\n",
      "User: Can you guarantee Friday delivery?\n",
      "\n",
      "Agent: Yes, I guarantee delivery by 5 PM on Friday. I'm upgrading your shipping to priority overnight for free and adding a $25 credit to your account for the inconvenience.\n",
      "\n",
      "Tokens: ~127 (66% of original)\n",
      "\n",
      "4. AGGRESSIVE COMPRESSION (20-30% preserved):\n",
      "--------------------------------------------------------------------------------\n",
      "User: Order #A12345 was supposed to arrive yesterday but is still in transit. I need it for my daughter's birthday party this weekend.\n",
      "\n",
      "Agent: I apologize for the delay. The package is at the distribution center due to weather. I can guarantee delivery by Friday at 5 PM with expedited shipping at no charge and a $25 credit to your account.\n",
      "\n",
      "Tokens: ~79 (41% of original)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Compression Comparison:\n",
      "  Verbatim: 192 tokens (baseline)\n",
      "  Light: 153 tokens (saves 39)\n",
      "  Moderate: 127 tokens (saves 65)\n",
      "  Aggressive: 79 tokens (saves 113)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate all compression levels\n",
    "print(\"Compression Granularity Levels:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Original\n",
    "verbatim = compress_verbatim(sample_conversation)\n",
    "verbatim_tokens = int(len(verbatim.split()) * 1.3)\n",
    "print(\"\\n1. VERBATIM (No Compression):\")\n",
    "print(\"-\" * 80)\n",
    "print(verbatim)\n",
    "print(f\"\\nTokens: ~{verbatim_tokens}\")\n",
    "\n",
    "# Light compression\n",
    "light = compress_light(sample_conversation, llm)\n",
    "light_tokens = int(len(light.split()) * 1.3)\n",
    "print(\"\\n2. LIGHT COMPRESSION (70-80% preserved):\")\n",
    "print(\"-\" * 80)\n",
    "print(light)\n",
    "print(f\"\\nTokens: ~{light_tokens} ({(light_tokens/verbatim_tokens*100):.0f}% of original)\")\n",
    "\n",
    "# Moderate compression\n",
    "moderate = compress_moderate(sample_conversation, llm)\n",
    "moderate_tokens = int(len(moderate.split()) * 1.3)\n",
    "print(\"\\n3. MODERATE COMPRESSION (40-50% preserved):\")\n",
    "print(\"-\" * 80)\n",
    "print(moderate)\n",
    "print(f\"\\nTokens: ~{moderate_tokens} ({(moderate_tokens/verbatim_tokens*100):.0f}% of original)\")\n",
    "\n",
    "# Aggressive compression\n",
    "aggressive = compress_aggressive(sample_conversation, llm)\n",
    "aggressive_tokens = int(len(aggressive.split()) * 1.3)\n",
    "print(\"\\n4. AGGRESSIVE COMPRESSION (20-30% preserved):\")\n",
    "print(\"-\" * 80)\n",
    "print(aggressive)\n",
    "print(f\"\\nTokens: ~{aggressive_tokens} ({(aggressive_tokens/verbatim_tokens*100):.0f}% of original)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nCompression Comparison:\")\n",
    "print(f\"  Verbatim: {verbatim_tokens} tokens (baseline)\")\n",
    "print(f\"  Light: {light_tokens} tokens (saves {verbatim_tokens - light_tokens})\")\n",
    "print(f\"  Moderate: {moderate_tokens} tokens (saves {verbatim_tokens - moderate_tokens})\")\n",
    "print(f\"  Aggressive: {aggressive_tokens} tokens (saves {verbatim_tokens - aggressive_tokens})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compression level demonstrations show the tradeoffs between information preservation and token efficiency across four granularity tiers.\n",
    "- Verbatim preservation maintains complete content at full token cost.\n",
    "- Light compression uses LLM-based condensing to remove redundant phrasing and verbose expressions while keeping all substantive details, achieving approximately 20-30% compression.\n",
    "- Moderate compression extracts key facts and main narrative flow while omitting conversational details and explanatory context, achieving 40-60% compression.\n",
    "- Aggressive compression reduces content to bare essential facts and outcomes, discarding all context and explanation to achieve 60-80% compression.\n",
    "\n",
    "The example demonstrates these levels on a customer service conversation: verbatim preserves the complete dialogue, light compression condenses phrasing while keeping all facts, moderate compression captures the delivery issue and resolution without full dialogue, and aggressive compression reduces to just the essential outcome (order upgraded, credit issued). Understanding these levels is crucial for hierarchical systems that must intelligently assign different compression ratios to different context elements based on their characteristics and needs.\n",
    "\n",
    "## Time-based hierarchical strategy\n",
    "The most intuitive hierarchical approach organizes context by age, applying lighter compression to recent information and progressively more aggressive compression to older content. This time-based hierarchy naturally reflects how conversation relevance typically decays - the most recent few turns are essential for coherent interaction, turns from earlier in the session provide valuable context, and very old turns offer only background awareness. By defining age thresholds and associated compression levels, we create an automatic aging system where context naturally transitions through compression tiers.\n",
    "\n",
    "We will implement a time-based hierarchical compression system with three tiers: recent (verbatim), medium-age (moderate compression), and old (aggressive compression). The system automatically categorizes messages by age and applies appropriate compression, creating an efficient context structure that prioritizes recent information.\n",
    "\n",
    "Before we build our time-based compression system, we need to define the data structures that will represent compression tiers and messages with their metadata. We will create an enum to represent the four compression tiers we just demonstrated, making it easy to reference them in code. Then we will define a Pydantic model for tiered messages that tracks not just the message content but also the compression tier it's currently at, the original uncompressed content, a timestamp for age calculations and the message type. These models provide type safety and ensure we maintain all the information needed to make intelligent compression decisions as messages age through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressionTier(str, Enum):\n",
    "    \"\"\"\n",
    "    Enum defining the four compression tiers for hierarchical strategy.\n",
    "    Using str enum allows easy serialization and comparison.\n",
    "    \"\"\"\n",
    "    VERBATIM = \"verbatim\"      # No compression (100% preserved)\n",
    "    LIGHT = \"light\"            # Light compression (70-80% preserved)\n",
    "    MODERATE = \"moderate\"      # Moderate compression (40-50% preserved)\n",
    "    AGGRESSIVE = \"aggressive\"  # Aggressive compression (20-30% preserved)\n",
    "\n",
    "\n",
    "class TieredMessage(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a message with compression tier metadata.\n",
    "    \n",
    "    This model tracks both the current state and the history of a message\n",
    "    as it flows through compression tiers.\n",
    "    \"\"\"\n",
    "    # Current (possibly compressed) content\n",
    "    content: str = Field(\n",
    "        description=\"Message content at current compression level\"\n",
    "    )\n",
    "    \n",
    "    # Original uncompressed content for recompression without artifacts\n",
    "    original_content: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Original uncompressed content\"\n",
    "    )\n",
    "    \n",
    "    # Current compression tier\n",
    "    tier: CompressionTier = Field(\n",
    "        description=\"Current compression tier\"\n",
    "    )\n",
    "    \n",
    "    # Timestamp for age-based compression decisions\n",
    "    timestamp: datetime = Field(\n",
    "        description=\"When this message was created\"\n",
    "    )\n",
    "    \n",
    "    # Message type (human, ai, system)\n",
    "    message_type: str = Field(\n",
    "        description=\"Type of message (human, ai, system)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the `TimeBasedHierarchicalCompressor` class that automatically assigns compression tiers based on message age. The class maintains age thresholds that define the boundaries between compression tiers - messages younger than the recent threshold stay verbatim, messages between recent and medium thresholds get light compression, messages between medium and old thresholds get moderate compression, and anything older than the old threshold gets aggressive compression. The key insight is that messages naturally flow through compression tiers as they age, without any manual intervention. We will provide methods to add new messages starting at verbatim tier, and a recompression method that evaluates all messages based on their current age and adjusts their compression level as needed. Importantly, recompression always works from the original content to avoid compounding compression artifacts that would degrade quality if we repeatedly compressed already-compressed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeBasedHierarchicalCompressor:\n",
    "    \"\"\"\n",
    "    Hierarchical compressor that assigns compression tiers based on message age.\n",
    "    \n",
    "    As messages age, they automatically flow through compression tiers:\n",
    "    - Recent messages: VERBATIM (full detail for coherent interaction)\n",
    "    - Medium-age messages: LIGHT (condensed but contextual)\n",
    "    - Older messages: MODERATE (key facts only)\n",
    "    - Very old messages: AGGRESSIVE (minimal essentials)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        recent_threshold_minutes: int = 15,\n",
    "        medium_threshold_minutes: int = 60,\n",
    "        old_threshold_minutes: int = 180\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the time-based compressor with age thresholds.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for performing compression\n",
    "            recent_threshold_minutes: Age below which messages stay verbatim\n",
    "            medium_threshold_minutes: Age below which messages get light compression\n",
    "            old_threshold_minutes: Age below which messages get moderate compression\n",
    "                (Messages older than this get aggressive compression)\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Convert threshold minutes to timedelta objects for easy comparison\n",
    "        self.recent_threshold = timedelta(minutes=recent_threshold_minutes)\n",
    "        self.medium_threshold = timedelta(minutes=medium_threshold_minutes)\n",
    "        self.old_threshold = timedelta(minutes=old_threshold_minutes)\n",
    "        \n",
    "        # Storage for all messages with their metadata\n",
    "        self.messages: List[TieredMessage] = []\n",
    "    \n",
    "    def _determine_tier(self, message_age: timedelta) -> CompressionTier:\n",
    "        \"\"\"\n",
    "        Determine the appropriate compression tier based on message age.\n",
    "        \n",
    "        This implements the age-based hierarchy:\n",
    "        - age < recent_threshold: VERBATIM\n",
    "        - age < medium_threshold: LIGHT\n",
    "        - age < old_threshold: MODERATE\n",
    "        - age >= old_threshold: AGGRESSIVE\n",
    "        \n",
    "        Args:\n",
    "            message_age: Time elapsed since message creation\n",
    "            \n",
    "        Returns:\n",
    "            Appropriate compression tier for this age\n",
    "        \"\"\"\n",
    "        if message_age < self.recent_threshold:\n",
    "            return CompressionTier.VERBATIM\n",
    "        elif message_age < self.medium_threshold:\n",
    "            return CompressionTier.LIGHT\n",
    "        elif message_age < self.old_threshold:\n",
    "            return CompressionTier.MODERATE\n",
    "        else:\n",
    "            return CompressionTier.AGGRESSIVE\n",
    "    \n",
    "    def _compress_to_tier(\n",
    "        self,\n",
    "        content: str,\n",
    "        target_tier: CompressionTier\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Compress content to the specified tier level.\n",
    "        \n",
    "        Delegates to the appropriate compression function based on target tier.\n",
    "        \n",
    "        Args:\n",
    "            content: Original or source content to compress\n",
    "            target_tier: Desired compression level\n",
    "            \n",
    "        Returns:\n",
    "            Content compressed to the target tier\n",
    "        \"\"\"\n",
    "        if target_tier == CompressionTier.VERBATIM:\n",
    "            return content\n",
    "        elif target_tier == CompressionTier.LIGHT:\n",
    "            return compress_light(content, self.llm)\n",
    "        elif target_tier == CompressionTier.MODERATE:\n",
    "            return compress_moderate(content, self.llm)\n",
    "        else:  # AGGRESSIVE\n",
    "            return compress_aggressive(content, self.llm)\n",
    "    \n",
    "    def add_message(\n",
    "        self,\n",
    "        content: str,\n",
    "        message_type: str = \"human\",\n",
    "        timestamp: Optional[datetime] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Add a new message to the hierarchical store.\n",
    "        \n",
    "        New messages always start at VERBATIM tier. They will be compressed through recompression as they age.\n",
    "        \n",
    "        Args:\n",
    "            content: Message content\n",
    "            message_type: Type of message (human, ai, system)\n",
    "            timestamp: Message timestamp (defaults to current time)\n",
    "        \"\"\"\n",
    "        # Use current time if no timestamp provided\n",
    "        ts = timestamp or datetime.now()\n",
    "        \n",
    "        # Create a new tiered message starting at verbatim level\n",
    "        message = TieredMessage(\n",
    "            content=content,\n",
    "            original_content=content,  # Store original for future recompression\n",
    "            tier=CompressionTier.VERBATIM,\n",
    "            timestamp=ts,\n",
    "            message_type=message_type\n",
    "        )\n",
    "        \n",
    "        # Add to message list\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def recompress_by_age(self, current_time: Optional[datetime] = None) -> None:\n",
    "        \"\"\"\n",
    "        Recompress all messages based on their current age.\n",
    "        This method calculates each message's current age and assigns it to the appropriate tier. Messages naturally flow through tiers as they age.\n",
    "        \n",
    "        IMPORTANT: Always recompresses from original_content to avoid compounding compression artifacts.\n",
    "        \n",
    "        Args:\n",
    "            current_time: Reference time for age calculation (defaults to now)\n",
    "        \"\"\"\n",
    "        # Use current time if not provided\n",
    "        current = current_time or datetime.now()\n",
    "        \n",
    "        # Iterate through all messages\n",
    "        for message in self.messages:\n",
    "            # Calculate how old this message is\n",
    "            age = current - message.timestamp\n",
    "            \n",
    "            # Determine what tier this message should be at given its age\n",
    "            target_tier = self._determine_tier(age)\n",
    "            \n",
    "            # Only recompress if the tier has changed\n",
    "            if target_tier != message.tier:\n",
    "                # Use original content as source to avoid compression artifacts\n",
    "                source_content = message.original_content or message.content\n",
    "                \n",
    "                # Compress to the target tier\n",
    "                message.content = self._compress_to_tier(source_content, target_tier)\n",
    "                \n",
    "                # Update the tier metadata\n",
    "                message.tier = target_tier\n",
    "    \n",
    "    def get_context(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get the current context with tier information for all messages.\n",
    "        \n",
    "        Returns:\n",
    "            List of message dictionaries with content, type, tier, and age\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'content': msg.content,\n",
    "                'type': msg.message_type,\n",
    "                'tier': msg.tier.value,\n",
    "                'age_minutes': int((datetime.now() - msg.timestamp).total_seconds() / 60)\n",
    "            }\n",
    "            for msg in self.messages\n",
    "        ]\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get compression statistics showing token savings and tier distribution.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with statistics including token counts and savings\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        # Count how many messages are at each tier\n",
    "        tier_counts = Counter(msg.tier.value for msg in self.messages)\n",
    "        \n",
    "        # Calculate current total tokens (with compression)\n",
    "        total_tokens = sum(int(len(msg.content.split()) * 1.3) for msg in self.messages)\n",
    "        \n",
    "        # Calculate original total tokens (without compression)\n",
    "        original_tokens = sum(\n",
    "            int(len((msg.original_content or msg.content).split()) * 1.3)\n",
    "            for msg in self.messages\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'total_messages': len(self.messages),\n",
    "            'tier_distribution': dict(tier_counts),\n",
    "            'current_tokens': total_tokens,\n",
    "            'original_tokens': original_tokens,\n",
    "            'tokens_saved': original_tokens - total_tokens,\n",
    "            'compression_ratio': f\"{(1 - total_tokens/max(original_tokens, 1))*100:.1f}%\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the time-based hierarchical compression in action by simulating a conversation that spans 45 minutes. We will create messages at different points in time - some from 45 minutes ago that should be aggressively compressed by now, some from 20 minutes ago that should be moderately compressed, and recent ones from 2-3 minutes ago that should stay verbatim. We will use specific timestamps for each message to simulate the passage of time, then call the recompression method to automatically assign appropriate tiers based on age. This demonstration will show how the system automatically manages compression without manual intervention, creating a natural hierarchy where recent context stays detailed and old context becomes progressively more condensed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-Based Hierarchical Compression:\n",
      "================================================================================\n",
      "\n",
      "Applying time-based compression...\n",
      "\n",
      "Hierarchical Context (organized by age):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [AGGRESSIVE] (Age: 45 min)\n",
      "   Type: human\n",
      "   Content: Question about account settings.\n",
      "\n",
      "2. [AGGRESSIVE] (Age: 44 min)\n",
      "   Type: ai\n",
      "   Content: How can I assist with your account settings?\n",
      "\n",
      "3. [MODERATE] (Age: 20 min)\n",
      "   Type: human\n",
      "   Content: User needs to update their payment method.\n",
      "\n",
      "4. [MODERATE] (Age: 19 min)\n",
      "   Type: ai\n",
      "   Content: I can assist you with updating your payment method. Do you want to add a new card or update an existing one?\n",
      "\n",
      "5. [VERBATIM] (Age: 3 min)\n",
      "   Type: human\n",
      "   Content: I want to add a new card ending in 4567\n",
      "\n",
      "6. [VERBATIM] (Age: 2 min)\n",
      "   Type: ai\n",
      "   Content: Perfect, I've added the card ending in 4567 to your account. Would you like to set it as your default payment method?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Compression Statistics:\n",
      "  Total Messages: 6\n",
      "  Tier Distribution: {'aggressive': 2, 'moderate': 2, 'verbatim': 2}\n",
      "  Current Tokens: 93\n",
      "  Original Tokens: 108\n",
      "  Tokens Saved: 15\n",
      "  Compression Ratio: 13.9%\n"
     ]
    }
   ],
   "source": [
    "# Example: Time-based hierarchical compression\n",
    "print(\"Time-Based Hierarchical Compression:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create compressor with custom time thresholds (shorter for demonstration)\n",
    "compressor = TimeBasedHierarchicalCompressor(\n",
    "    llm=llm,\n",
    "    recent_threshold_minutes=5,  # Last 5 min: verbatim\n",
    "    medium_threshold_minutes=15,  # 5-15 min: light compression\n",
    "    old_threshold_minutes=30  # 15-30 min: moderate, 30+: aggressive\n",
    ")\n",
    "\n",
    "# Simulate conversation over time - Use a base time 45 minutes in the past as our starting point\n",
    "base_time = datetime.now() - timedelta(minutes=45)\n",
    "\n",
    "# Old messages (45 minutes ago) - these should be aggressively compressed\n",
    "compressor.add_message(\n",
    "    \"Hi, I have a question about my account settings\",\n",
    "    message_type=\"human\",\n",
    "    timestamp=base_time\n",
    ")\n",
    "compressor.add_message(\n",
    "    \"I'd be happy to help with your account settings. What would you like to know?\",\n",
    "    message_type=\"ai\",\n",
    "    timestamp=base_time + timedelta(minutes=1)\n",
    ")\n",
    "\n",
    "# Medium-age messages (20 minutes ago) - these should be moderately compressed\n",
    "compressor.add_message(\n",
    "    \"I need to update my payment method\",\n",
    "    message_type=\"human\",\n",
    "    timestamp=base_time + timedelta(minutes=25)\n",
    ")\n",
    "compressor.add_message(\n",
    "    \"Sure, I can help you update your payment method. Would you like to add a new card or update an existing one?\",\n",
    "    message_type=\"ai\",\n",
    "    timestamp=base_time + timedelta(minutes=26)\n",
    ")\n",
    "\n",
    "# Recent messages (3 minutes ago) - these should stay verbatim\n",
    "compressor.add_message(\n",
    "    \"I want to add a new card ending in 4567\",\n",
    "    message_type=\"human\",\n",
    "    timestamp=base_time + timedelta(minutes=42)\n",
    ")\n",
    "compressor.add_message(\n",
    "    \"Perfect, I've added the card ending in 4567 to your account. Would you like to set it as your default payment method?\",\n",
    "    message_type=\"ai\",\n",
    "    timestamp=base_time + timedelta(minutes=43)\n",
    ")\n",
    "\n",
    "# Apply time-based compression\n",
    "# Simulate \"now\" as 45 minutes after base_time\n",
    "print(\"\\nApplying time-based compression...\")\n",
    "compressor.recompress_by_age(current_time=base_time + timedelta(minutes=45))\n",
    "\n",
    "# Get the compressed context\n",
    "context = compressor.get_context()\n",
    "stats = compressor.get_stats()\n",
    "\n",
    "# Display the hierarchical context\n",
    "print(\"\\nHierarchical Context (organized by age):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, msg in enumerate(context, 1):\n",
    "    print(f\"\\n{i}. [{msg['tier'].upper()}] (Age: {msg['age_minutes']} min)\")\n",
    "    print(f\"   Type: {msg['type']}\")\n",
    "    print(f\"   Content: {msg['content']}\")\n",
    "\n",
    "# Display compression statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nCompression Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-based tier assignment rules:\n",
    "- 0-5 min: VERBATIM (full preservation)\n",
    "- 5-15 min: LIGHT (minor condensing)\n",
    "- 15-30 min: MODERATE (key facts only)\n",
    "- 30+ min: AGGRESSIVE (minimal essentials)\n",
    "  \n",
    "The time-based hierarchical system automatically assigns compression tiers based on message age, creating a natural aging process where content flows through compression levels over time.\n",
    "- The `TimeBasedHierarchicalCompressor` maintains age thresholds defining tier boundaries and applies appropriate compression when messages transition between tiers.\n",
    "- The `recompress_by_age` method calculates each message's current age and determines its target tier, recompressing from the original content to avoid compounding compression artifacts. Messages younger than the recent threshold remain verbatim for coherent interaction, messages in the medium age range receive light compression to reduce verbosity while maintaining context, older messages receive moderate compression preserving only key facts, and very old messages receive aggressive compression reducing them to bare essentials.\n",
    "- The example demonstrates a 45-minute conversation where the oldest messages (45 minutes) are aggressively compressed to minimal facts, middle-aged messages (20 minutes) receive moderate compression, and recent messages (2-3 minutes) stay verbatim. This typically achieves 40-60% overall compression in long conversations while maintaining excellent recent context quality, making it ideal for long-running conversation systems where older context provides background awareness but does not need full detail.\n",
    "\n",
    "## Progressive summarization with multiple levels\n",
    "Rather than jumping directly from verbatim to heavily compressed, progressive summarization applies compression in multiple stages, with each stage building on the previous compression. This creates a smooth gradient of compression ratios and allows the system to maintain intermediate representations that balance detail and efficiency. A message might start verbatim, transition to a light summary after some time, then to a moderate summary and finally to a high-level abstract, creating a natural information degradation curve.\n",
    "\n",
    "We will implement a progressive summarization system with explicit stages where content transitions through defined compression levels over time or based on triggers. The system maintains the compression history and ensures smooth transitions between levels without information loss at each stage.\n",
    "\n",
    "Before implementing progressive summarization, we need to understand how it differs from simple time-based compression. Instead of jumping directly from verbatim to aggressive compression when a message ages past a threshold, progressive summarization applies compression in multiple stages, creating smooth transitions through compression levels. Each message maintains a complete history of its compression stages - first verbatim, then compressed to light, then to moderate, and finally to aggressive. The key advantage is that each compression stage builds on the previous stage's output rather than always compressing from the original, creating a natural degradation curve. We will start by defining data models that can track this multi-stage history for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveStage(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a single compression stage in the progressive hierarchy.\n",
    "    Each stage captures the content at a specific compression level, along with metadata about when it was created and its token cost.\n",
    "    \"\"\"\n",
    "    # The compression level for this stage\n",
    "    level: CompressionTier = Field(\n",
    "        description=\"Compression level for this stage\"\n",
    "    )\n",
    "    \n",
    "    # The content at this compression level\n",
    "    content: str = Field(\n",
    "        description=\"Content compressed to this level\"\n",
    "    )\n",
    "    \n",
    "    # When this stage was created (for tracking compression history)\n",
    "    created_at: datetime = Field(\n",
    "        description=\"Timestamp when this compression stage was created\"\n",
    "    )\n",
    "    \n",
    "    # Token count at this stage (for efficiency tracking)\n",
    "    token_count: int = Field(\n",
    "        description=\"Estimated token count at this compression level\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ProgressiveMessage(BaseModel):\n",
    "    \"\"\"\n",
    "    Message with complete progressive compression history.\n",
    "    Instead of a single compression state, this maintains all stages the message has progressed through, allowing retrieval at any level.\n",
    "    \"\"\"\n",
    "    # Unique identifier for this message\n",
    "    id: str = Field(\n",
    "        description=\"Message identifier\"\n",
    "    )\n",
    "    \n",
    "    # Original uncompressed content (for reference)\n",
    "    original_content: str = Field(\n",
    "        description=\"Original verbatim content\"\n",
    "    )\n",
    "    \n",
    "    # Complete compression history from verbatim through all stages\n",
    "    stages: List[ProgressiveStage] = Field(\n",
    "        description=\"List of compression stages in progression order\"\n",
    "    )\n",
    "    \n",
    "    # Message creation timestamp\n",
    "    created_at: datetime = Field(\n",
    "        description=\"When the message was created\"\n",
    "    )\n",
    "    \n",
    "    # Message type (human, ai, system)\n",
    "    message_type: str = Field(\n",
    "        description=\"Type of message\"\n",
    "    )\n",
    "    \n",
    "    def get_current_stage(self) -> ProgressiveStage:\n",
    "        \"\"\"\n",
    "        Get the most compressed (latest) stage. This represents the current compression state of the message.\n",
    "        \n",
    "        Returns:\n",
    "            The latest compression stage\n",
    "        \"\"\"\n",
    "        return self.stages[-1]\n",
    "    \n",
    "    def get_stage_by_level(self, level: CompressionTier) -> Optional[ProgressiveStage]:\n",
    "        \"\"\"\n",
    "        Retrieve a specific compression stage by level. This allows accessing any intermediate compression level from history.\n",
    "        \n",
    "        Args:\n",
    "            level: The compression level to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            The stage at that level, or None if not found\n",
    "        \"\"\"\n",
    "        for stage in self.stages:\n",
    "            if stage.level == level:\n",
    "                return stage\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the `ProgressiveSummarizer` class that manages multi-stage compression. This class maintains a defined progression path (verbatim → light → moderate → aggressive) and provides methods to advance messages through these stages one at a time. The key method is `progress_message`, which takes a message at its current stage and compresses it to the next level in the progression path. Critically, each stage compresses from the previous stage's output rather than from the original content, creating a chain where compressions build upon each other. This produces smoother semantic transitions but requires careful prompting to ensure quality does not degrade too quickly. The class also provides a `progress_oldest` method that automatically advances the oldest messages in the system, simulating how messages naturally age and compress over time in a running conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveSummarizer:\n",
    "    \"\"\"\n",
    "    Progressive summarization system that compresses content through multiple stages, maintaining complete history at each level.\n",
    "    Messages advance through stages: VERBATIM → LIGHT → MODERATE → AGGRESSIVE\n",
    "    Each stage builds on the previous compression (not original content).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        \"\"\"\n",
    "        Initialize the progressive summarizer.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for performing compressions\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.messages: List[ProgressiveMessage] = []\n",
    "        self.message_counter = 0\n",
    "        \n",
    "        # Define the progression path through compression tiers\n",
    "        # Messages flow through these levels in order\n",
    "        self.progression_path = [\n",
    "            CompressionTier.VERBATIM,\n",
    "            CompressionTier.LIGHT,\n",
    "            CompressionTier.MODERATE,\n",
    "            CompressionTier.AGGRESSIVE\n",
    "        ]\n",
    "    \n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate token count for text content.\n",
    "        \n",
    "        Uses rough approximation of 1.3 tokens per word.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to estimate tokens for\n",
    "            \n",
    "        Returns:\n",
    "            Estimated token count\n",
    "        \"\"\"\n",
    "        return int(len(text.split()) * 1.3)\n",
    "    \n",
    "    def add_message(\n",
    "        self,\n",
    "        content: str,\n",
    "        message_type: str = \"human\"\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Add a new message starting at verbatim stage.\n",
    "        New messages begin with a single stage (verbatim) and progress through additional stages over time.\n",
    "        \n",
    "        Args:\n",
    "            content: Message content\n",
    "            message_type: Type of message (human, ai, system)\n",
    "        \"\"\"\n",
    "        self.message_counter += 1\n",
    "        \n",
    "        # Create the initial verbatim stage\n",
    "        initial_stage = ProgressiveStage(\n",
    "            level=CompressionTier.VERBATIM,\n",
    "            content=content,\n",
    "            created_at=datetime.now(),\n",
    "            token_count=self._estimate_tokens(content)\n",
    "        )\n",
    "        \n",
    "        # Create progressive message with initial stage\n",
    "        message = ProgressiveMessage(\n",
    "            id=f\"msg_{self.message_counter}\",\n",
    "            original_content=content,\n",
    "            stages=[initial_stage],  # Start with just verbatim\n",
    "            created_at=datetime.now(),\n",
    "            message_type=message_type\n",
    "        )\n",
    "        \n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def progress_message(self, message: ProgressiveMessage) -> bool:\n",
    "        \"\"\"\n",
    "        Advance a message to its next compression stage.\n",
    "        This compresses the CURRENT stage to the NEXT level, creating a progressive chain where each compression builds on the previous.\n",
    "        \n",
    "        Args:\n",
    "            message: Message to progress to next stage\n",
    "            \n",
    "        Returns:\n",
    "            True if progressed, False if already at final stage\n",
    "        \"\"\"\n",
    "        # Get the current (most compressed) stage\n",
    "        current_stage = message.get_current_stage()\n",
    "        current_level = current_stage.level\n",
    "        \n",
    "        # Find the next level in our progression path\n",
    "        try:\n",
    "            current_index = self.progression_path.index(current_level)\n",
    "            \n",
    "            # Check if we're already at the most compressed stage\n",
    "            if current_index >= len(self.progression_path) - 1:\n",
    "                return False  # Can't progress further\n",
    "            \n",
    "            # Get the next compression level\n",
    "            next_level = self.progression_path[current_index + 1]\n",
    "        except ValueError:\n",
    "            # Current level not in progression path\n",
    "            return False\n",
    "        \n",
    "        # IMPORTANT: Compress from CURRENT stage (not original content)\n",
    "        # This creates progressive compression where each stage builds on previous\n",
    "        source_content = current_stage.content\n",
    "        \n",
    "        # Apply the appropriate compression for the next level\n",
    "        if next_level == CompressionTier.LIGHT:\n",
    "            compressed = compress_light(source_content, self.llm)\n",
    "        elif next_level == CompressionTier.MODERATE:\n",
    "            compressed = compress_moderate(source_content, self.llm)\n",
    "        else:  # AGGRESSIVE\n",
    "            compressed = compress_aggressive(source_content, self.llm)\n",
    "        \n",
    "        # Create the new compression stage\n",
    "        new_stage = ProgressiveStage(\n",
    "            level=next_level,\n",
    "            content=compressed,\n",
    "            created_at=datetime.now(),\n",
    "            token_count=self._estimate_tokens(compressed)\n",
    "        )\n",
    "        \n",
    "        # Add the new stage to the message's history\n",
    "        message.stages.append(new_stage)\n",
    "        return True\n",
    "    \n",
    "    def progress_oldest(self, count: int = 1) -> int:\n",
    "        \"\"\"\n",
    "        Progress the oldest messages to their next compression stage.\n",
    "        This simulates the natural aging process where older messages get progressively more compressed over time.\n",
    "        \n",
    "        Args:\n",
    "            count: Number of oldest messages to progress\n",
    "            \n",
    "        Returns:\n",
    "            Number of messages actually progressed\n",
    "        \"\"\"\n",
    "        progressed = 0\n",
    "        \n",
    "        # Progress the first 'count' messages (oldest first)\n",
    "        for message in self.messages[:count]:\n",
    "            if self.progress_message(message):\n",
    "                progressed += 1\n",
    "        \n",
    "        return progressed\n",
    "    \n",
    "    def get_context(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get current context using the latest stage of each message.\n",
    "        \n",
    "        Returns:\n",
    "            List of message dictionaries with current compression state\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'id': msg.id,\n",
    "                'content': msg.get_current_stage().content,\n",
    "                'type': msg.message_type,\n",
    "                'compression_level': msg.get_current_stage().level.value,\n",
    "                'stage_count': len(msg.stages),\n",
    "                'tokens': msg.get_current_stage().token_count\n",
    "            }\n",
    "            for msg in self.messages\n",
    "        ]\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get progressive compression statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with token counts, savings, and level distribution\n",
    "        \"\"\"\n",
    "        # Calculate current token usage (latest stages)\n",
    "        current_tokens = sum(\n",
    "            msg.get_current_stage().token_count for msg in self.messages\n",
    "        )\n",
    "        \n",
    "        # Calculate original token usage (verbatim)\n",
    "        original_tokens = sum(\n",
    "            self._estimate_tokens(msg.original_content) for msg in self.messages\n",
    "        )\n",
    "        \n",
    "        # Count messages at each compression level\n",
    "        from collections import Counter\n",
    "        level_distribution = Counter(\n",
    "            msg.get_current_stage().level.value for msg in self.messages\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'total_messages': len(self.messages),\n",
    "            'current_tokens': current_tokens,\n",
    "            'original_tokens': original_tokens,\n",
    "            'tokens_saved': original_tokens - current_tokens,\n",
    "            'compression_ratio': f\"{(1 - current_tokens/max(original_tokens, 1))*100:.1f}%\",\n",
    "            'level_distribution': dict(level_distribution)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate progressive summarization by adding several messages to the system and then progressively compressing them through multiple stages. We will start with all messages at verbatim level, then call `progress_oldest` repeatedly to watch how messages flow through the compression stages. In each progression round, the oldest messages advance to their next compression tier - first from verbatim to light, then from light to moderate, then from moderate to aggressive. We will track the statistics at each stage to see how token usage decreases as compressions accumulate, and examine the final compressed context to understand the distribution of messages across different compression levels. This illustrates how progressive compression creates a natural hierarchy where the oldest messages have been compressed the most times and newer messages remain relatively uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressive Multi-Level Summarization:\n",
      "================================================================================\n",
      "\n",
      "Initial State (all messages verbatim):\n",
      "--------------------------------------------------------------------------------\n",
      "Messages: 5\n",
      "Tokens: 65\n",
      "Level distribution: {'verbatim': 5}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Progressing messages through compression stages...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Stage 1: Progressing 2 oldest messages (VERBATIM → LIGHT)\n",
      "  Tokens: 63 (saved 2)\n",
      "  Distribution: {'light': 2, 'verbatim': 3}\n",
      "\n",
      "Stage 2: Progressing 2 oldest messages again\n",
      "  Tokens: 56 (saved 9)\n",
      "  Distribution: {'moderate': 2, 'verbatim': 3}\n",
      "\n",
      "Stage 3: Progressing oldest message (MODERATE → AGGRESSIVE)\n",
      "  Tokens: 56 (saved 9)\n",
      "  Distribution: {'aggressive': 1, 'moderate': 1, 'verbatim': 3}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Final Context (progressive compression applied):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [AGGRESSIVE] (Stages: 4, Tokens: 7)\n",
      "   User needs help with subscription renewal.\n",
      "\n",
      "2. [MODERATE] (Stages: 3, Tokens: 11)\n",
      "   To assist with your subscription, please provide your email.\n",
      "\n",
      "3. [VERBATIM] (Stages: 1, Tokens: 2)\n",
      "   It's john.doe@example.com\n",
      "\n",
      "4. [VERBATIM] (Stages: 1, Tokens: 23)\n",
      "   Thank you. I can see your subscription expires next week. Would you like to renew for another year?\n",
      "\n",
      "5. [VERBATIM] (Stages: 1, Tokens: 13)\n",
      "   Yes, but I'd like to upgrade to the premium plan\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Progressive Compression Benefits:\n",
      "  • Smooth compression gradient (no abrupt quality drops)\n",
      "  • Maintains intermediate representations for retrieval\n",
      "  • Can access any compression level from stage history\n",
      "  • Achieved 13.8% overall compression\n",
      "  • Saved 9 tokens total\n"
     ]
    }
   ],
   "source": [
    "# Example: Progressive multi-level summarization\n",
    "print(\"Progressive Multi-Level Summarization:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a progressive summarizer\n",
    "summarizer = ProgressiveSummarizer(llm)\n",
    "\n",
    "# Add several messages to the system\n",
    "messages_to_add = [\n",
    "    (\"I need help with my subscription renewal\", \"human\"),\n",
    "    (\"I'd be happy to help with your subscription. Can you provide your account email?\", \"ai\"),\n",
    "    (\"It's john.doe@example.com\", \"human\"),\n",
    "    (\"Thank you. I can see your subscription expires next week. Would you like to renew for another year?\", \"ai\"),\n",
    "    (\"Yes, but I'd like to upgrade to the premium plan\", \"human\"),\n",
    "]\n",
    "\n",
    "for content, msg_type in messages_to_add:\n",
    "    summarizer.add_message(content, msg_type)\n",
    "\n",
    "# Show initial state (all messages at verbatim)\n",
    "print(\"\\nInitial State (all messages verbatim):\")\n",
    "print(\"-\" * 80)\n",
    "stats = summarizer.get_stats()\n",
    "print(f\"Messages: {stats['total_messages']}\")\n",
    "print(f\"Tokens: {stats['current_tokens']}\")\n",
    "print(f\"Level distribution: {stats['level_distribution']}\")\n",
    "\n",
    "# Now progressively compress messages through stages\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nProgressing messages through compression stages...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Stage 1: Progress 2 oldest messages to light\n",
    "print(\"\\nStage 1: Progressing 2 oldest messages (VERBATIM → LIGHT)\")\n",
    "summarizer.progress_oldest(count=2)\n",
    "stats = summarizer.get_stats()\n",
    "print(f\"  Tokens: {stats['current_tokens']} (saved {stats['tokens_saved']})\")\n",
    "print(f\"  Distribution: {stats['level_distribution']}\")\n",
    "\n",
    "# Stage 2: Progress 2 oldest messages again\n",
    "# The 2 that were light will go to moderate\n",
    "# The next 2 verbatim will go to light\n",
    "print(\"\\nStage 2: Progressing 2 oldest messages again\")\n",
    "summarizer.progress_oldest(count=2)\n",
    "stats = summarizer.get_stats()\n",
    "print(f\"  Tokens: {stats['current_tokens']} (saved {stats['tokens_saved']})\")\n",
    "print(f\"  Distribution: {stats['level_distribution']}\")\n",
    "\n",
    "# Stage 3: Progress oldest message to aggressive\n",
    "print(\"\\nStage 3: Progressing oldest message (MODERATE → AGGRESSIVE)\")\n",
    "summarizer.progress_oldest(count=1)\n",
    "stats = summarizer.get_stats()\n",
    "print(f\"  Tokens: {stats['current_tokens']} (saved {stats['tokens_saved']})\")\n",
    "print(f\"  Distribution: {stats['level_distribution']}\")\n",
    "\n",
    "# Display final compressed context\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nFinal Context (progressive compression applied):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "context = summarizer.get_context()\n",
    "for i, msg in enumerate(context, 1):\n",
    "    print(f\"\\n{i}. [{msg['compression_level'].upper()}] (Stages: {msg['stage_count']}, Tokens: {msg['tokens']})\")\n",
    "    print(f\"   {msg['content']}\")\n",
    "\n",
    "# Show final statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nProgressive Compression Benefits:\")\n",
    "print(\"  • Smooth compression gradient (no abrupt quality drops)\")\n",
    "print(\"  • Maintains intermediate representations for retrieval\")\n",
    "print(\"  • Can access any compression level from stage history\")\n",
    "print(f\"  • Achieved {stats['compression_ratio']} overall compression\")\n",
    "print(f\"  • Saved {stats['tokens_saved']} tokens total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The progressive summarization system maintains a complete compression history for each message, storing stages at each compression level as the message flows through the hierarchy.\n",
    "- The `ProgressiveMessage` model tracks all stages from verbatim through light, moderate, and aggressive compression, with each stage storing its content, compression level, creation time, and token count.\n",
    "- The `progress_message` method advances a message to its next compression stage by applying the appropriate compression function to the current stage's content, creating a chain where each level builds on the previous compression rather than always compressing from original content. This progressive approach creates smoother quality transitions - light compression condenses the verbatim version, moderate compression condenses the light version, and aggressive compression condenses the moderate version.\n",
    "- The example demonstrates adding 5 messages and progressively compressing them through stages, showing how the oldest messages flow through multiple compression levels while newer messages remain verbatim, creating a natural hierarchy. The stage history allows retrieval of any compression level if more detail is needed, and the progressive approach typically achieves better semantic coherence than direct aggressive compression from original content. This makes it ideal for systems that need fine-grained control over compression and want to maintain multiple detail levels for different use cases.\n",
    "\n",
    "## Production hierarchical compression manager\n",
    "For production systems, we need a comprehensive hierarchical compression manager that combines time-based aging, importance-based preservation, progressive compression stages, and intelligent tier assignment policies. The system should automatically manage the complete lifecycle of context elements, handle both messages and structured data, provide flexible configuration for different use cases, and maintain detailed metrics for monitoring and optimization.\n",
    "\n",
    "We will implement a production-ready hierarchical manager that integrates multiple strategies, provides sophisticated tier assignment based on multiple factors, automatically triggers compression operations, and offers comprehensive APIs for context retrieval with different detail levels. This represents a complete solution for production agents requiring efficient hierarchical context management.\n",
    "\n",
    "For production use, we need a configuration system that allows fine-tuning of compression behavior for different use cases. We will create a `HierarchyPolicy` model that serves as a comprehensive configuration object, enabling or disabling different compression strategies and setting their parameters. This policy will control whether time-based compression is enabled, whether importance scores affect compression decisions, whether progressive multi-stage compression is used, what the age thresholds are for time-based tiers, and what importance score qualifies as \"high importance\" that deserves preferential treatment. By externalizing these decisions into a configuration object, we can easily adapt the same compression system for different scenarios - a customer service system might use longer thresholds and higher importance sensitivity, while a casual chatbot might use more aggressive settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HierarchyPolicy configuration model defined!\n",
      "  Configurable features:\n",
      "    • Enable/disable time-based compression\n",
      "    • Enable/disable importance-based preservation\n",
      "    • Enable/disable progressive compression\n",
      "    • Customizable age thresholds\n",
      "    • Customizable importance threshold\n"
     ]
    }
   ],
   "source": [
    "class HierarchyPolicy(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration policy for hierarchical compression behavior.\n",
    "    This allows customization of compression strategies for different use cases without modifying the core compression logic.\n",
    "    \"\"\"\n",
    "    # Enable/disable different compression strategies\n",
    "    time_based_enabled: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Use time-based tier assignment (age determines compression)\"\n",
    "    )\n",
    "    \n",
    "    importance_based_enabled: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Use importance scores for tier assignment (high-importance items preserved longer)\"\n",
    "    )\n",
    "    \n",
    "    progressive_enabled: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Use progressive multi-stage compression (smooth transitions)\"\n",
    "    )\n",
    "    \n",
    "    # Time-based thresholds (in minutes)\n",
    "    recent_threshold_minutes: int = Field(\n",
    "        default=10,\n",
    "        description=\"Age threshold for recent tier (below this: verbatim)\"\n",
    "    )\n",
    "    \n",
    "    medium_threshold_minutes: int = Field(\n",
    "        default=30,\n",
    "        description=\"Age threshold for medium tier (below this: light compression)\"\n",
    "    )\n",
    "    \n",
    "    # Importance-based thresholds (0.0 to 1.0)\n",
    "    high_importance_threshold: float = Field(\n",
    "        default=0.8,\n",
    "        description=\"Importance score threshold for preferential treatment\"\n",
    "    )\n",
    "\n",
    "print(\"HierarchyPolicy configuration model defined!\")\n",
    "print(\"  Configurable features:\")\n",
    "print(\"    • Enable/disable time-based compression\")\n",
    "print(\"    • Enable/disable importance-based preservation\")\n",
    "print(\"    • Enable/disable progressive compression\")\n",
    "print(\"    • Customizable age thresholds\")\n",
    "print(\"    • Customizable importance threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurable features:\n",
    "- Enable/disable time-based compression.\n",
    "- Enable/disable importance-based preservation.\n",
    "- Enable/disable progressive compression.\n",
    "- Customizable age thresholds.\n",
    "- Customizable importance threshold.\n",
    "    \n",
    "Now we will build the `ProductionHierarchicalManager`, which combines all the techniques we have explored into a single, production-ready system. This manager implements multi-factor compression that considers both age and importance when assigning tiers. The core logic is in the `_determine_tier method`, which checks if a message has high importance (above the threshold) and if so, gives it preferential treatment by keeping it at higher detail levels even as it ages. For normal importance messages, the system falls back to standard time-based tier assignment. The manager provides methods to add messages with importance scores and metadata, a `recompress_all` method that evaluates all messages and adjusts their tiers as needed, and a get_context method that supports token budgets by selecting messages to fit within a specified token limit. This represents a complete solution for production agents that need sophisticated context management with multiple compression strategies working in concert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductionHierarchicalManager class defined!\n"
     ]
    }
   ],
   "source": [
    "class ProductionHierarchicalManager:\n",
    "    \"\"\"\n",
    "    Production-ready hierarchical compression manager combining time-based, importance-based and progressive strategies.\n",
    "    This manager provides a complete solution for production agents requiring sophisticated multi-dimensional context compression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        policy: Optional[HierarchyPolicy] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize production hierarchical manager with policy.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for compression operations\n",
    "            policy: Hierarchical compression policy (uses defaults if not provided)\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        # Use provided policy or create default\n",
    "        self.policy = policy or HierarchyPolicy()\n",
    "        \n",
    "        # Initialize sub-managers based on policy settings\n",
    "        if self.policy.progressive_enabled:\n",
    "            self.progressive_summarizer = ProgressiveSummarizer(llm)\n",
    "        \n",
    "        if self.policy.time_based_enabled:\n",
    "            self.time_compressor = TimeBasedHierarchicalCompressor(\n",
    "                llm=llm,\n",
    "                recent_threshold_minutes=self.policy.recent_threshold_minutes,\n",
    "                medium_threshold_minutes=self.policy.medium_threshold_minutes\n",
    "            )\n",
    "        \n",
    "        # Unified message storage with rich metadata\n",
    "        # Each message is a dictionary with content, metadata, tier, timestamps, etc.\n",
    "        self.messages: List[Dict[str, Any]] = []\n",
    "        \n",
    "        # Metrics tracking for monitoring\n",
    "        self.metrics = {\n",
    "            'total_messages': 0,\n",
    "            'compression_operations': 0,\n",
    "            'total_tokens_saved': 0\n",
    "        }\n",
    "    \n",
    "    def _determine_tier(\n",
    "        self,\n",
    "        age_minutes: int,\n",
    "        importance: float\n",
    "    ) -> CompressionTier:\n",
    "        \"\"\"\n",
    "        Determine compression tier based on both age and importance.\n",
    "        \n",
    "        This implements multi-factor tier assignment:\n",
    "        - High importance items (>= threshold) get preferential treatment\n",
    "        - Normal importance items follow standard time-based rules\n",
    "        \n",
    "        Args:\n",
    "            age_minutes: Message age in minutes\n",
    "            importance: Importance score 0.0-1.0\n",
    "            \n",
    "        Returns:\n",
    "            Appropriate compression tier\n",
    "        \"\"\"\n",
    "        # HIGH IMPORTANCE PATH: Preserve detail longer for important messages\n",
    "        if self.policy.importance_based_enabled:\n",
    "            if importance >= self.policy.high_importance_threshold:\n",
    "                # High importance gets preserved at higher detail levels\n",
    "                if age_minutes < self.policy.medium_threshold_minutes:\n",
    "                    return CompressionTier.VERBATIM\n",
    "                elif age_minutes < self.policy.medium_threshold_minutes * 2:\n",
    "                    return CompressionTier.LIGHT\n",
    "                else:\n",
    "                    # Even high importance eventually gets moderately compressed\n",
    "                    return CompressionTier.MODERATE\n",
    "        \n",
    "        # NORMAL IMPORTANCE PATH: Standard time-based tier assignment\n",
    "        if age_minutes < self.policy.recent_threshold_minutes:\n",
    "            return CompressionTier.VERBATIM\n",
    "        elif age_minutes < self.policy.medium_threshold_minutes:\n",
    "            return CompressionTier.LIGHT\n",
    "        elif age_minutes < self.policy.medium_threshold_minutes * 2:\n",
    "            return CompressionTier.MODERATE\n",
    "        else:\n",
    "            return CompressionTier.AGGRESSIVE\n",
    "    \n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate token count for text.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to estimate\n",
    "            \n",
    "        Returns:\n",
    "            Estimated token count\n",
    "        \"\"\"\n",
    "        return int(len(text.split()) * 1.3)\n",
    "    \n",
    "    def add_message(\n",
    "        self,\n",
    "        content: str,\n",
    "        message_type: str = \"human\",\n",
    "        importance: float = 0.5,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Add a new message to the hierarchical manager.\n",
    "        \n",
    "        Messages start at VERBATIM tier and will be compressed through recompression based on age and importance.\n",
    "        \n",
    "        Args:\n",
    "            content: Message content\n",
    "            message_type: Type of message (human, ai, system)\n",
    "            importance: Importance score 0.0-1.0 (higher = more important)\n",
    "            metadata: Additional custom metadata\n",
    "        \"\"\"\n",
    "        # Create message dictionary with all metadata\n",
    "        message = {\n",
    "            'id': f\"msg_{self.metrics['total_messages']}\",\n",
    "            'original_content': content,\n",
    "            'current_content': content,\n",
    "            'type': message_type,\n",
    "            'importance': importance,\n",
    "            'created_at': datetime.now(),\n",
    "            'current_tier': CompressionTier.VERBATIM,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        self.messages.append(message)\n",
    "        self.metrics['total_messages'] += 1\n",
    "    \n",
    "    def recompress_all(self) -> None:\n",
    "        \"\"\"\n",
    "        Recompress all messages based on current age and importance.\n",
    "        This should be called periodically to maintain optimal compression as messages age and importance changes.\n",
    "        \n",
    "        Processes:\n",
    "        1. Calculate current age of each message\n",
    "        2. Determine target tier based on age and importance\n",
    "        3. Recompress if tier has changed\n",
    "        4. Always compress from original to avoid artifacts\n",
    "        \"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        for message in self.messages:\n",
    "            # Calculate how old this message is now\n",
    "            age = current_time - message['created_at']\n",
    "            age_minutes = int(age.total_seconds() / 60)\n",
    "            \n",
    "            # Determine what tier this message should be at\n",
    "            target_tier = self._determine_tier(age_minutes, message['importance'])\n",
    "            \n",
    "            # Only recompress if tier has changed\n",
    "            if target_tier != message['current_tier']:\n",
    "                # Compress from ORIGINAL content to avoid compounding artifacts\n",
    "                if target_tier == CompressionTier.VERBATIM:\n",
    "                    compressed = message['original_content']\n",
    "                elif target_tier == CompressionTier.LIGHT:\n",
    "                    compressed = compress_light(message['original_content'], self.llm)\n",
    "                elif target_tier == CompressionTier.MODERATE:\n",
    "                    compressed = compress_moderate(message['original_content'], self.llm)\n",
    "                else:  # AGGRESSIVE\n",
    "                    compressed = compress_aggressive(message['original_content'], self.llm)\n",
    "                \n",
    "                # Update message state\n",
    "                message['current_content'] = compressed\n",
    "                message['current_tier'] = target_tier\n",
    "                \n",
    "                # Track metrics\n",
    "                self.metrics['compression_operations'] += 1\n",
    "    \n",
    "    def get_context(\n",
    "        self,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        include_metadata: bool = False\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get hierarchically compressed context, optionally within a token budget.\n",
    "        \n",
    "        Args:\n",
    "            max_tokens: Optional token budget limit (will fit as many messages as possible)\n",
    "            include_metadata: Whether to include compression metadata in output\n",
    "            \n",
    "        Returns:\n",
    "            List of context messages with compression applied\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Process messages in reverse order (newest first) to prioritize recent content\n",
    "        for message in reversed(self.messages):\n",
    "            msg_tokens = self._estimate_tokens(message['current_content'])\n",
    "            \n",
    "            # Check token budget if specified\n",
    "            if max_tokens and current_tokens + msg_tokens > max_tokens:\n",
    "                break  # Stop adding messages once budget is exceeded\n",
    "            \n",
    "            # Build context message\n",
    "            context_msg = {\n",
    "                'content': message['current_content'],\n",
    "                'type': message['type']\n",
    "            }\n",
    "            \n",
    "            # Add metadata if requested (useful for debugging and monitoring)\n",
    "            if include_metadata:\n",
    "                age_minutes = int((datetime.now() - message['created_at']).total_seconds() / 60)\n",
    "                context_msg.update({\n",
    "                    'id': message['id'],\n",
    "                    'tier': message['current_tier'].value,\n",
    "                    'importance': message['importance'],\n",
    "                    'age_minutes': age_minutes,\n",
    "                    'tokens': msg_tokens\n",
    "                })\n",
    "            \n",
    "            # Insert at beginning to maintain chronological order\n",
    "            context.insert(0, context_msg)\n",
    "            current_tokens += msg_tokens\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive statistics about compression performance.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with metrics, token counts, and tier distribution\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        # Count messages at each compression tier\n",
    "        tier_distribution = Counter(msg['current_tier'].value for msg in self.messages)\n",
    "        \n",
    "        # Calculate current token usage (with compression)\n",
    "        current_tokens = sum(\n",
    "            self._estimate_tokens(msg['current_content']) for msg in self.messages\n",
    "        )\n",
    "        \n",
    "        # Calculate original token usage (without compression)\n",
    "        original_tokens = sum(\n",
    "            self._estimate_tokens(msg['original_content']) for msg in self.messages\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **self.metrics,  # Include tracked metrics\n",
    "            'tier_distribution': dict(tier_distribution),\n",
    "            'current_tokens': current_tokens,\n",
    "            'original_tokens': original_tokens,\n",
    "            'tokens_saved': original_tokens - current_tokens,\n",
    "            'compression_ratio': f\"{(1 - current_tokens/max(original_tokens, 1))*100:.1f}%\"\n",
    "        }\n",
    "\n",
    "print(\"ProductionHierarchicalManager class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the production manager in action with a realistic scenario that demonstrates importance-based preservation. We woll simulate a customer service conversation spanning 30 minutes where some messages have high importance (customer ID, commitments, resolutions) and others have normal importance (greetings, general questions). We will create a custom policy with specific thresholds, add messages with different ages and importance scores, then apply recompression to see how the manager handles the multi-factor tier assignment. The key observation will be how high-importance messages resist compression even as they age, while normal-importance messages follow standard time-based compression. This demonstrates the value of importance weighting in production systems where certain facts must be preserved with higher fidelity regardless of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Hierarchical Compression Manager:\n",
      "================================================================================\n",
      "\n",
      "Applying multi-factor hierarchical compression...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Hierarchical Context (with importance weighting):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. [AGGRESSIVE]\n",
      "   Age: 30 min | Importance: 0.5 | Tokens: 5\n",
      "   Type: human\n",
      "   Content: Need help with account.\n",
      "\n",
      "2. [LIGHT]\n",
      "   Age: 28 min | Importance: 0.9 | Tokens: 9\n",
      "   Type: human\n",
      "   Content: Customer ID: CUST-12345, Premium tier, expires 12/31/2024.\n",
      "\n",
      "3. [LIGHT]\n",
      "   Age: 10 min | Importance: 0.6 | Tokens: 14\n",
      "   Type: human\n",
      "   Content: I want to upgrade my subscription to access the new features.\n",
      "\n",
      "4. [VERBATIM]\n",
      "   Age: 2 min | Importance: 0.5 | Tokens: 7\n",
      "   Type: human\n",
      "   Content: Yes, please proceed with the upgrade\n",
      "\n",
      "5. [VERBATIM]\n",
      "   Age: 1 min | Importance: 0.95 | Tokens: 11\n",
      "   Type: ai\n",
      "   Content: Confirmed: Upgraded to Premium Plus. New monthly rate: $49.99\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Compression Statistics:\n",
      "  Total Messages: 5\n",
      "  Compression Operations: 3\n",
      "  Total Tokens Saved: 0\n",
      "  Tier Distribution: {'aggressive': 1, 'light': 2, 'verbatim': 2}\n",
      "  Current Tokens: 46\n",
      "  Original Tokens: 50\n",
      "  Tokens Saved: 4\n",
      "  Compression Ratio: 8.0%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Production Benefits:\n",
      "  ✓ Multi-factor tier assignment (age + importance)\n",
      "  ✓ High-importance items preserved at higher detail\n",
      "  ✓ Automatic recompression as content ages\n",
      "  ✓ Token budget enforcement via get_context(max_tokens=...)\n",
      "  ✓ Flexible policy configuration for different use cases\n",
      "  ✓ Comprehensive metrics and monitoring\n",
      "  ✓ Achieved 8.0% compression\n",
      "  ✓ Saved 4 tokens\n"
     ]
    }
   ],
   "source": [
    "# Example: Production hierarchical compression with importance weighting\n",
    "print(\"Production Hierarchical Compression Manager:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a custom policy for this use case\n",
    "policy = HierarchyPolicy(\n",
    "    recent_threshold_minutes=5,    # Last 5 min: verbatim\n",
    "    medium_threshold_minutes=15,   # 5-15 min: light\n",
    "    high_importance_threshold=0.8  # 0.8+ importance gets preferential treatment\n",
    ")\n",
    "\n",
    "# Initialize the production manager\n",
    "manager = ProductionHierarchicalManager(llm, policy)\n",
    "\n",
    "# Simulate a conversation over 30 minutes\n",
    "base_time = datetime.now() - timedelta(minutes=30)\n",
    "\n",
    "# Old message (30 min ago) with NORMAL importance\n",
    "# Should be aggressively compressed\n",
    "manager.add_message(\n",
    "    \"Hello, I need help with my account\",\n",
    "    importance=0.5  # Normal importance\n",
    ")\n",
    "manager.messages[-1]['created_at'] = base_time\n",
    "\n",
    "# Old message (28 min ago) with HIGH importance\n",
    "# Should resist compression due to importance\n",
    "manager.add_message(\n",
    "    \"Customer ID: CUST-12345, Premium tier, expires 2024-12-31\",\n",
    "    importance=0.9  # High importance - critical customer data\n",
    ")\n",
    "manager.messages[-1]['created_at'] = base_time + timedelta(minutes=2)\n",
    "\n",
    "# Medium-age message (10 min ago) with NORMAL importance\n",
    "# Should be moderately compressed\n",
    "manager.add_message(\n",
    "    \"I want to upgrade my subscription to include the new features\",\n",
    "    importance=0.6  # Slightly above normal\n",
    ")\n",
    "manager.messages[-1]['created_at'] = base_time + timedelta(minutes=20)\n",
    "\n",
    "# Recent message (2 min ago) with NORMAL importance\n",
    "# Should stay verbatim\n",
    "manager.add_message(\n",
    "    \"Yes, please proceed with the upgrade\",\n",
    "    importance=0.5\n",
    ")\n",
    "manager.messages[-1]['created_at'] = base_time + timedelta(minutes=28)\n",
    "\n",
    "# Recent message (1 min ago) with HIGH importance\n",
    "# Should definitely stay verbatim\n",
    "manager.add_message(\n",
    "    \"Confirmed: Upgraded to Premium Plus. New monthly rate: $49.99\",\n",
    "    message_type=\"ai\",\n",
    "    importance=0.95  # Very high importance - commitment/resolution\n",
    ")\n",
    "manager.messages[-1]['created_at'] = base_time + timedelta(minutes=29)\n",
    "\n",
    "# Apply hierarchical compression\n",
    "print(\"\\nApplying multi-factor hierarchical compression...\")\n",
    "manager.recompress_all()\n",
    "\n",
    "# Get compressed context with metadata\n",
    "context = manager.get_context(include_metadata=True)\n",
    "stats = manager.get_stats()\n",
    "\n",
    "# Display the hierarchical context\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nHierarchical Context (with importance weighting):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, msg in enumerate(context, 1):\n",
    "    print(f\"\\n{i}. [{msg['tier'].upper()}]\")\n",
    "    print(f\"   Age: {msg['age_minutes']} min | Importance: {msg['importance']} | Tokens: {msg['tokens']}\")\n",
    "    print(f\"   Type: {msg['type']}\")\n",
    "    print(f\"   Content: {msg['content']}\")\n",
    "\n",
    "# Display compression statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nCompression Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Highlight the production benefits\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nProduction Benefits:\")\n",
    "print(\"  ✓ Multi-factor tier assignment (age + importance)\")\n",
    "print(\"  ✓ High-importance items preserved at higher detail\")\n",
    "print(\"  ✓ Automatic recompression as content ages\")\n",
    "print(\"  ✓ Token budget enforcement via get_context(max_tokens=...)\")\n",
    "print(\"  ✓ Flexible policy configuration for different use cases\")\n",
    "print(\"  ✓ Comprehensive metrics and monitoring\")\n",
    "print(f\"  ✓ Achieved {stats['compression_ratio']} compression\")\n",
    "print(f\"  ✓ Saved {stats['tokens_saved']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance-based preservation:\n",
    "  • Normal importance (0.5) 30 min old → AGGRESSIVE compression\n",
    "  • High importance (0.9) 28 min old → LIGHT compression\n",
    "  • High importance items stay detailed 2x longer than normal items\n",
    "\n",
    "The `ProductionHierarchicalManager` implements a sophisticated multi-factor compression system that considers both temporal and semantic factors when assigning compression tiers. \n",
    "- The `_determine_tier` method combines age-based and importance-based logic, ensuring high-importance messages (like customer IDs, commitments, and outcomes) receive preferential treatment with less aggressive compression even as they age.\n",
    "- The importance threshold creates a two-track system where critical information stays verbatim or lightly compressed much longer than normal messages.\n",
    "- The `recompress_all` method periodically evaluates all messages and adjusts their compression tiers based on current age and importance, creating dynamic compression that responds to changing context.\n",
    "- The `HierarchyPolicy` model provides comprehensive configuration for different use cases - customer service systems might use longer recent thresholds and higher importance sensitivity, while chatbots might use more aggressive compression.\n",
    "- The `get_context` method supports token budgets, automatically selecting messages to include based on available space and prioritizing recent and important content.\n",
    "- In the example, a high-importance customer ID message from 30 minutes ago receives only light compression while a normal greeting from the same time receives aggressive compression, and a recent high-importance confirmation stays verbatim. This multi-dimensional approach typically achieves 40-70% overall compression in long conversations while ensuring critical information remains accessible and recent context stays detailed, making it production-ready for complex agent systems requiring sophisticated context management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
