{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation summarization\n",
    "\n",
    "In long-running conversations with AI agents, the context window quickly fills with message history, leaving less room for relevant information and task instructions. As conversations extend over dozens or hundreds of turns, the agent must maintain awareness of past interactions while staying within token limits. Without proper management, older messages consume valuable context space, forcing the system to either truncate important history or exceed token budgets.\n",
    "\n",
    "Conversation summarization addresses this challenge by periodically condensing older conversation turns into compact summaries that preserve essential information while dramatically reducing token count. Rather than keeping every message verbatim, the system identifies conversation segments that can be compressed, applies LLM-based summarization with explicit preservation instructions, and replaces the original turns with concise summaries. This maintains conversation continuity and context awareness while freeing space for new interactions and relevant information.\n",
    "\n",
    "This notebook demonstrates how to implement conversation summarization from basic approaches to production-ready systems. We will explore simple message window truncation, progressive summarization strategies, selective information preservation, and complete conversation memory managers that balance verbatim recent history with summarized older context. The techniques shown here are essential for building agents that maintain coherent, context-aware interactions across extended conversation sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the language model for summarization and conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gpt-4o-mini for cost-effective summarization\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(), temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `temperature=0.0` to ensure our agent produces consistent summaries.\n",
    "\n",
    "## Basic conversation truncation\n",
    "When conversations grow beyond a certain length, the simplest survival strategy is to just cut off the old parts and keep only what is recent. Think of it like a sliding window that moves forward with each new message, dropping whatever falls outside the frame. This approach is appealingly straightforward - no complex logic, no LLM calls, just straightforward list slicing. But simplicity comes at a cost. When we truncate, we are not selectively forgetting less important details; we are indiscriminately erasing entire chunks of history.\n",
    "\n",
    "This naive truncation strategy serves an important purpose in our learning journey though. It establishes a baseline that helps us understand why more sophisticated approaches are necessary. In production systems where agents need to maintain coherent conversations over dozens or hundreds of turns, remembering that a customer has been promised expedited shipping or that their order number is #12345 is not optional - it is essential for maintaining trust and delivering quality service. We will implement this basic truncation to see exactly how much context gets lost and why that matters, setting the stage for understanding the value of proper summarization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_conversation(messages: List[Any], max_messages: int = 10) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Simple truncation keeping only the most recent messages.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        max_messages: Maximum number of messages to retain\n",
    "        \n",
    "    Returns:\n",
    "        Truncated list of messages\n",
    "    \"\"\"\n",
    "    # Keep only the last N messages to stay within limits\n",
    "    if len(messages) <= max_messages:\n",
    "        return messages\n",
    "    \n",
    "    # Separate system messages from conversation history\n",
    "    system_messages = [msg for msg in messages if isinstance(msg, SystemMessage)]  # System messages contain instructions and should typically be preserved\n",
    "    conversation_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]\n",
    "\n",
    "    # Calculate how many conversation messages we can keep\n",
    "    available_slots = max_messages - len(system_messages)\n",
    "    # Keep system messages and truncate conversation history\n",
    "    truncated_conversation = conversation_messages[-available_slots:]\n",
    "\n",
    "    # Return system messages first, then truncated conversation history\n",
    "    return system_messages + truncated_conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truncation function implements a straightforward sliding window approach by first separating system messages from conversation turns, ensuring that important system instructions are always preserved regardless of truncation limits. It then calculates how many conversation messages can fit in the remaining space after accounting for system messages, using negative list slicing to grab only the most recent turns that fit within the budget. System messages are distinguished using isinstance checks and are always placed at the beginning of the returned list to maintain proper message ordering for LLM consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created conversation with 12 messages\n",
      "Includes 1 system message(s)\n",
      "And 11 conversation turns\n",
      "\n",
      "Truncated Conversation:\n",
      "================================================================================\n",
      "1. SystemMessage: You are a helpful customer service assistant.\n",
      "2. HumanMessage: How much will that cost?\n",
      "3. AIMessage: Expedited shipping is $15, but I can waive the fee.\n",
      "4. HumanMessage: That would be great, thank you!\n",
      "5. AIMessage: Done! Your order will arrive tomorrow by 5 PM.\n",
      "6. HumanMessage: Actually, can I change the delivery address?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original: 12 messages\n",
      "Truncated: 6 messages\n",
      "Lost: 6 messages\n"
     ]
    }
   ],
   "source": [
    "# Create a simulated customer service conversation to demonstrate truncation\n",
    "# This represents a typical support interaction with multiple back-and-forth turns\n",
    "long_conversation = [\n",
    "    SystemMessage(content=\"You are a helpful customer service assistant.\"),\n",
    "    HumanMessage(content=\"I need help with my order #12345\"),\n",
    "    AIMessage(content=\"I'd be happy to help! Let me look up order #12345.\"),\n",
    "    HumanMessage(content=\"It was supposed to arrive yesterday\"),\n",
    "    AIMessage(content=\"I see it's delayed. Let me check the shipping status.\"),\n",
    "    HumanMessage(content=\"I need it urgently for a gift\"),\n",
    "    AIMessage(content=\"I understand the urgency. I can offer expedited shipping.\"),\n",
    "    HumanMessage(content=\"How much will that cost?\"),\n",
    "    AIMessage(content=\"Expedited shipping is $15, but I can waive the fee.\"),\n",
    "    HumanMessage(content=\"That would be great, thank you!\"),\n",
    "    AIMessage(content=\"Done! Your order will arrive tomorrow by 5 PM.\"),\n",
    "    HumanMessage(content=\"Actually, can I change the delivery address?\"),\n",
    "]\n",
    "\n",
    "print(f\"Created conversation with {len(long_conversation)} messages\")\n",
    "print(f\"Includes {sum(1 for msg in long_conversation if isinstance(msg, SystemMessage))} system message(s)\")\n",
    "print(f\"And {sum(1 for msg in long_conversation if not isinstance(msg, SystemMessage))} conversation turns\")\n",
    "\n",
    "# Apply truncation to keep only recent messages\n",
    "truncated = truncate_conversation(long_conversation, max_messages=6)\n",
    "\n",
    "print(\"\\nTruncated Conversation:\")\n",
    "print(\"=\"*80)\n",
    "for i, msg in enumerate(truncated, 1):\n",
    "    msg_type = type(msg).__name__\n",
    "    print(f\"{i}. {msg_type}: {msg.content}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\\nOriginal: {len(long_conversation)} messages\")\n",
    "print(f\"Truncated: {len(truncated)} messages\")\n",
    "print(f\"Lost: {len(long_conversation) - len(truncated)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our demonstration, the 12-message conversation gets truncated to just 6 messages total, which means the agent loses visibility into the first 6 conversation turns. This includes the order number #12345 that was mentioned at the start, the entire context about why the delivery is urgent (it's a gift), and the commitment to expedited shipping with a waived fee. The agent now only sees the tail end of the conversation starting from around the fee discussion, making it impossible to provide contextually aware responses about the order details or honor the commitments made earlier. This dramatic information loss demonstrates why naive truncation is insufficient for production systems that need to maintain conversation continuity and customer trust.\n",
    "\n",
    "## LLM-based summarization\n",
    "Instead of throwing away old messages, what if we could distill them down to their essence? This is where LLM-based summarization transforms our approach from crude amputation to intelligent compression. The language model becomes a skilled editor, reading through conversation turns and extracting what truly matters while letting the conversational fluff fall away. When we ask the LLM to summarize, we are not leaving it to guess what is important - we provide explicit preservation instructions that guide it toward the information that has lasting value across the conversation lifecycle.\n",
    "\n",
    "The key insight here is that not all information ages equally. Order numbers, customer names, specific problems raised, solutions promised, and pending actions all have enduring relevance that transcends the immediate conversational turn. Meanwhile, the social niceties and transitional phrases that make conversations feel natural lose their value once the moment passes. A well-crafted summarization prompt instructs the model to identify and preserve these high-value information categories while condensing the rest into concise narrative form. This creates summaries that are typically 60-80% shorter than the original messages while maintaining all the critical context an agent needs to provide coherent, informed responses. The result is conversation histories that can extend much further within the same context budget, enabling longer, more complex interactions without sacrificing continuity or overwhelming the model with irrelevant historical detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(messages: List[Any], llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Use LLM to create a concise summary of conversation messages.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of messages to summarize\n",
    "        llm: Language model for generating summaries\n",
    "        \n",
    "    Returns:\n",
    "        Concise summary string preserving key information\n",
    "    \"\"\"\n",
    "    # Convert message objects into a readable text format for the LLM - Each line shows the message type (HumanMessage/AIMessage) and content\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{type(msg).__name__}: {msg.content}\" \n",
    "        for msg in messages\n",
    "    ])\n",
    "    \n",
    "    # Create a detailed summarization prompt with explicit preservation instructions\n",
    "    summarization_prompt = f\"\"\"Summarize the following conversation concisely in 1-2 sentences.\n",
    "\n",
    "Preserve these key details:\n",
    "- Order/ticket numbers and IDs\n",
    "- Main problem or request\n",
    "- Solution provided\n",
    "- Any commitments made\n",
    "\n",
    "Conversation:\n",
    "{conversation_text}\n",
    "\n",
    "Write a brief summary (1-2 sentences):\"\"\"\n",
    "    \n",
    "    # Invoke the LLM to generate the summary based on our instructions\n",
    "    response = llm.invoke([HumanMessage(content=summarization_prompt)])\n",
    "\n",
    "    # Return just the text content of the summary\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summarization function transforms message objects into formatted text that the LLM can process, then constructs a carefully designed prompt that explicitly lists the types of information that must survive compression. This prompt engineering is critical - without clear preservation instructions, the LLM might focus on narrative flow while accidentally dropping crucial identifiers or commitments. By specifying exactly what matters (identifiers, problems, solutions, status), we guide the model toward consistent, reliable summarization behavior. The `conversation_text` formatting uses `type(msg).name` to include message type information, helping the LLM understand the conversational structure when generating summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Messages:\n",
      "================================================================================\n",
      "HumanMessage: I need help with my order #12345\n",
      "AIMessage: I'd be happy to help! Let me look up order #12345.\n",
      "HumanMessage: It was supposed to arrive yesterday\n",
      "AIMessage: I see it's delayed. Let me check the shipping status.\n",
      "HumanMessage: I need it urgently for a gift\n",
      "AIMessage: I understand the urgency. I can offer expedited shipping.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Generated Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "The customer inquired about order #12345, which was delayed and was supposed to arrive yesterday, expressing urgency as it was a gift. The AI offered a solution of expedited shipping to address the issue.\n",
      "\n",
      "================================================================================\n",
      "  Original characters: 256\n",
      "  Summary characters: 204\n",
      "  Compression: 20.3%\n",
      "  Estimated token savings: ~13 tokens\n"
     ]
    }
   ],
   "source": [
    "# Example: Summarize the early part of the conversation\n",
    "messages_to_summarize = long_conversation[1:7]  # Skip system message, summarize first 6 turns\n",
    "\n",
    "summary = summarize_messages(messages_to_summarize, llm)\n",
    "\n",
    "print(\"Original Messages:\")\n",
    "print(\"=\"*80)\n",
    "for msg in messages_to_summarize:\n",
    "    print(f\"{type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(summary)\n",
    "\n",
    "# Calculate token savings (approximate)\n",
    "original_chars = sum(len(msg.content) for msg in messages_to_summarize)\n",
    "summary_chars = len(summary)\n",
    "compression_ratio = (1 - summary_chars / original_chars) * 100\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"  Original characters: {original_chars}\")\n",
    "print(f\"  Summary characters: {summary_chars}\")\n",
    "print(f\"  Compression: {compression_ratio:.1f}%\")\n",
    "print(f\"  Estimated token savings: ~{int((original_chars - summary_chars) / 4)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example demonstrates typical compression ratios of 20-60%, and it varies depending on how verbose the original messages were. This reduction in token count allows systems to maintain much longer conversation histories within the same context window.\n",
    "\n",
    "## Progressive summarization strategy\n",
    "In production systems, we do not want to summarize all messages at once or wait until the conversation is too long. Instead, we implement a progressive strategy that periodically summarizes older message segments while keeping recent messages verbatim. This creates a layered conversation history where recent turns remain detailed and immediately accessible, while older turns exist as increasingly compressed summaries. The agent maintains both immediate context and historical awareness without exponential context growth.\n",
    "\n",
    "This progressive approach typically divides conversation history into segments: recent messages kept verbatim, middle-aged messages that get summarized into a rolling summary, and very old history that may be further compressed or stored externally. We will implement a system that automatically triggers summarization when the conversation exceeds thresholds, replacing old message batches with their summaries to maintain a stable context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummarizer:\n",
    "    \"\"\"\n",
    "    Manages conversation history with progressive summarization.\n",
    "    Keeps recent messages verbatim, older messages as summaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        llm: ChatOpenAI,\n",
    "        max_recent_messages: int = 10,\n",
    "        summarize_batch_size: int = 6\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the conversation summarizer.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for summarization\n",
    "            max_recent_messages: Number of recent messages to keep verbatim\n",
    "            summarize_batch_size: Number of messages to include in each summary\n",
    "        \"\"\"\n",
    "        # Store the LLM instance for generating summaries when needed\n",
    "        self.llm = llm\n",
    "        # Configuration parameters for managing the layered memory\n",
    "        self.max_recent_messages = max_recent_messages  # Threshold for triggering summarization\n",
    "        self.summarize_batch_size = summarize_batch_size  # How many old messages to compress at once\n",
    "        \n",
    "        # Storage for different types of conversation content\n",
    "        self.system_messages = []  # System instructions (always preserved)\n",
    "        self.summaries = []  # Summaries of older conversation segments\n",
    "        self.recent_messages = []  # Recent messages kept in full detail\n",
    "    \n",
    "    def add_message(self, message: Any) -> None:\n",
    "        \"\"\"\n",
    "        Add a new message and trigger summarization if needed.\n",
    "        \n",
    "        Args:\n",
    "            message: New message to add to conversation history\n",
    "        \"\"\"\n",
    "        # Separate system messages from conversation messages\n",
    "        if isinstance(message, SystemMessage):\n",
    "            self.system_messages.append(message)  # System messages go into their own preserved list - they never get summarized because they define agent behavior\n",
    "            return\n",
    "        \n",
    "        # Regular conversation messages go into the recent buffer\n",
    "        self.recent_messages.append(message)\n",
    "        \n",
    "        # Check if the recent buffer has exceeded our threshold. If so, trigger automatic summarization to compress old messages\n",
    "        if len(self.recent_messages) > self.max_recent_messages:\n",
    "            self._summarize_old_messages()\n",
    "    \n",
    "    def _summarize_old_messages(self) -> None:\n",
    "        \"\"\"\n",
    "        Internal method to summarize a batch of older messages.\n",
    "        Moves oldest messages from recent_messages to summaries.\n",
    "        \"\"\"\n",
    "        # Extract the oldest batch of messages to summarize - these are messages that have \"aged out\" of the detailed view\n",
    "        messages_to_summarize = self.recent_messages[:self.summarize_batch_size]\n",
    "        \n",
    "        # Generate summary of this batch using our LLM summarization function\n",
    "        summary_text = summarize_messages(messages_to_summarize, self.llm)\n",
    "        \n",
    "        # Create a summary message with metadata for tracking - the additional_kwargs stores useful information about the summary\n",
    "        summary_message = AIMessage(\n",
    "            content=f\"[CONVERSATION SUMMARY]: {summary_text}\",\n",
    "            additional_kwargs={\n",
    "                \"is_summary\": True,  # Flag to identify summary messages\n",
    "                \"summarized_count\": len(messages_to_summarize),  # How many messages were compressed\n",
    "                \"timestamp\": datetime.now().isoformat()  # When summarization occurred\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Add summary to summaries list\n",
    "        self.summaries.append(summary_message)\n",
    "        \n",
    "        # Remove the now-summarized messages from recent messages\n",
    "        self.recent_messages = self.recent_messages[self.summarize_batch_size:]\n",
    "    \n",
    "    def get_messages(self) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Get the complete conversation history: system + summaries + recent.\n",
    "        \n",
    "        Returns:\n",
    "            Combined list of all messages in chronological order\n",
    "        \"\"\"\n",
    "        # Combine all three tiers in chronological order:\n",
    "        # 1. System messages (always first)\n",
    "        # 2. Summaries of old conversations\n",
    "        # 3. Recent detailed messages\n",
    "        return self.system_messages + self.summaries + self.recent_messages\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get statistics about the conversation state.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with message counts and total context size\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"system_messages\": len(self.system_messages),\n",
    "            \"summaries\": len(self.summaries),\n",
    "            \"recent_messages\": len(self.recent_messages),\n",
    "            \"total_messages\": len(self.get_messages())\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `ConversationSummarizer` class initializes with three separate storage lists to implement a layered memory architecture.\n",
    "    - The `system_messages` list holds system instructions that define agent behavior and should never be compressed or removed.\n",
    "    - The `summaries` list accumulates compressed representations of older conversation segments as they age out of recent memory.\n",
    "    - The `recent_messages` list maintains a sliding window of verbatim messages, configured by `max_recent_messages` parameter.\n",
    "    - The `summarize_batch_size` parameter controls how many old messages get compressed together in each summarization operation, typically set to 4-6 messages to create meaningful summaries without losing too much context in a single compression step.\n",
    "- The `add_message method` implements the automatic triggering logic for progressive summarization.\n",
    "    - It first checks if the incoming message is a `SystemMessage` using `isinstance`, routing it to the preserved `system_messages list` if so.\n",
    "    - Regular conversation messages (`HumanMessage` and `AIMessage`) are appended to the `recent_messages` buffer.\n",
    "    - After each addition, the method checks whether the recent buffer exceeds the `max_recent_messages` threshold, automatically calling `_summarize_old_messages` if the limit is breached.\n",
    "  \n",
    "  This creates a self-managing system where conversation history is automatically compressed as it grows, without requiring external intervention or manual triggers.\n",
    "- The `_summarize_old_messages` method implements the core compression logic by extracting the oldest batch of messages using list slicing `[:self.summarize_batch_size]`, generating a summary via the `summarize_messages` function, and wrapping the result in an `AIMessage` with tracking metadata in `additional_kwargs`.\n",
    "    - The metadata includes `is_summary` flag for easy identification, `summarized_count` to track how many messages were compressed, and a `timestamp` for audit trails.\n",
    "  \n",
    "  After creating the summary, the method removes the summarized messages from recent_messages using list slicing `[self.summarize_batch_size:]`, maintaining a stable buffer size.\n",
    "- The `get_messages` method provides a unified view by concatenating all three lists in chronological order.\n",
    "- `get_stats` returns metrics for monitoring the memory state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Statistics:\n",
      "  System messages: 1\n",
      "  Summary segments: 2\n",
      "  Recent verbatim messages: 3\n",
      "  Total context items: 6\n",
      "\n",
      "Original message count: 12\n",
      "Compressed message count: 6\n",
      "Reduction: 6 messages\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Final Conversation Context:\n",
      "1.  SystemMessage: You are a helpful customer service assistant.\n",
      "2. [SUMMARY] AIMessage: [CONVERSATION SUMMARY]: The customer inquired about order #12345, which was supposed to arrive yeste...\n",
      "3. [SUMMARY] AIMessage: [CONVERSATION SUMMARY]: The customer urgently requested expedited shipping for a gift, and the AI of...\n",
      "4.  HumanMessage: That would be great, thank you!\n",
      "5.  AIMessage: Done! Your order will arrive tomorrow by 5 PM.\n",
      "6.  HumanMessage: Actually, can I change the delivery address?\n"
     ]
    }
   ],
   "source": [
    "# Example: Process the long conversation with progressive summarization\n",
    "summarizer = ConversationSummarizer(llm, max_recent_messages=6, summarize_batch_size=4)\n",
    "\n",
    "# Add all messages one by one\n",
    "for msg in long_conversation:\n",
    "    summarizer.add_message(msg)\n",
    "    \n",
    "# Display the final conversation state\n",
    "final_messages = summarizer.get_messages()\n",
    "stats = summarizer.get_stats()\n",
    "\n",
    "print(\"Conversation Statistics:\")\n",
    "print(f\"  System messages: {stats['system_messages']}\")\n",
    "print(f\"  Summary segments: {stats['summaries']}\")\n",
    "print(f\"  Recent verbatim messages: {stats['recent_messages']}\")\n",
    "print(f\"  Total context items: {stats['total_messages']}\")\n",
    "print(f\"\\nOriginal message count: {len(long_conversation)}\")\n",
    "print(f\"Compressed message count: {stats['total_messages']}\")\n",
    "print(f\"Reduction: {len(long_conversation) - stats['total_messages']} messages\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nFinal Conversation Context:\")\n",
    "for i, msg in enumerate(final_messages, 1):\n",
    "    msg_type = type(msg).__name__\n",
    "    is_summary = msg.additional_kwargs.get(\"is_summary\", False) if hasattr(msg, \"additional_kwargs\") else False\n",
    "    prefix = \"[SUMMARY]\" if is_summary else \"\"\n",
    "    content_preview = msg.content[:100] + \"...\" if len(msg.content) > 100 else msg.content\n",
    "    print(f\"{i}. {prefix} {msg_type}: {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our demonstration with a 12-message conversation, the system compressed it to 6 context items: 1 system message, 2 summaries covering the first 8 conversation turns, and 3 recent messages kept verbatim. This reduction from 12 to 6 items (50% compression) maintains complete conversation awareness while stabilizing context size. The key insight is that as conversations grow longer, the context size remains bounded - a 100-message conversation might still compress to just 10-15 context items, making the system scalable to arbitrarily long interactions without context overflow. The summary preserves critical information like order #12345 and the expedited shipping commitment, while the recent messages provide detailed context for the most current exchanges.\n",
    "\n",
    "## Structured summary with preservation rules\n",
    "For production systems handling complex conversations with multiple types of important information, we need more sophisticated summarization that uses structured formats and explicit preservation rules. Rather than generating free-form summaries, we can define schema-based summary structures that ensure specific types of information are always captured. This might include separate fields for customer identifiers, issues raised, resolutions provided, pending actions, and important dates or numbers.\n",
    "\n",
    "Using Pydantic models, we can enforce that summaries contain required information fields, making the summaries more reliable and easier to parse programmatically. This structured approach is particularly valuable when summaries need to be referenced by downstream systems or when specific information types must be preserved for compliance or business logic reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured format for conversation summaries with explicit fields.\n",
    "    Ensures critical information categories are always captured.\n",
    "    \"\"\"\n",
    "    # Required field: Identifiers and references that must be preserved\n",
    "    identifiers: List[str] = Field(\n",
    "        description=\"Important IDs, order numbers, account numbers, etc.\"\n",
    "    )\n",
    "    \n",
    "    # Required field: Customer's main concerns or requests - Captures the \"why\" of the conversation - what brought the customer here\n",
    "    key_issues: List[str] = Field(\n",
    "        description=\"Main problems, questions, or requests raised\"\n",
    "    )\n",
    "    \n",
    "    # Required field: Actions taken or solutions provided - Documents what the agent committed to or accomplished\n",
    "    resolutions: List[str] = Field(\n",
    "        description=\"Solutions provided, actions taken, commitments made\"\n",
    "    )\n",
    "    \n",
    "    # Optional field: Outstanding items that need follow-up - Tracks unresolved issues or future actions\n",
    "    pending_actions: List[str] = Field(\n",
    "        default_factory=list,  # Uses default_factory to return empty list if no pending actions\n",
    "        description=\"Unresolved issues, pending tasks, or future actions\"\n",
    "    )\n",
    "    \n",
    "    # Optional field: Important contextual details - Captures anything else that might be relevant but doesn't fit other categories\n",
    "    context_notes: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Additional context that may be relevant\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pydantic model defines a rigid structure with three required fields (`identifiers`, `key_issues`, `resolutions`) and two optional fields (`pending_actions`, `context_notes`), ensuring that every summary captures the minimum essential information while allowing flexibility for additional context. Each field includes a description that serves double duty: documenting the schema for developers and guiding the LLM on what information belongs in each category. The `identifiers` field uses `List[str]` to capture multiple IDs or reference numbers, while `pending_actions` uses `default_factory=list` to provide an empty list when no pending items exist, avoiding `None` values that require additional handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_summary(\n",
    "    messages: List[Any], \n",
    "    llm: ChatOpenAI\n",
    ") -> ConversationSummary:\n",
    "    \"\"\"\n",
    "    Generate a structured summary using Pydantic model enforcement.\n",
    "    \n",
    "    Args:\n",
    "        messages: Messages to summarize\n",
    "        llm: Language model with structured output support\n",
    "        \n",
    "    Returns:\n",
    "        Structured ConversationSummary object\n",
    "    \"\"\"\n",
    "    # Format conversation for analysis\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{type(msg).__name__}: {msg.content}\" \n",
    "        for msg in messages\n",
    "    ])\n",
    "    \n",
    "    # Create prompt for structured summarization\n",
    "    prompt = f\"\"\"Analyze the following conversation and extract structured information.\n",
    "\n",
    "Conversation:\n",
    "{conversation_text}\n",
    "\n",
    "Extract:\n",
    "- identifiers: Any order numbers, IDs, account numbers, or reference codes\n",
    "- key_issues: Main problems, questions, or requests from the user\n",
    "- resolutions: Solutions provided, actions taken, or commitments made\n",
    "- pending_actions: Any unresolved issues or tasks that need follow-up\n",
    "- context_notes: Other relevant context\n",
    "\"\"\"\n",
    "    \n",
    "    # Use structured output to ensure schema compliance\n",
    "    llm_with_structure = llm.with_structured_output(ConversationSummary)\n",
    "    \n",
    "    # Generate structured summary\n",
    "    summary = llm_with_structure.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_structured_summary` function leverages LangChain's `with_structured_output` method, which constrains the LLM to produce output that exactly matches the Pydantic schema. This is not just validation after the fact - the structured output mechanism actually modifies how the LLM generates its response, ensuring compliance with the schema structure. The prompt provides additional guidance by explaining what each field should contain, helping the LLM understand which pieces of information map to which schema fields. The conversation formatting uses `type(msg).name` to preserve message type information, giving the LLM context about who said what when extracting key information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Conversation Summary:\n",
      "================================================================================\n",
      "\n",
      "Identifiers:\n",
      "  - #12345\n",
      "\n",
      "Key Issues:\n",
      "  - Order was supposed to arrive yesterday\n",
      "  - Need the order urgently for a gift\n",
      "  - Cost of expedited shipping\n",
      "\n",
      "Resolutions:\n",
      "  - Checked the shipping status and confirmed the order is delayed\n",
      "  - Offered expedited shipping and waived the fee\n",
      "\n",
      "Pending Actions:\n",
      "  None\n",
      "\n",
      "Context Notes: The user requires the order urgently for a gift.\n"
     ]
    }
   ],
   "source": [
    "# Example: Create structured summary of conversation segment\n",
    "messages_for_structured_summary = long_conversation[1:9]\n",
    "\n",
    "structured_summary = create_structured_summary(messages_for_structured_summary, llm)\n",
    "\n",
    "print(\"Structured Conversation Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nIdentifiers:\")\n",
    "for identifier in structured_summary.identifiers:\n",
    "    print(f\"  - {identifier}\")\n",
    "\n",
    "print(\"\\nKey Issues:\")\n",
    "for issue in structured_summary.key_issues:\n",
    "    print(f\"  - {issue}\")\n",
    "\n",
    "print(\"\\nResolutions:\")\n",
    "for resolution in structured_summary.resolutions:\n",
    "    print(f\"  - {resolution}\")\n",
    "\n",
    "print(\"\\nPending Actions:\")\n",
    "if structured_summary.pending_actions:\n",
    "    for action in structured_summary.pending_actions:\n",
    "        print(f\"  - {action}\")\n",
    "else:\n",
    "    print(\"  None\")\n",
    "\n",
    "if structured_summary.context_notes:\n",
    "    print(f\"\\nContext Notes: {structured_summary.context_notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply this to our conversation segment, we get a `ConversationSummary` object where we can reliably access `structured_summary.identifiers[0]` to get the order number, iterate through `structured_summary.resolutions` to see what was promised, and programmatically check `structured_summary.pending_actions` to identify follow-up needs. This machine-readable format enables integration with business systems - we could automatically create support tickets from `pending_actions`, log resolutions to a CRM, or validate that all interactions captured required identifiers for compliance. The structured format eliminates ambiguity and parsing complexity, making summaries first-class data objects rather than unstructured text blobs. In our example, the system correctly extracts \"Order #12345\" to `identifiers`, \"Delayed delivery\" to `key_issues`, and \"Expedited shipping with waived fee\" to `resolutions`.\n",
    "\n",
    "## Production conversation memory manager\n",
    "Bringing together all these techniques, we can build a production-ready conversation memory manager that handles the complete lifecycle of conversation history. This system automatically manages context size, applies progressive summarization with structured preservation, tracks token budgets, and provides interfaces for both adding new messages and retrieving optimized context for LLM calls. The manager maintains conversation quality while ensuring the agent never exceeds context limits.\n",
    "\n",
    "This production implementation includes token estimation, automatic summarization triggers, structured summary storage, and efficient context retrieval. It provides a complete abstraction over conversation memory management, allowing application code to simply add messages without worrying about context overflow or information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionConversationManager:\n",
    "    \"\"\"\n",
    "    Production-ready conversation manager with automatic summarization,\n",
    "    token budgeting, and structured preservation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        max_context_tokens: int = 4000,\n",
    "        recent_message_count: int = 10,\n",
    "        summarize_batch_size: int = 6\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize production conversation manager.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for summarization and conversation\n",
    "            max_context_tokens: Maximum tokens to use for conversation history\n",
    "            recent_message_count: Number of recent messages to keep verbatim\n",
    "            summarize_batch_size: Messages per summary batch\n",
    "        \"\"\"\n",
    "        # Store LLM and configuration\n",
    "        self.llm = llm\n",
    "        self.max_context_tokens = max_context_tokens  # Hard token limit\n",
    "        self.recent_message_count = recent_message_count  # Recent message threshold\n",
    "        self.summarize_batch_size = summarize_batch_size  # Batch size for summarization\n",
    "        \n",
    "        # Three-tier conversation storage with structured summaries\n",
    "        self.system_messages: List[Any] = []  # System instructions\n",
    "        self.structured_summaries: List[ConversationSummary] = []  # Pydantic summary objects\n",
    "        self.recent_messages: List[Any] = []  # Recent detailed messages\n",
    "        \n",
    "        # Metrics for monitoring and debugging\n",
    "        self.total_messages_processed = 0  # Lifetime message count\n",
    "        self.summarization_count = 0  # How many summarizations have occurred\n",
    "    \n",
    "    def _estimate_tokens(self, messages: List[Any]) -> int:\n",
    "        \"\"\"\n",
    "        Estimate token count for a list of messages.\n",
    "        Uses rough approximation of 1 token per 4 characters.\n",
    "        \n",
    "        Args:\n",
    "            messages: Messages to estimate\n",
    "            \n",
    "        Returns:\n",
    "            Estimated token count\n",
    "        \"\"\"\n",
    "        # Simple approximation: ~4 chars per token for English text\n",
    "        total_chars = sum(len(msg.content) for msg in messages)  # Count total characters across all message content\n",
    "        return total_chars // 4  # Apply the ~4 chars per token heuristic\n",
    "    \n",
    "    def _check_and_summarize(self) -> None:\n",
    "        \"\"\"\n",
    "        Check if summarization is needed and perform it.\n",
    "        Triggers when recent messages exceed threshold or token budget.\n",
    "        \"\"\"\n",
    "        # First check: Are we under the message count threshold?\n",
    "        if len(self.recent_messages) <= self.recent_message_count:\n",
    "            return  # If yes, no summarization needed yet\n",
    "        \n",
    "        # Second check: Are we within our token budget?\n",
    "        all_messages = self.system_messages + self.recent_messages\n",
    "        current_tokens = self._estimate_tokens(all_messages)\n",
    "        \n",
    "        # If we are within budget and message count, no summarization needed\n",
    "        if current_tokens < self.max_context_tokens * 0.8:  # We check against 80% of max to leave headroom for response generation\n",
    "            return\n",
    "        \n",
    "        # Both thresholds exceeded - time to summarize\n",
    "        # Extract the oldest batch of messages from the recent buffer\n",
    "        messages_to_summarize = self.recent_messages[:self.summarize_batch_size]\n",
    "        \n",
    "        # Generate a structured summary using our Pydantic model\n",
    "        structured_summary = create_structured_summary(messages_to_summarize, self.llm)  # This preserves information in a reliable, machine-readable format\n",
    "        \n",
    "        # Store the structured summary object\n",
    "        self.structured_summaries.append(structured_summary)\n",
    "        \n",
    "        # Remove the now-summarized messages from recent list\n",
    "        self.recent_messages = self.recent_messages[self.summarize_batch_size:]\n",
    "        \n",
    "        # Update metrics for monitoring\n",
    "        self.summarization_count += 1\n",
    "    \n",
    "    def add_message(self, message: Any) -> None:\n",
    "        \"\"\"\n",
    "        Add a new message to the conversation.\n",
    "        Automatically triggers summarization if thresholds are exceeded.\n",
    "        \n",
    "        Args:\n",
    "            message: Message to add (HumanMessage, AIMessage, or SystemMessage)\n",
    "        \"\"\"\n",
    "        # System messages go into their own preserved list\n",
    "        if isinstance(message, SystemMessage):\n",
    "            self.system_messages.append(message)\n",
    "            return\n",
    "        \n",
    "        # Regular messages go into the recent buffer\n",
    "        self.recent_messages.append(message)\n",
    "        self.total_messages_processed += 1\n",
    "        \n",
    "        # Check if we need to trigger summarization - This happens automatically based on thresholds\n",
    "        self._check_and_summarize()\n",
    "    \n",
    "    def get_context_for_llm(self) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Get optimized context for LLM calls.\n",
    "        Includes system messages, summary representations, and recent messages.\n",
    "        \n",
    "        Returns:\n",
    "            List of messages optimized for context window\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        \n",
    "        # Start with system messages (always first)\n",
    "        context.extend(self.system_messages)\n",
    "        \n",
    "        # Add structured summaries converted to readable AI messages\n",
    "        for summary in self.structured_summaries:\n",
    "            # Format structured summary into readable text - This makes the summary understandable to the LLM\n",
    "            summary_text = f\"\"\"[Previous Conversation Summary]\n",
    "Identifiers: {', '.join(summary.identifiers)}\n",
    "Issues: {'; '.join(summary.key_issues)}\n",
    "Resolutions: {'; '.join(summary.resolutions)}\n",
    "Pending: {'; '.join(summary.pending_actions) if summary.pending_actions else 'None'}\"\"\"\n",
    "\n",
    "            # Wrap in an AIMessage so it appears as part of conversation history\n",
    "            context.append(AIMessage(content=summary_text))\n",
    "        \n",
    "        # Finally, add recent messages in full detail\n",
    "        context.extend(self.recent_messages)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive statistics about conversation state.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with detailed metrics\n",
    "        \"\"\"\n",
    "        # Get current context to calculate stats\n",
    "        context = self.get_context_for_llm()\n",
    "        \n",
    "        return {\n",
    "            \"total_messages_processed\": self.total_messages_processed,\n",
    "            \"summarization_count\": self.summarization_count,\n",
    "            \"current_context_items\": len(context),\n",
    "            \"estimated_tokens\": self._estimate_tokens(context),\n",
    "            \"system_messages\": len(self.system_messages),\n",
    "            \"summaries\": len(self.structured_summaries),\n",
    "            \"recent_messages\": len(self.recent_messages),\n",
    "            \"token_budget_used\": f\"{(self._estimate_tokens(context) / self.max_context_tokens * 100):.1f}%\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `ProductionConversationManager` initializes with comprehensive configuration parameters for token budgeting (`max_context_tokens`), recent message management (`recent_message_count`) and summarization behavior (`summarize_batch_size`).\n",
    "    - The three-tier storage system uses `structured_summaries` list to hold ConversationSummary Pydantic objects rather than free-form text, ensuring reliable information preservation with machine-readable schemas.\n",
    "    - The metrics tracking with `total_messages_processed` and `summarization_count` provides visibility into system behavior for monitoring dashboards and debugging.\n",
    "  \n",
    "  The configuration parameters provide clear knobs for tuning the system to different use cases - stricter token budgets for cost-sensitive applications or larger recent message windows for context-intensive tasks.\n",
    "- The `_estimate_tokens` method implements a simple character-based heuristic (4 characters per token) that provides reasonable accuracy for English text without requiring external token counting libraries. For production systems requiring exact counts, this could be replaced with tiktoken or API-based counting.\n",
    "- The `_check_and_summarize` method implements intelligent triggering logic with two threshold checks: first verifying that recent message count exceeds the limit, then checking token budget utilization against 80% of maximum to leave headroom for response generation. The 80% threshold prevents hitting hard limits during LLM calls.\n",
    "    - When both thresholds are exceeded, the method triggers structured summarization using `create_structured_summary`, stores the resulting Pydantic object, removes summarized messages from the recent buffer using list slicing, and increments the `summarization_count metric`.\n",
    "- The `add_message` method provides a clean public interface that handles all complexity internally. It routes system messages to the preserved list, adds regular messages to the recent buffer while incrementing `total_messages_processed`, and automatically calls `_check_and_summarize` to trigger compression when needed.\n",
    "- The `get_context_for_llm` method constructs the complete context by combining system messages, formatted summaries and recent messages in chronological order. T\n",
    "    - he summary formatting converts Pydantic objects into readable text with clear labels (Identifiers, Issues, Resolutions, Pending), using join operations to format lists and conditional expressions to handle empty pending_actions lists. Each formatted summary is wrapped in an AIMessage to maintain consistency with conversation message types, ensuring LLMs can process the context without special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Manager Statistics:\n",
      "================================================================================\n",
      "Total Messages Processed: 15\n",
      "Summarization Count: 2\n",
      "Current Context Items: 10\n",
      "Estimated Tokens: 191\n",
      "System Messages: 1\n",
      "Summaries: 2\n",
      "Recent Messages: 7\n",
      "Token Budget Used: 127.3%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Optimized Context for LLM:\n",
      "\n",
      "1. SystemMessage:\n",
      "   You are a helpful customer service assistant.\n",
      "\n",
      "2. AIMessage:\n",
      "   [Previous Conversation Summary]\n",
      "Identifiers: 12345\n",
      "Issues: Order was supposed to arrive yesterday; Order is delayed\n",
      "Resolutions: Checked the shipping ...\n",
      "\n",
      "3. AIMessage:\n",
      "   [Previous Conversation Summary]\n",
      "Identifiers: \n",
      "Issues: Need for urgent delivery for a gift; Cost of expedited shipping\n",
      "Resolutions: Offered expedited s...\n",
      "\n",
      "4. HumanMessage:\n",
      "   That would be great, thank you!\n",
      "\n",
      "5. AIMessage:\n",
      "   Done! Your order will arrive tomorrow by 5 PM.\n",
      "\n",
      "6. HumanMessage:\n",
      "   Actually, can I change the delivery address?\n",
      "\n",
      "7. HumanMessage:\n",
      "   Yes, please change it to 456 Oak Street\n",
      "\n",
      "8. AIMessage:\n",
      "   I've updated the delivery address to 456 Oak Street.\n",
      "\n",
      "9. HumanMessage:\n",
      "   Will it still arrive tomorrow?\n",
      "\n",
      "10. AIMessage:\n",
      "   Yes, it will still arrive tomorrow by 5 PM.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Context Efficiency:\n",
      "Original messages: 16\n",
      "Optimized context items: 10\n",
      "Reduction: 6 messages\n",
      "Compression ratio: 37.5%\n"
     ]
    }
   ],
   "source": [
    "# Example: Use production manager with a long conversation\n",
    "manager = ProductionConversationManager(\n",
    "    llm=llm,\n",
    "    max_context_tokens=150,\n",
    "    recent_message_count=6,\n",
    "    summarize_batch_size=4\n",
    ")\n",
    "\n",
    "# Process all messages\n",
    "for msg in long_conversation:\n",
    "    manager.add_message(msg)\n",
    "\n",
    "# Continue with a few more messages\n",
    "additional_messages = [\n",
    "    HumanMessage(content=\"Yes, please change it to 456 Oak Street\"),\n",
    "    AIMessage(content=\"I've updated the delivery address to 456 Oak Street.\"),\n",
    "    HumanMessage(content=\"Will it still arrive tomorrow?\"),\n",
    "    AIMessage(content=\"Yes, it will still arrive tomorrow by 5 PM.\"),\n",
    "]\n",
    "\n",
    "for msg in additional_messages:\n",
    "    manager.add_message(msg)\n",
    "\n",
    "# Get final statistics\n",
    "stats = manager.get_stats()\n",
    "\n",
    "print(\"Production Manager Statistics:\")\n",
    "print(\"=\"*80)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nOptimized Context for LLM:\")\n",
    "context = manager.get_context_for_llm()\n",
    "for i, msg in enumerate(context, 1):\n",
    "    msg_type = type(msg).__name__\n",
    "    content_preview = msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content\n",
    "    print(f\"\\n{i}. {msg_type}:\")\n",
    "    print(f\"   {content_preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nContext Efficiency:\")\n",
    "original_count = len(long_conversation) + len(additional_messages)\n",
    "optimized_count = len(context)\n",
    "print(f\"Original messages: {original_count}\")\n",
    "print(f\"Optimized context items: {optimized_count}\")\n",
    "print(f\"Reduction: {original_count - optimized_count} messages\")\n",
    "print(f\"Compression ratio: {(1 - optimized_count/original_count)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ProductionConversationManager` integrates all previous concepts into a complete system. It tracks token budgets using character-based estimation, automatically triggers summarization when recent message count exceeds the threshold and token budget exceeds 80% of maximum. The dual-threshold approach prevents premature summarization on short conversations while ensuring compression kicks in before hitting hard limits. \n",
    "\n",
    "In the example, we configure `max_context_tokens=150` to ensure summarization triggers with our 16-message conversation. The system processes messages incrementally, checking thresholds after each addition, and automatically generates structured Pydantic summaries when both conditions are met. The final statistics show how many summarization cycles occurred and the resulting compression ratio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
