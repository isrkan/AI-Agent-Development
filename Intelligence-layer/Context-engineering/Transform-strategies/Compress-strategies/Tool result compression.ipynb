{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Result Compression\n",
    "\n",
    "AI agents that interact with external tools and APIs often receive verbose responses containing extensive data structures, detailed metadata, debug information and fields irrelevant to the current task. When an agent calls a database query tool, API endpoint or file system operation, the raw result might include hundreds or thousands of tokens of JSON data, timestamps, internal identifiers, and system information that the agent does not need for decision-making. Including these complete tool results in the context window wastes valuable tokens and can overwhelm the agent with unnecessary detail, reducing its ability to focus on task-relevant information.\n",
    "\n",
    "Tool result compression addresses this challenge by intelligently reducing the size of tool outputs before they enter the agent's context. Rather than passing raw API responses or complete data structures directly to the agent, the system applies selective field extraction, summarization, schema-based filtering, and intelligent compression that preserves only the information needed for the agent's current task. This maintains the agent's awareness of tool execution results while dramatically reducing token consumption, allowing more tools to be called and more context to be preserved within the same budget.\n",
    "\n",
    "This notebook demonstrates how to implement tool result compression from basic field filtering to production-ready systems. We will explore selective field extraction from JSON responses, LLM-based summarization of verbose outputs, schema-driven compression with Pydantic models, structured truncation strategies, and complete tool result managers that automatically compress outputs based on configurable rules. These techniques are essential for building agents that use multiple tools extensively while maintaining efficient context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize language model for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(), temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The verbosity problem\n",
    "\n",
    "Before implementing compression, we need to understand the scale of the verbosity problem with typical tool results. Many APIs and tools return rich data structures with extensive metadata, nested objects, arrays of related information, and system details that are valuable for debugging but unnecessary for agent decision-making. A simple database query might return row data along with query execution time, database version, connection details and internal identifiers. An API call might include response headers, rate limit information, pagination metadata, and nested related objects.\n",
    "\n",
    "We will demonstrate this problem by simulating realistic tool results from common agent tools like database queries, API calls and file operations. This baseline shows the token cost of raw tool results and motivates the need for intelligent compression strategies.\n",
    "\n",
    "To understand the verbosity problem, we will first create a simulation function that generates a realistic database query result. This represents what an agent might receive when querying a customer database - the result includes not just the customer data, but also extensive metadata about the query execution, connection details, and nested objects for addresses and preferences that may or may not be relevant to the agent's current task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_database_query_result():\n",
    "    \"\"\"\n",
    "    Simulate a database query result with extensive metadata.\n",
    "    This represents a typical response from SQL databases, ORMs, and data access layers.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Verbose database query result with metadata and nested structures\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # The original SQL query that was executed\n",
    "        \"query\": \"SELECT * FROM customers WHERE status = 'active' AND total_purchases > 1000\",\n",
    "        \n",
    "        # Execution metadata - useful for debugging but rarely needed by agents\n",
    "        \"execution_time_ms\": 45,\n",
    "        \"rows_examined\": 15234,  # Total rows scanned\n",
    "        \"rows_returned\": 3,      # Actual results returned\n",
    "        \n",
    "        # System metadata - almost never needed for agent decision-making\n",
    "        \"database_version\": \"PostgreSQL 14.5\",\n",
    "        \"connection_id\": \"conn_9a7b3c2d\",\n",
    "        \"query_id\": \"qry_2024112201\",\n",
    "        \n",
    "        # The actual data results - this is what agents typically need\n",
    "        \"results\": [\n",
    "            {\n",
    "                # Core customer identifiers\n",
    "                \"id\": \"cust_001\",\n",
    "                \"name\": \"Alice Johnson\",\n",
    "                \"email\": \"alice.j@example.com\",\n",
    "                \"status\": \"active\",\n",
    "                \n",
    "                # Key business data\n",
    "                \"total_purchases\": 2500.00,\n",
    "                \"account_created\": \"2023-01-15T10:30:00Z\",\n",
    "                \"last_login\": \"2024-11-20T14:22:10Z\",\n",
    "                \n",
    "                # Nested address object - often verbose but sometimes needed\n",
    "                \"address\": {\n",
    "                    \"street\": \"123 Main St\",\n",
    "                    \"city\": \"San Francisco\",\n",
    "                    \"state\": \"CA\",\n",
    "                    \"zip\": \"94102\",\n",
    "                    \"country\": \"USA\"\n",
    "                },\n",
    "                \n",
    "                # User preferences - rarely relevant to business logic\n",
    "                \"preferences\": {\n",
    "                    \"newsletter\": True,\n",
    "                    \"notifications\": True,\n",
    "                    \"theme\": \"dark\"\n",
    "                },\n",
    "                \n",
    "                # Tracking metadata - useful for analytics but not agent decisions\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"web\",\n",
    "                    \"referrer\": \"google\",\n",
    "                    \"user_agent\": \"Mozilla/5.0...\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"cust_002\",\n",
    "                \"name\": \"Bob Smith\",\n",
    "                \"email\": \"bob.smith@example.com\",\n",
    "                \"status\": \"active\",\n",
    "                \"total_purchases\": 1850.00,\n",
    "                \"account_created\": \"2023-03-22T09:15:00Z\",\n",
    "                \"last_login\": \"2024-11-21T08:45:30Z\",\n",
    "                \"address\": {\n",
    "                    \"street\": \"456 Oak Ave\",\n",
    "                    \"city\": \"Austin\",\n",
    "                    \"state\": \"TX\",\n",
    "                    \"zip\": \"78701\",\n",
    "                    \"country\": \"USA\"\n",
    "                },\n",
    "                \"preferences\": {\n",
    "                    \"newsletter\": False,\n",
    "                    \"notifications\": True,\n",
    "                    \"theme\": \"light\"\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"mobile_app\",\n",
    "                    \"referrer\": \"direct\",\n",
    "                    \"user_agent\": \"MobileApp/2.1...\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"cust_003\",\n",
    "                \"name\": \"Carol White\",\n",
    "                \"email\": \"carol.w@example.com\",\n",
    "                \"status\": \"active\",\n",
    "                \"total_purchases\": 3200.00,\n",
    "                \"account_created\": \"2022-11-05T16:20:00Z\",\n",
    "                \"last_login\": \"2024-11-22T11:30:15Z\",\n",
    "                \"address\": {\n",
    "                    \"street\": \"789 Pine Rd\",\n",
    "                    \"city\": \"Seattle\",\n",
    "                    \"state\": \"WA\",\n",
    "                    \"zip\": \"98101\",\n",
    "                    \"country\": \"USA\"\n",
    "                },\n",
    "                \"preferences\": {\n",
    "                    \"newsletter\": True,\n",
    "                    \"notifications\": False,\n",
    "                    \"theme\": \"dark\"\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"web\",\n",
    "                    \"referrer\": \"facebook\",\n",
    "                    \"user_agent\": \"Mozilla/5.0...\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the actual size and token cost of this raw tool result. We will serialize it to JSON and calculate how many tokens it would consume if passed directly to the agent's context. This demonstrates the severity of the verbosity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tool Result:\n",
      "================================================================================\n",
      "{\n",
      "  \"query\": \"SELECT * FROM customers WHERE status = 'active' AND total_purchases > 1000\",\n",
      "  \"execution_time_ms\": 45,\n",
      "  \"rows_examined\": 15234,\n",
      "  \"rows_returned\": 3,\n",
      "  \"database_version\": \"PostgreSQL 14.5\",\n",
      "  \"connection_id\": \"conn_9a7b3c2d\",\n",
      "  \"query_id\": \"qry_2024112201\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"id\": \"cust_001\",\n",
      "      \"name\": \"Alice Johnson\",\n",
      "      \"email\": \"alice.j@example.com\",\n",
      "      \"status\": \"active\",\n",
      "      \"total_purchases\": 2500.0,\n",
      "      \"account_created\": \"2023-01-15T10:30:00Z\",\n",
      "   ...\n",
      "[truncated]\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Character count: 2274\n",
      "Estimated tokens: 236\n",
      "\n",
      "Problem: This single tool result consumes ~236 tokens!\n",
      "If agent calls 5 tools, that's ~1183 tokens just for tool results.\n",
      "\n",
      "Most of this data (metadata, preferences, addresses) may be irrelevant to the agent's current task.\n"
     ]
    }
   ],
   "source": [
    "# Generate a sample tool result\n",
    "raw_result = simulate_database_query_result()\n",
    "\n",
    "# Convert to JSON string to see actual size that would be sent to LLM\n",
    "raw_json = json.dumps(raw_result, indent=2)\n",
    "\n",
    "# Calculate approximate token count (rough estimate: words * 1.3)\n",
    "token_estimate = len(raw_json.split()) * 1.3\n",
    "\n",
    "# Display a preview of the raw result\n",
    "print(\"Raw Tool Result:\")\n",
    "print(\"=\"*80)\n",
    "print(raw_json[:500] + \"...\\n[truncated]\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show the token cost statistics\n",
    "print(f\"\\nCharacter count: {len(raw_json)}\")\n",
    "print(f\"Estimated tokens: {int(token_estimate)}\")\n",
    "print(f\"\\nProblem: This single tool result consumes ~{int(token_estimate)} tokens!\")\n",
    "print(f\"If agent calls 5 tools, that's ~{int(token_estimate * 5)} tokens just for tool results.\")\n",
    "print(\"\\nMost of this data (metadata, preferences, addresses) may be irrelevant to the agent's current task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simulation demonstrates the verbosity problem inherent in typical tool outputs. The database query result includes essential data (customer names and purchase totals) but also extensive metadata like execution times, database version, connection IDs, complete address objects, user preferences and tracking information. The JSON serialization produces approximately 1000+ characters translating to 200+ tokens for just three customer records. The nested structure includes multiple levels of objects with fields like `preferences` and `metadata` that are typically unused by agents making business logic decisions. This example shows how a simple query to find high-value customers returns a payload where perhaps 70-80% of the content is irrelevant to the agent's decision-making, illustrating why compression is critical for token efficiency in tool-using agents.\n",
    "\n",
    "## Selective field extraction\n",
    "The simplest and most effective compression technique is selective field extraction, where we explicitly specify which fields from tool results are relevant to the agent's task and discard everything else. Rather than passing the complete nested data structure, we create a filtered version containing only the fields the agent needs for decision-making. This requires understanding the agent's task context and configuring extraction rules that preserve essential information while removing metadata, debug data, and auxiliary fields.\n",
    "\n",
    "We will implement a field extraction system that uses path-based selection to navigate nested structures and extract specific fields. This approach provides fine-grained control over what information enters the agent's context, achieving significant compression while maintaining full access to task-relevant data.\n",
    "\n",
    "The foundation of selective extraction is the ability to navigate nested data structures using dot notation paths like `results.name` or `address.city`. We will start by building a helper function that can traverse these paths through dictionaries and lists, handling the complexity of nested objects automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_value(obj: Any, path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Navigate nested data structure using dot notation path.\n",
    "    Handles both dictionary keys and list iteration automatically.\n",
    "    \n",
    "    Args:\n",
    "        obj: The data structure to navigate (dict, list, or primitive)\n",
    "        path: Dot-separated path (e.g., 'results.name' or 'address.city')\n",
    "        \n",
    "    Returns:\n",
    "        The value at the specified path, or None if path doesn't exist\n",
    "        \n",
    "    Examples:\n",
    "        >>> data = {'user': {'name': 'Alice', 'age': 30}}\n",
    "        >>> get_nested_value(data, 'user.name')\n",
    "        'Alice'\n",
    "        \n",
    "        >>> data = {'users': [{'name': 'Alice'}, {'name': 'Bob'}]}\n",
    "        >>> get_nested_value(data, 'users.name')\n",
    "        ['Alice', 'Bob']\n",
    "    \"\"\"\n",
    "    # Split the path into individual parts (e.g., 'results.name' -> ['results', 'name'])\n",
    "    parts = path.split('.')\n",
    "    current = obj\n",
    "    \n",
    "    # Navigate through each part of the path\n",
    "    for part in parts:\n",
    "        if isinstance(current, list):  # If current value is a list, we need to extract from ALL items in the list\n",
    "            # Recursively apply the remaining path to each item\n",
    "            remaining_path = '.'.join(parts[parts.index(part):])\n",
    "            current = [get_nested_value(item, remaining_path) for item in current]\n",
    "            return current\n",
    "            \n",
    "        elif isinstance(current, dict):\n",
    "            # Navigate into the dictionary using the current path part as a key\n",
    "            current = current.get(part)\n",
    "            if current is None:\n",
    "                # Path doesn't exist - return None\n",
    "                return None\n",
    "        else:\n",
    "            # Reached a primitive value but path continues - invalid path\n",
    "            return None\n",
    "    \n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the main extraction function that uses these paths to pull specific fields from complex tool results. This function takes a list of field paths and creates a new dictionary containing only those extracted values, dramatically reducing the data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields(data: Union[Dict, List], field_paths: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract specific fields from nested data structures using path notation.\n",
    "    This is the core compression function that selects only relevant fields.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary or list containing tool result data\n",
    "        field_paths: List of dot-notation paths to extract (e.g., ['results.name', 'results.total_purchases'])\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing only the extracted fields with their values\n",
    "        \n",
    "    Example:\n",
    "        >>> data = {'results': [{'name': 'Alice', 'age': 30, 'email': 'alice@example.com'}]}\n",
    "        >>> extract_fields(data, ['results.name', 'results.age'])\n",
    "        {'results.name': ['Alice'], 'results.age': [30]}\n",
    "    \"\"\"\n",
    "    extracted = {}\n",
    "    \n",
    "    # Process each field path that was requested\n",
    "    for path in field_paths:\n",
    "        # Use our helper to navigate to the value at this path\n",
    "        value = get_nested_value(data, path)\n",
    "        \n",
    "        if value is not None:\n",
    "            # Store the extracted value using the full path as the key\n",
    "            # This preserves information about where the data came from\n",
    "            extracted[path] = value\n",
    "    \n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our extraction functions ready, we can now build a complete compression function that not only extracts the relevant fields but also tracks compression metrics so we can measure the effectiveness of this approach. The metrics help us understand how much token savings we are achieving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_tool_result_selective(\n",
    "    raw_result: Dict[str, Any],\n",
    "    relevant_fields: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compress tool result by extracting only relevant fields.\n",
    "    Includes compression metadata for monitoring effectiveness.\n",
    "    \n",
    "    Args:\n",
    "        raw_result: Complete tool result with all fields\n",
    "        relevant_fields: List of field paths to preserve\n",
    "        \n",
    "    Returns:\n",
    "        Compressed result with only specified fields plus compression metrics\n",
    "    \"\"\"\n",
    "    # Extract only the fields we care about\n",
    "    compressed = extract_fields(raw_result, relevant_fields)\n",
    "    \n",
    "    # Calculate compression metrics for monitoring\n",
    "    original_json = json.dumps(raw_result)\n",
    "    compressed_json = json.dumps(compressed)\n",
    "    \n",
    "    # Add metadata about the compression that was performed\n",
    "    # This helps track compression effectiveness in production\n",
    "    compressed['_compression_info'] = {\n",
    "        'original_fields': len(json.dumps(raw_result).split()),  # Approximate original token count\n",
    "        'compressed_fields': len(json.dumps(compressed).split()), # Approximate compressed token count\n",
    "        'compression_ratio': 1 - (len(compressed_json) / len(original_json))  # Percentage reduction\n",
    "    }\n",
    "    \n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see selective field extraction in action. Imagine our agent's task is to \"Find high-value customers and their purchase totals\" - in this case, we do not need addresses, preferences, metadata or execution details. We only need customer names and purchase amounts. By specifying just those fields, we can dramatically reduce the token cost while preserving all information needed for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed Tool Result (Selective Extraction):\n",
      "================================================================================\n",
      "{\n",
      "  \"results.name\": [\n",
      "    \"Alice Johnson\",\n",
      "    \"Bob Smith\",\n",
      "    \"Carol White\"\n",
      "  ],\n",
      "  \"results.total_purchases\": [\n",
      "    2500.0,\n",
      "    1850.0,\n",
      "    3200.0\n",
      "  ],\n",
      "  \"rows_returned\": 3,\n",
      "  \"_compression_info\": {\n",
      "    \"original_fields\": 154,\n",
      "    \"compressed_fields\": 13,\n",
      "    \"compression_ratio\": 0.9202813599062134\n",
      "  }\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original tokens: ~200\n",
      "Compressed tokens: ~26\n",
      "Token savings: ~174 (92.0% reduction)\n",
      "\n",
      "The agent still has all information needed for the task:\n",
      "  - 3 customer names\n",
      "  - 3 purchase totals\n",
      "  - Result count: 3\n"
     ]
    }
   ],
   "source": [
    "# Get our verbose database result again\n",
    "raw_result = simulate_database_query_result()\n",
    "\n",
    "# Define relevant fields for the task: \"Find high-value customers and their purchase totals\"\n",
    "# We only need:\n",
    "#   - Customer names (to identify them)\n",
    "#   - Purchase totals (to assess value)\n",
    "#   - Result count (to know how many matches)\n",
    "relevant_fields = [\n",
    "    'results.name',           # Extract customer names from the results array\n",
    "    'results.total_purchases', # Extract purchase amounts from the results array\n",
    "    'rows_returned'           # Get the count of results returned\n",
    "]\n",
    "\n",
    "# Apply selective compression\n",
    "compressed_result = compress_tool_result_selective(raw_result, relevant_fields)\n",
    "\n",
    "# Format for display\n",
    "compressed_json = json.dumps(compressed_result, indent=2)\n",
    "\n",
    "print(\"Compressed Tool Result (Selective Extraction):\")\n",
    "print(\"=\"*80)\n",
    "print(compressed_json)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Show the compression effectiveness\n",
    "original_tokens = len(json.dumps(raw_result).split()) * 1.3\n",
    "compressed_tokens = len(json.dumps(compressed_result).split()) * 1.3\n",
    "\n",
    "print(f\"\\nOriginal tokens: ~{int(original_tokens)}\")\n",
    "print(f\"Compressed tokens: ~{int(compressed_tokens)}\")\n",
    "print(f\"Token savings: ~{int(original_tokens - compressed_tokens)} ({compressed_result['_compression_info']['compression_ratio']*100:.1f}% reduction)\")\n",
    "\n",
    "# Verify we still have all the information needed\n",
    "print(f\"\\nThe agent still has all information needed for the task:\")\n",
    "print(f\"  - {len(compressed_result['results.name'])} customer names\")\n",
    "print(f\"  - {len(compressed_result['results.total_purchases'])} purchase totals\")\n",
    "print(f\"  - Result count: {compressed_result['rows_returned']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selective field extraction system uses dot notation to navigate nested data structures, supporting both dictionary traversal and list iteration.\n",
    "- The `get_nested_value` function implements recursive path navigation, splitting paths like `results.name` into components and traversing the data structure step by step. When encountering a list, it applies the remaining path to each list item, automatically handling array fields without requiring explicit iteration syntax. \n",
    "- The `compress_tool_result_selective` function wraps this extraction and adds compression metadata for monitoring effectiveness. In the example, extracting only customer names and purchase totals from the full result reduces token count from approximately 200 to 26 tokens, achieving 90%+ compression while preserving all information needed for the task of identifying high-value customers.\n",
    "- The compressed result excludes addresses, preferences, metadata, execution times and connection details that are irrelevant to the business logic decision.\n",
    "\n",
    "## LLM-Based result summarization\n",
    "For tool results that contain natural language content, detailed descriptions or complex data that can't easily be filtered by field selection alone, LLM-based summarization provides an alternative compression approach. The language model can understand the semantic content of tool results and generate concise summaries that capture essential information while discarding verbose details. This is particularly valuable for tools that return documentation, search results, log files or other text-heavy content.\n",
    "\n",
    "We will implement an LLM-powered summarization system that takes verbose tool results and produces compact summaries preserving key information. The summarization prompt is designed to maintain factual accuracy while aggressively reducing token count, ensuring the agent receives actionable information without unnecessary verbosity.\n",
    "\n",
    "The key to effective LLM-based compression is creating a summarization prompt that understands the agent's task context. We do not want a generic summary - we want a summary that preserves exactly the information the agent needs for its current task while discarding everything else. Let's build a function that takes the tool result, the task context, and generates a targeted summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_tool_result(\n",
    "    tool_name: str,\n",
    "    raw_result: Union[Dict, str],\n",
    "    task_context: str,\n",
    "    llm: ChatOpenAI,\n",
    "    max_summary_length: int = 100\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use LLM to create a concise, task-aware summary of tool results.\n",
    "    \n",
    "    Args:\n",
    "        tool_name: Name of the tool that produced the result\n",
    "        raw_result: Raw tool result (dict or string)\n",
    "        task_context: Description of what the agent is trying to accomplish\n",
    "        llm: Language model for summarization\n",
    "        max_summary_length: Target maximum words for summary\n",
    "        \n",
    "    Returns:\n",
    "        Concise summary string preserving essential information\n",
    "    \"\"\"\n",
    "    # Convert result to string representation if it's a dictionary - The LLM needs text input to process\n",
    "    if isinstance(raw_result, dict):\n",
    "        result_text = json.dumps(raw_result, indent=2)\n",
    "    else:\n",
    "        result_text = str(raw_result)\n",
    "    \n",
    "    # Create a task-aware summarization prompt - The key is emphasizing preservation of task-relevant information\n",
    "    summarization_prompt = f\"\"\"You are helping an AI agent process tool results efficiently.\n",
    "\n",
    "Tool: {tool_name}\n",
    "Agent's Task: {task_context}\n",
    "\n",
    "Raw Tool Result:\n",
    "{result_text}\n",
    "\n",
    "Create a concise summary (max {max_summary_length} words) that:\n",
    "1. Preserves all information relevant to the agent's task\n",
    "2. Includes key data points, numbers, and identifiers\n",
    "3. Omits metadata, debug info, and irrelevant details\n",
    "4. Uses clear, direct language\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    # Generate the summary using the LLM\n",
    "    response = llm.invoke([HumanMessage(content=summarization_prompt)])\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply LLM-based summarization to our database query result. We will provide the task context about targeting customers for a premium membership offer, and the LLM will intelligently summarize the results to focus on information relevant to that specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-Generated Summary:\n",
      "================================================================================\n",
      "Three high-value customers identified for premium membership targeting: \n",
      "\n",
      "1. Alice Johnson (cust_001) - $2500 total purchases, last login: 2024-11-20, email: alice.j@example.com, active status.\n",
      "2. Bob Smith (cust_002) - $1850 total purchases, last login: 2024-11-21, email: bob.smith@example.com, active status.\n",
      "3. Carol White (cust_003) - $3200 total purchases, last login: 2024-11-22, email: carol.w@example.com, active status.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original result tokens: ~200\n",
      "Summary tokens: ~68\n",
      "Compression: 65.6%\n"
     ]
    }
   ],
   "source": [
    "# Get our verbose database result\n",
    "raw_result = simulate_database_query_result()\n",
    "\n",
    "# Define the agent's task context - this guides what information to preserve\n",
    "task_context = \"Identify high-value customers to target for a premium membership offer\"\n",
    "\n",
    "# Generate a task-aware summary\n",
    "summary = summarize_tool_result(\n",
    "    tool_name=\"database_query\",\n",
    "    raw_result=raw_result,\n",
    "    task_context=task_context,\n",
    "    llm=llm,\n",
    "    max_summary_length=80  # Limit to 80 words for aggressive compression\n",
    ")\n",
    "\n",
    "print(\"LLM-Generated Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(summary)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Compare the token costs\n",
    "original_size = len(json.dumps(raw_result).split()) * 1.3\n",
    "summary_size = len(summary.split()) * 1.3\n",
    "\n",
    "print(f\"\\nOriginal result tokens: ~{int(original_size)}\")\n",
    "print(f\"Summary tokens: ~{int(summary_size)}\")\n",
    "print(f\"Compression: {(1 - summary_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary preserves:\n",
    "  - Number of customers found\n",
    "  - Their names and purchase totals\n",
    "  - Relevance to the premium membership task\n",
    "\n",
    "The summary excludes:\n",
    "  - Database execution metadata\n",
    "  - Complete address information\n",
    "  - User preferences\n",
    "\n",
    "The LLM-based summarization approach uses task-aware prompting to guide the model toward preserving information relevant to the agent's current objective.\n",
    "- The summarization prompt explicitly includes the task context (targeting customers for premium membership) alongside the raw tool result, enabling the LLM to make intelligent decisions about what information is essential versus superfluous.\n",
    "- The `max_summary_length` parameter provides control over compression aggressiveness.\n",
    "\n",
    "The LLM processes the verbose JSON structure and generates a natural language summary that captures key facts - customer names, purchase amounts, and their relevance to the task - while discarding execution metadata, nested address objects and preference settings. This typically achieves 60-75% compression while maintaining semantic completeness, though it introduces a small latency cost from the additional LLM call. The approach is particularly valuable for results containing natural language content where field selection is not sufficient.\n",
    "\n",
    "## Structured compression with schemas\n",
    "For production systems requiring reliable, consistent compression, we can define schemas that specify exactly what information should be extracted from each tool result type. Using Pydantic models, we create structured compression definitions that ensure critical fields are always preserved while providing type safety and validation. This schema-based approach combines the token efficiency of selective extraction with the reliability and maintainability of strongly-typed structures.\n",
    "\n",
    "We will implement a schema-driven compression system where each tool type has a defined output schema specifying which fields to extract, how to transform them, and what summary information to include. This provides a scalable, maintainable approach for systems with many different tools, ensuring consistent compression behavior across all tool types.\n",
    "\n",
    "First, we will define Pydantic schemas that describe the compressed format for our tool results. These schemas act as contracts, specifying exactly what fields must be present in compressed results and their types. This ensures that downstream code can rely on a consistent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerRecord(BaseModel):\n",
    "    \"\"\"\n",
    "    Compressed customer record preserving only essential information.\n",
    "    This schema defines the minimal customer data needed for most agent tasks.\n",
    "    \"\"\"\n",
    "    # Core identifiers - always needed\n",
    "    id: str = Field(description=\"Customer identifier\")\n",
    "    name: str = Field(description=\"Customer name\")\n",
    "    \n",
    "    # Business-critical data\n",
    "    total_purchases: float = Field(description=\"Total purchase amount\")\n",
    "    \n",
    "    # Optional location data - included when geography is relevant\n",
    "    city: Optional[str] = Field(default=None, description=\"City location\")\n",
    "\n",
    "\n",
    "class CompressedDatabaseResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for compressed database query results.\n",
    "    Defines the structure that all database query compressions must follow.\n",
    "    \"\"\"\n",
    "    # Metadata about the query results\n",
    "    row_count: int = Field(description=\"Number of rows returned\")\n",
    "    \n",
    "    # The actual data - array of compressed customer records\n",
    "    customers: List[CustomerRecord] = Field(description=\"Customer records\")\n",
    "    \n",
    "    # Optional performance data - can be useful for monitoring\n",
    "    execution_time_ms: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"Query execution time (optional for performance monitoring)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CompressedAPIResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for compressed API call results.\n",
    "    Generic structure for API responses that need summarization.\n",
    "    \"\"\"\n",
    "    # Response status\n",
    "    status: str = Field(description=\"API call status\")\n",
    "    \n",
    "    # Summarized content instead of full response\n",
    "    data_summary: str = Field(description=\"Brief summary of returned data\")\n",
    "    \n",
    "    # Record count for pagination awareness\n",
    "    record_count: int = Field(description=\"Number of records in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a compressor class that uses these schemas to transform raw tool results into compressed versions. The compressor will have methods for each tool type, applying the appropriate schema-based extraction and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolResultCompressor:\n",
    "    \"\"\"\n",
    "    Schema-driven tool result compression system.\n",
    "    Maps tool types to compression schemas and applies structured transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        \"\"\"\n",
    "        Initialize compressor with LLM for schema extraction.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model with structured output support\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Register compression schemas for different tool types\n",
    "        # This creates a mapping from tool name to the schema it should use\n",
    "        self.schemas = {\n",
    "            'database_query': CompressedDatabaseResult,\n",
    "            'api_call': CompressedAPIResult\n",
    "        }\n",
    "    \n",
    "    def compress_database_result(\n",
    "        self,\n",
    "        raw_result: Dict[str, Any]\n",
    "    ) -> CompressedDatabaseResult:\n",
    "        \"\"\"\n",
    "        Compress database query result using the database schema.\n",
    "        Extracts only fields defined in CompressedDatabaseResult.\n",
    "        \n",
    "        Args:\n",
    "            raw_result: Raw database query result with all fields\n",
    "            \n",
    "        Returns:\n",
    "            Compressed result matching CompressedDatabaseResult schema\n",
    "        \"\"\"\n",
    "        # Extract customer records with only essential fields\n",
    "        customers = []\n",
    "        for record in raw_result.get('results', []):\n",
    "            # Create CustomerRecord instance with only schema-defined fields\n",
    "            # This automatically excludes email, status, preferences, metadata, etc.\n",
    "            customer = CustomerRecord(\n",
    "                id=record['id'],\n",
    "                name=record['name'],\n",
    "                total_purchases=record['total_purchases'],\n",
    "                # Extract city from nested address object\n",
    "                city=record.get('address', {}).get('city')\n",
    "            )\n",
    "            customers.append(customer)\n",
    "        \n",
    "        # Build the compressed result according to schema\n",
    "        compressed = CompressedDatabaseResult(\n",
    "            row_count=raw_result.get('rows_returned', len(customers)),\n",
    "            customers=customers,\n",
    "            execution_time_ms=raw_result.get('execution_time_ms')\n",
    "        )\n",
    "        \n",
    "        return compressed\n",
    "    \n",
    "    def compress(self, tool_name: str, raw_result: Dict[str, Any]) -> Union[CompressedDatabaseResult, CompressedAPIResult, Dict]:\n",
    "        \"\"\"\n",
    "        Compress tool result based on registered schema.\n",
    "        Routes to appropriate compression method for the tool type.\n",
    "        \n",
    "        Args:\n",
    "            tool_name: Name of the tool\n",
    "            raw_result: Raw tool result\n",
    "            \n",
    "        Returns:\n",
    "            Compressed result matching appropriate schema\n",
    "        \"\"\"\n",
    "        # Route to appropriate compression method based on tool type\n",
    "        if tool_name == 'database_query':\n",
    "            return self.compress_database_result(raw_result)\n",
    "        else:\n",
    "            # Fallback: return raw result if no schema registered\n",
    "            # In production, you might want to apply generic summarization here\n",
    "            return raw_result\n",
    "    \n",
    "    def to_dict(self, compressed_result: BaseModel) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert Pydantic model to dictionary for serialization.\n",
    "        Useful for converting compressed results to JSON for storage or transmission.\n",
    "        \n",
    "        Args:\n",
    "            compressed_result: Pydantic model instance\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary representation of the model\n",
    "        \"\"\"\n",
    "        return compressed_result.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see schema-driven compression in action. We will use our compressor to transform the verbose database result into a structured, validated compressed format that is guaranteed to have the fields we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema-Based Compressed Result:\n",
      "================================================================================\n",
      "{\n",
      "  \"row_count\": 3,\n",
      "  \"customers\": [\n",
      "    {\n",
      "      \"id\": \"cust_001\",\n",
      "      \"name\": \"Alice Johnson\",\n",
      "      \"total_purchases\": 2500.0,\n",
      "      \"city\": \"San Francisco\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cust_002\",\n",
      "      \"name\": \"Bob Smith\",\n",
      "      \"total_purchases\": 1850.0,\n",
      "      \"city\": \"Austin\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cust_003\",\n",
      "      \"name\": \"Carol White\",\n",
      "      \"total_purchases\": 3200.0,\n",
      "      \"city\": \"Seattle\"\n",
      "    }\n",
      "  ],\n",
      "  \"execution_time_ms\": 45\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original tokens: ~200\n",
      "Compressed tokens: ~55\n",
      "Compression ratio: 72.1%\n"
     ]
    }
   ],
   "source": [
    "# Create the schema-driven compressor\n",
    "compressor = ToolResultCompressor(llm)\n",
    "\n",
    "# Get our verbose database result\n",
    "raw_result = simulate_database_query_result()\n",
    "\n",
    "# Compress using the database schema\n",
    "compressed = compressor.compress('database_query', raw_result)\n",
    "\n",
    "# Convert Pydantic model to dict for display\n",
    "compressed_dict = compressor.to_dict(compressed)\n",
    "compressed_json = json.dumps(compressed_dict, indent=2)\n",
    "\n",
    "print(\"Schema-Based Compressed Result:\")\n",
    "print(\"=\"*80)\n",
    "print(compressed_json)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Analyze compression effectiveness\n",
    "original_tokens = len(json.dumps(raw_result).split()) * 1.3\n",
    "compressed_tokens = len(compressed_json.split()) * 1.3\n",
    "\n",
    "print(f\"\\nOriginal tokens: ~{int(original_tokens)}\")\n",
    "print(f\"Compressed tokens: ~{int(compressed_tokens)}\")\n",
    "print(f\"Compression ratio: {(1 - compressed_tokens/original_tokens)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema guarantees:\n",
    "  - Customer IDs always preserved\n",
    "  - Names and purchase totals always included\n",
    "  - City information extracted from nested address\n",
    "  - Type safety and validation enforced\n",
    "  - Consistent compression across all database queries\n",
    "\n",
    "The schema-driven compression system uses Pydantic models to define explicit extraction and transformation rules for each tool type. \n",
    "- The `CompressedDatabaseResult` schema specifies that compressed results must include row count, a list of customer records, and optionally execution time.\n",
    "- Each `CustomerRecord` schema defines the minimal set of fields needed (id, name, total_purchases, city), automatically excluding addresses, preferences, metadata, and other verbose fields.\n",
    "- The `ToolResultCompressor` class maintains a registry mapping tool names to compression schemas and provides tool-specific compression methods that extract and transform data according to the schema.\n",
    "\n",
    "Pydantic's validation ensures that required fields are always present and correctly typed, preventing compression errors that could break downstream agent logic. In the example, the schema-based compression achieves similar 70-80% token reduction as manual extraction but provides stronger guarantees about output structure, making it more suitable for production systems with multiple tool types and complex result structures.\n",
    "\n",
    "## Production tool result manager\n",
    "Bringing together all compression techniques, we can build a production-ready tool result manager that automatically selects and applies the appropriate compression strategy based on tool type, result size and task context. This system provides a unified interface for tool result processing, handling everything from simple field extraction to LLM-based summarization to schema-driven compression. The manager includes intelligent fallbacks, compression effectiveness monitoring, and configurable strategies that can be tuned for different tools and use cases.\n",
    "\n",
    "We will implement a complete tool result manager that examines each tool result, determines the optimal compression strategy, applies it, tracks effectiveness metrics, and provides APIs for both adding new compression rules and retrieving compressed results. This represents a production-grade solution for managing tool result verbosity in complex multi-tool agent systems.\n",
    "\n",
    "We will start by defining a configuration model for compression strategies. This allows us to declaratively specify how each tool type should be compressed, making it easy to register and manage compression rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressionStrategy(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration for a specific compression strategy.\n",
    "    Defines how a particular tool's results should be compressed.\n",
    "    \"\"\"\n",
    "    # The compression method to use\n",
    "    method: str = Field(description=\"Compression method: 'selective', 'summarize', or 'schema'\")\n",
    "    \n",
    "    # For selective extraction: which field paths to keep\n",
    "    field_paths: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"Field paths for selective extraction (e.g., ['results.name', 'results.total_purchases'])\"\n",
    "    )\n",
    "    \n",
    "    # For schema-based compression: which Pydantic schema to use\n",
    "    schema_class: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Pydantic schema class name for schema-based compression\"\n",
    "    )\n",
    "    \n",
    "    # For LLM summarization: how aggressively to compress\n",
    "    max_summary_length: int = Field(\n",
    "        default=100,\n",
    "        description=\"Maximum words for LLM summarization\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the foundation of our production manager. We will initialize it with configuration options and set up the infrastructure for tracking compression strategies and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionToolResultManager:\n",
    "    \"\"\"\n",
    "    Production-ready manager for tool result compression.\n",
    "    Automatically selects and applies optimal compression strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        default_compression_threshold: int = 200,\n",
    "        enable_metrics: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize tool result manager.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for summarization\n",
    "            default_compression_threshold: Token count above which to compress (default: 200)\n",
    "            enable_metrics: Whether to track compression metrics (default: True)\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.default_compression_threshold = default_compression_threshold\n",
    "        self.enable_metrics = enable_metrics\n",
    "        \n",
    "        # Initialize the schema-driven compressor for schema-based strategies\n",
    "        self.schema_compressor = ToolResultCompressor(llm)\n",
    "        \n",
    "        # Register default compression strategies for known tools - This can be extended by calling register_strategy()\n",
    "        self.strategies: Dict[str, CompressionStrategy] = {\n",
    "            'database_query': CompressionStrategy(\n",
    "                method='schema',\n",
    "                schema_class='CompressedDatabaseResult'\n",
    "            ),\n",
    "            'api_call': CompressionStrategy(\n",
    "                method='summarize',\n",
    "                max_summary_length=80\n",
    "            ),\n",
    "            'file_read': CompressionStrategy(\n",
    "                method='summarize',\n",
    "                max_summary_length=120\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Initialize metrics tracking\n",
    "        self.metrics = {\n",
    "            'total_results': 0,  # Total number of tool results processed\n",
    "            'compressed_results': 0,  # How many were actually compressed\n",
    "            'total_tokens_saved': 0,  # Cumulative token savings\n",
    "            'compression_by_method': {  # Breakdown by compression method\n",
    "                'selective': 0,\n",
    "                'summarize': 0,\n",
    "                'schema': 0,\n",
    "                'none': 0  # Results that didn't need compression\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need helper methods for the manager - one to estimate token counts, one to decide if compression is needed, and one to allow registering new compression strategies for additional tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _estimate_tokens(self, text: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate token count for text.\n",
    "    Uses a simple heuristic: words * 1.3 (approximates GPT tokenization).\n",
    "    \n",
    "    Args:\n",
    "        text: Text to estimate tokens for\n",
    "        \n",
    "    Returns:\n",
    "        Estimated token count\n",
    "    \"\"\"\n",
    "    return int(len(text.split()) * 1.3)\n",
    "\n",
    "def _should_compress(self, raw_result: Union[Dict, str]) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if result should be compressed based on size.\n",
    "    Small results don't benefit from compression (overhead not worth it).\n",
    "    \n",
    "    Args:\n",
    "        raw_result: Raw tool result\n",
    "        \n",
    "    Returns:\n",
    "        True if result exceeds compression threshold\n",
    "    \"\"\"\n",
    "    # Convert to string for size estimation\n",
    "    if isinstance(raw_result, dict):\n",
    "        result_str = json.dumps(raw_result)\n",
    "    else:\n",
    "        result_str = str(raw_result)\n",
    "    \n",
    "    # Check if token count exceeds our threshold\n",
    "    tokens = self._estimate_tokens(result_str)\n",
    "    return tokens > self.default_compression_threshold\n",
    "\n",
    "def register_strategy(\n",
    "    self,\n",
    "    tool_name: str,\n",
    "    strategy: CompressionStrategy\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Register a compression strategy for a specific tool.\n",
    "    Allows extending the manager with new tool types.\n",
    "    \n",
    "    Args:\n",
    "        tool_name: Name of the tool\n",
    "        strategy: Compression strategy configuration\n",
    "    \"\"\"\n",
    "    self.strategies[tool_name] = strategy\n",
    "\n",
    "# Add these helper methods to the ProductionToolResultManager class\n",
    "ProductionToolResultManager._estimate_tokens = _estimate_tokens\n",
    "ProductionToolResultManager._should_compress = _should_compress\n",
    "ProductionToolResultManager.register_strategy = register_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the core compression logic. This method is the heart of the production manager - it examines each tool result, selects the appropriate compression strategy, applies it, and tracks metrics. The logic handles routing to different compression methods, calculating savings, and maintaining detailed statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_result(\n",
    "    self,\n",
    "    tool_name: str,\n",
    "    raw_result: Union[Dict, str],\n",
    "    task_context: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compress tool result using appropriate strategy.\n",
    "    This is the main entry point for compression.\n",
    "    \n",
    "    Args:\n",
    "        tool_name: Name of the tool that produced the result\n",
    "        raw_result: Raw tool result (dict or string)\n",
    "        task_context: Optional context about agent's current task\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - compressed: bool indicating if compression was applied\n",
    "            - result: the compressed (or raw) result\n",
    "            - method: compression method used\n",
    "            - metrics: token counts and savings (if compressed)\n",
    "    \"\"\"\n",
    "    # Increment total results counter\n",
    "    self.metrics['total_results'] += 1\n",
    "\n",
    "    # Calculate original size for all results (needed for consistent return structure)\n",
    "    original_str = json.dumps(raw_result) if isinstance(raw_result, dict) else str(raw_result)\n",
    "    original_tokens = self._estimate_tokens(original_str)\n",
    "    \n",
    "    # Check if compression is needed based on size\n",
    "    if not self._should_compress(raw_result):\n",
    "        # Result is small enough - no compression needed\n",
    "        # Return consistent structure with token metrics showing no compression\n",
    "        self.metrics['compression_by_method']['none'] += 1\n",
    "        return {\n",
    "            'compressed': False,\n",
    "            'result': raw_result,\n",
    "            'method': 'none',\n",
    "            'original_tokens': original_tokens,\n",
    "            'compressed_tokens': original_tokens,  # Same as original since no compression\n",
    "            'tokens_saved': 0,  # No tokens saved\n",
    "            'compression_ratio': 0.0  # No compression ratio\n",
    "        }\n",
    "    \n",
    "    # Get the compression strategy for this tool (if registered)\n",
    "    strategy = self.strategies.get(tool_name)\n",
    "    \n",
    "    # Initialize variables for compression\n",
    "    compressed_result = None\n",
    "    method_used = 'none'\n",
    "    \n",
    "    # Apply the appropriate compression method\n",
    "    if strategy:\n",
    "        if strategy.method == 'schema':\n",
    "            # Use schema-driven compression\n",
    "            compressed_obj = self.schema_compressor.compress(tool_name, raw_result)\n",
    "            compressed_result = self.schema_compressor.to_dict(compressed_obj)\n",
    "            method_used = 'schema'\n",
    "            \n",
    "        elif strategy.method == 'selective' and strategy.field_paths:\n",
    "            # Use selective field extraction\n",
    "            compressed_result = extract_fields(raw_result, strategy.field_paths)\n",
    "            method_used = 'selective'\n",
    "            \n",
    "        elif strategy.method == 'summarize':\n",
    "            # Use LLM summarization\n",
    "            summary = summarize_tool_result(\n",
    "                tool_name=tool_name,\n",
    "                raw_result=raw_result,\n",
    "                task_context=task_context or \"Processing tool result\",\n",
    "                llm=self.llm,\n",
    "                max_summary_length=strategy.max_summary_length\n",
    "            )\n",
    "            compressed_result = {'summary': summary}\n",
    "            method_used = 'summarize'\n",
    "    else:\n",
    "        # No strategy registered for this tool - use default summarization as fallback\n",
    "        summary = summarize_tool_result(\n",
    "            tool_name=tool_name,\n",
    "            raw_result=raw_result,\n",
    "            task_context=task_context or \"Processing tool result\",\n",
    "            llm=self.llm\n",
    "        )\n",
    "        compressed_result = {'summary': summary}\n",
    "        method_used = 'summarize'\n",
    "    \n",
    "    # Calculate compression metrics\n",
    "    compressed_str = json.dumps(compressed_result)\n",
    "    compressed_tokens = self._estimate_tokens(compressed_str)\n",
    "    tokens_saved = original_tokens - compressed_tokens\n",
    "    \n",
    "    # Update metrics tracking\n",
    "    if self.enable_metrics:\n",
    "        self.metrics['compressed_results'] += 1\n",
    "        self.metrics['total_tokens_saved'] += tokens_saved\n",
    "        self.metrics['compression_by_method'][method_used] += 1\n",
    "    \n",
    "    # Return compressed result with full metadata\n",
    "    return {\n",
    "        'compressed': True,\n",
    "        'result': compressed_result,\n",
    "        'method': method_used,\n",
    "        'original_tokens': original_tokens,\n",
    "        'compressed_tokens': compressed_tokens,\n",
    "        'tokens_saved': tokens_saved,\n",
    "        'compression_ratio': (1 - compressed_tokens / original_tokens) * 100\n",
    "    }\n",
    "\n",
    "# Add the compress_result method to the ProductionToolResultManager class\n",
    "ProductionToolResultManager.compress_result = compress_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add a method to retrieve compression metrics. This allows monitoring the effectiveness of compression strategies in production, helping identify opportunities for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(self) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get compression metrics and statistics.\n",
    "    Useful for monitoring compression effectiveness in production.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - total_results: total tool results processed\n",
    "            - compressed_results: how many were compressed\n",
    "            - total_tokens_saved: cumulative token savings\n",
    "            - compression_by_method: breakdown by method\n",
    "            - compression_rate: percentage of results that were compressed\n",
    "    \"\"\"\n",
    "    return {\n",
    "        **self.metrics,  # Include all tracked metrics\n",
    "        # Add calculated compression rate\n",
    "        'compression_rate': (\n",
    "            self.metrics['compressed_results'] / \n",
    "            max(self.metrics['total_results'], 1) * 100\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Add the get_metrics method to the ProductionToolResultManager class\n",
    "ProductionToolResultManager.get_metrics = get_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the production manager in action with a realistic multi-tool workflow. We will process several different tool results - a database query, an API call and a small result - demonstrating how the manager intelligently selects compression strategies and tracks metrics across all operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Result Processing:\n",
      "================================================================================\n",
      "\n",
      "1. Database Query Result:\n",
      "   Method: schema\n",
      "   Original: 200 tokens\n",
      "   Compressed: 42 tokens\n",
      "   Savings: 158 tokens (79.0%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the production manager with our configuration\n",
    "manager = ProductionToolResultManager(\n",
    "    llm=llm,\n",
    "    default_compression_threshold=150  # Compress results larger than 150 tokens\n",
    ")\n",
    "\n",
    "# Simulate a multi-tool workflow\n",
    "print(\"Tool Result Processing:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tool 1: Database query (will use schema compression)\n",
    "db_result = simulate_database_query_result()\n",
    "compressed_db = manager.compress_result(\n",
    "    'database_query',\n",
    "    db_result,\n",
    "    task_context=\"Find customers for premium membership offer\"\n",
    ")\n",
    "\n",
    "print(\"\\n1. Database Query Result:\")\n",
    "print(f\"   Method: {compressed_db['method']}\")\n",
    "print(f\"   Original: {compressed_db['original_tokens']} tokens\")\n",
    "print(f\"   Compressed: {compressed_db['compressed_tokens']} tokens\")\n",
    "print(f\"   Savings: {compressed_db['tokens_saved']} tokens ({compressed_db['compression_ratio']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. API Call Result:\n",
      "   Method: summarize\n",
      "   Original: 169 tokens\n",
      "   Compressed: 91 tokens\n",
      "   Savings: 78 tokens (46.2%)\n"
     ]
    }
   ],
   "source": [
    "# Tool 2: API call (will use summarization)\n",
    "api_result = {\n",
    "    'status': 200,\n",
    "    'timestamp': '2024-11-22T10:30:00Z',\n",
    "    'request_id': 'req_9x7b2c4d',\n",
    "    'headers': {\n",
    "        'content-type': 'application/json',\n",
    "        'x-rate-limit': '100',\n",
    "        'x-rate-limit-remaining': '87',\n",
    "        'x-request-id': 'req_9x7b2c4d',\n",
    "        'server': 'nginx/1.21.0',\n",
    "        'cache-control': 'no-cache'\n",
    "    },\n",
    "    'metadata': {\n",
    "        'total_count': 8,\n",
    "        'page': 1,\n",
    "        'per_page': 10,\n",
    "        'api_version': '2.1.0'\n",
    "    },\n",
    "    'data': {\n",
    "        'products': [\n",
    "            {\n",
    "                'id': 'p1',\n",
    "                'name': 'Widget',\n",
    "                'description': 'High-quality widget for industrial use',\n",
    "                'price': 29.99,\n",
    "                'stock': 150,\n",
    "                'category': 'Tools',\n",
    "                'manufacturer': 'WidgetCo',\n",
    "                'sku': 'WDG-001'\n",
    "            },\n",
    "            {\n",
    "                'id': 'p2',\n",
    "                'name': 'Gadget',\n",
    "                'description': 'Multi-purpose gadget with premium features',\n",
    "                'price': 49.99,\n",
    "                'stock': 75,\n",
    "                'category': 'Electronics',\n",
    "                'manufacturer': 'GadgetInc',\n",
    "                'sku': 'GDG-002'\n",
    "            },\n",
    "            {\n",
    "                'id': 'p3',\n",
    "                'name': 'Gizmo',\n",
    "                'description': 'Advanced gizmo for professional applications',\n",
    "                'price': 79.99,\n",
    "                'stock': 45,\n",
    "                'category': 'Tools',\n",
    "                'manufacturer': 'GizmoCorp',\n",
    "                'sku': 'GZM-003'\n",
    "            },\n",
    "            {\n",
    "                'id': 'p4',\n",
    "                'name': 'Doohickey',\n",
    "                'description': 'Essential doohickey for everyday tasks',\n",
    "                'price': 19.99,\n",
    "                'stock': 200,\n",
    "                'category': 'Accessories',\n",
    "                'manufacturer': 'DoohickeyLtd',\n",
    "                'sku': 'DHK-004'\n",
    "            },\n",
    "            {\n",
    "                'id': 'p5',\n",
    "                'name': 'Thingamajig',\n",
    "                'description': 'Versatile thingamajig with multiple uses',\n",
    "                'price': 34.99,\n",
    "                'stock': 120,\n",
    "                'category': 'Tools',\n",
    "                'manufacturer': 'ThingCo',\n",
    "                'sku': 'THG-005'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "compressed_api = manager.compress_result(\n",
    "    'api_call',\n",
    "    api_result,\n",
    "    task_context=\"Check product availability\"\n",
    ")\n",
    "\n",
    "print(\"\\n2. API Call Result:\")\n",
    "print(f\"   Method: {compressed_api['method']}\")\n",
    "print(f\"   Original: {compressed_api['original_tokens']} tokens\")\n",
    "print(f\"   Compressed: {compressed_api['compressed_tokens']} tokens\")\n",
    "print(f\"   Savings: {compressed_api['tokens_saved']} tokens ({compressed_api['compression_ratio']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Small Tool Result:\n",
      "   Method: none\n",
      "   Compressed: False\n",
      "   (Result was below threshold - no compression needed)\n"
     ]
    }
   ],
   "source": [
    "# Tool 3: Small result (will not be compressed)\n",
    "small_result = {'status': 'success', 'count': 5}\n",
    "compressed_small = manager.compress_result('simple_tool', small_result)\n",
    "\n",
    "print(\"\\n3. Small Tool Result:\")\n",
    "print(f\"   Method: {compressed_small['method']}\")\n",
    "print(f\"   Compressed: {compressed_small['compressed']}\")\n",
    "print(f\"   (Result was below threshold - no compression needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Overall Compression Metrics:\n",
      "  Total tool results processed: 3\n",
      "  Results compressed: 2\n",
      "  Compression rate: 66.7%\n",
      "  Total tokens saved: 236\n",
      "\n",
      "  Compression methods used:\n",
      "    selective: 0\n",
      "    summarize: 1\n",
      "    schema: 1\n",
      "    none: 1\n"
     ]
    }
   ],
   "source": [
    "# Display overall metrics from the entire workflow\n",
    "metrics = manager.get_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nOverall Compression Metrics:\")\n",
    "print(f\"  Total tool results processed: {metrics['total_results']}\")\n",
    "print(f\"  Results compressed: {metrics['compressed_results']}\")\n",
    "print(f\"  Compression rate: {metrics['compression_rate']:.1f}%\")\n",
    "print(f\"  Total tokens saved: {metrics['total_tokens_saved']}\")\n",
    "print(f\"\\n  Compression methods used:\")\n",
    "for method, count in metrics['compression_by_method'].items():\n",
    "    print(f\"    {method}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ProductionToolResultManager` implements a complete tool result processing pipeline with automatic strategy selection, multiple compression methods and comprehensive metrics tracking. \n",
    "- The system maintains a registry of compression strategies mapped to tool names, allowing different tools to use optimal compression approaches - schema-based for structured database results, summarization for text-heavy API responses and selective extraction for specific use cases.\n",
    "- The `compress_result` method first checks if compression is needed by comparing token count to a configurable threshold, then applies the registered strategy or falls back to default summarization.\n",
    "- The manager tracks detailed metrics including total results processed, compression rates by method, and cumulative token savings, providing visibility into compression effectiveness.\n",
    "\n",
    "The example demonstrates a realistic multi-tool workflow. This production-ready implementation provides the foundation for efficient context management in complex agent systems that make frequent tool calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
