{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Consolidation\n",
    "\n",
    "Long-running AI agents accumulate numerous memory entries over time, storing facts, preferences, past interactions and learned information about users and contexts. As agents interact across multiple sessions and handle diverse tasks, their memory systems can become fragmented with redundant, overlapping, or closely related information stored as separate entries. A customer service agent might store \"User prefers email communication\" in one memory, \"Customer likes email updates\" in another, and \"Contact via email requested\" in a third, consuming three times the necessary context space with semantically identical information.\n",
    "\n",
    "Memory consolidation addresses this fragmentation by identifying related memories, merging redundant or overlapping information, and creating unified memory entries that capture the same knowledge in less space. Rather than maintaining dozens of fragmented facts about a user or context, the system detects semantic similarity between memories, groups related information, and consolidates it into comprehensive but concise memory records. This reduces memory storage requirements, improves retrieval relevance by avoiding duplicate hits, and makes the agent's knowledge more coherent and maintainable.\n",
    "\n",
    "This notebook demonstrates how to implement memory consolidation from basic duplicate detection to production-ready systems. We will explore semantic similarity detection for finding related memories, deduplication strategies that merge identical or near-identical information, clustering techniques that group thematically related memories, and complete consolidation managers that automatically maintain clean, efficient memory stores. These techniques are essential for building agents that learn and remember effectively over extended periods without exponential memory growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any, Tuple, Set, Union\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the language and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize language model and embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(), temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory fragmentation problem\n",
    "Before implementing consolidation, we need to understand how memory fragmentation occurs in real agent systems. As agents interact with users, call tools and learn from experiences, they continuously add new memory entries. Without consolidation, semantically related or redundant information accumulates as separate entries, each consuming context tokens and potentially causing confusion when similar but slightly different memories are retrieved together.\n",
    "\n",
    "We will simulate a realistic fragmented memory store from a customer service agent that has accumulated redundant information about user preferences, past issues and account details across multiple interactions. This baseline demonstrates both the token cost and the semantic confusion caused by fragmentation, motivating the need for intelligent consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Entry Model:\n",
      "================================================================================\n",
      "{'description': \"Represents a single memory entry in the agent's memory system.\\nEach memory has content, metadata, and tracking information.\", 'properties': {'id': {'description': 'Unique memory identifier', 'title': 'Id', 'type': 'string'}, 'content': {'description': 'Memory content', 'title': 'Content', 'type': 'string'}, 'timestamp': {'description': 'When memory was created', 'title': 'Timestamp', 'type': 'string'}, 'category': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Memory category', 'title': 'Category'}, 'importance': {'default': 0.5, 'description': 'Importance score 0-1', 'title': 'Importance', 'type': 'number'}}, 'required': ['id', 'content', 'timestamp'], 'title': 'MemoryEntry', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "class MemoryEntry(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a single memory entry in the agent's memory system.\n",
    "    Each memory has content, metadata, and tracking information.\n",
    "    \"\"\"\n",
    "    id: str = Field(description=\"Unique memory identifier\")\n",
    "    content: str = Field(description=\"Memory content\")\n",
    "    timestamp: str = Field(description=\"When memory was created\")\n",
    "    category: Optional[str] = Field(default=None, description=\"Memory category\")\n",
    "    importance: float = Field(default=0.5, description=\"Importance score 0-1\")\n",
    "\n",
    "# Display the model structure\n",
    "print(\"Memory Entry Model:\")\n",
    "print(\"=\"*80)\n",
    "print(MemoryEntry.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will simulate a realistic scenario where a customer service agent has accumulated fragmented memories over several days of interactions. Notice how multiple memories contain essentially the same information but worded differently - this is the fragmentation problem we are solving. In a real system, these would accumulate gradually as the agent interacts with users, and without consolidation, the memory store would grow unnecessarily large while also making retrieval less effective since similar memories compete for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9 memory entries\n",
      "\n",
      "Sample memories:\n",
      "  [mem_001] (preference) User prefers email communication over phone calls\n",
      "  [mem_002] (preference) Customer likes to receive updates via email\n",
      "  [mem_003] (preference) User requested email as primary contact method\n"
     ]
    }
   ],
   "source": [
    "# Create fragmented memories simulating a customer service agent\n",
    "# These memories were accumulated over several days of interactions\n",
    "fragmented_memories = [\n",
    "    MemoryEntry(\n",
    "        id=\"mem_001\",\n",
    "        content=\"User prefers email communication over phone calls\",\n",
    "        timestamp=\"2024-11-01T10:00:00Z\",\n",
    "        category=\"preference\",\n",
    "        importance=0.7\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_002\",\n",
    "        content=\"Customer likes to receive updates via email\",\n",
    "        timestamp=\"2024-11-03T14:30:00Z\",\n",
    "        category=\"preference\",\n",
    "        importance=0.6\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_003\",\n",
    "        content=\"User requested email as primary contact method\",\n",
    "        timestamp=\"2024-11-05T09:15:00Z\",\n",
    "        category=\"preference\",\n",
    "        importance=0.8\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_004\",\n",
    "        content=\"User reported login issues on mobile app\",\n",
    "        timestamp=\"2024-11-02T11:20:00Z\",\n",
    "        category=\"issue\",\n",
    "        importance=0.9\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_005\",\n",
    "        content=\"Customer had trouble signing in to the mobile application\",\n",
    "        timestamp=\"2024-11-02T11:45:00Z\",\n",
    "        category=\"issue\",\n",
    "        importance=0.9\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_006\",\n",
    "        content=\"User's account is premium tier with annual subscription\",\n",
    "        timestamp=\"2024-11-01T09:00:00Z\",\n",
    "        category=\"account\",\n",
    "        importance=0.8\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_007\",\n",
    "        content=\"Customer has premium membership, annual billing\",\n",
    "        timestamp=\"2024-11-04T16:00:00Z\",\n",
    "        category=\"account\",\n",
    "        importance=0.7\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_008\",\n",
    "        content=\"User lives in California, PST timezone\",\n",
    "        timestamp=\"2024-11-01T10:30:00Z\",\n",
    "        category=\"personal\",\n",
    "        importance=0.5\n",
    "    ),\n",
    "    MemoryEntry(\n",
    "        id=\"mem_009\",\n",
    "        content=\"Customer is located in California\",\n",
    "        timestamp=\"2024-11-06T13:00:00Z\",\n",
    "        category=\"personal\",\n",
    "        importance=0.4\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(fragmented_memories)} memory entries\")\n",
    "print(\"\\nSample memories:\")\n",
    "for mem in fragmented_memories[:3]:\n",
    "    print(f\"  [{mem.id}] ({mem.category}) {mem.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the fragmentation by grouping memories by category and calculating the token usage. This will help us understand the extent of redundancy and the potential for consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragmented Memory Store Analysis:\n",
      "================================================================================\n",
      "\n",
      "Total memories: 9\n",
      "\n",
      "Memories by category:\n",
      "\n",
      "PREFERENCE (3 memories):\n",
      "  [mem_001] User prefers email communication over phone calls\n",
      "  [mem_002] Customer likes to receive updates via email\n",
      "  [mem_003] User requested email as primary contact method\n",
      "\n",
      "ISSUE (2 memories):\n",
      "  [mem_004] User reported login issues on mobile app\n",
      "  [mem_005] Customer had trouble signing in to the mobile application\n",
      "\n",
      "ACCOUNT (2 memories):\n",
      "  [mem_006] User's account is premium tier with annual subscription\n",
      "  [mem_007] Customer has premium membership, annual billing\n",
      "\n",
      "PERSONAL (2 memories):\n",
      "  [mem_008] User lives in California, PST timezone\n",
      "  [mem_009] Customer is located in California\n",
      "================================================================================\n",
      "\n",
      "Estimated token usage: ~80 tokens\n",
      "\n",
      "Potential after consolidation:\n",
      "  Current: 9 memories (~80 tokens)\n",
      "  After: ~4-5 memories (~40-48 tokens)\n",
      "  Expected reduction: ~40-50%\n"
     ]
    }
   ],
   "source": [
    "# Import defaultdict for easy grouping\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group memories by category to see patterns of redundancy\n",
    "by_category = defaultdict(list)\n",
    "for mem in fragmented_memories:\n",
    "    by_category[mem.category].append(mem)\n",
    "\n",
    "print(\"Fragmented Memory Store Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal memories: {len(fragmented_memories)}\")\n",
    "print(\"\\nMemories by category:\")\n",
    "\n",
    "# Display each category and its memories\n",
    "for category, mems in by_category.items():\n",
    "    print(f\"\\n{category.upper()} ({len(mems)} memories):\")\n",
    "    for mem in mems:\n",
    "        # Show memory ID and content for each entry\n",
    "        print(f\"  [{mem.id}] {mem.content}\")\n",
    "\n",
    "# Calculate estimated token usage\n",
    "# Rule of thumb: 1 word ≈ 1.3 tokens on average\n",
    "total_tokens = sum(len(mem.content.split()) * 1.3 for mem in fragmented_memories)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nEstimated token usage: ~{int(total_tokens)} tokens\")\n",
    "\n",
    "# Calculate potential savings after consolidation\n",
    "print(f\"\\nPotential after consolidation:\")\n",
    "print(f\"  Current: {len(fragmented_memories)} memories (~{int(total_tokens)} tokens)\")\n",
    "print(f\"  After: ~4-5 memories (~{int(total_tokens * 0.5)}-{int(total_tokens * 0.6)} tokens)\")\n",
    "print(f\"  Expected reduction: ~40-50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fragmented memory simulation demonstrates realistic accumulation patterns in long-running agents.\n",
    "- The `MemoryEntry` model captures both content and metadata including timestamps, categories, and importance scores.\n",
    "- The example shows multiple forms of fragmentation: exact semantic duplicates with different wording (mobile login issue memories), near-duplicates with minor detail variations (premium account memories), and redundant information stating the same fact differently (email preference and California location memories).\n",
    "- The category-based analysis reveals clustering of related memories, and the token calculation shows approximately 80 tokens consumed by 9 memories that could likely be consolidated to 4-5 memories representing the same information. This fragmentation is common in production systems where memories are added incrementally without consolidation, leading to both inefficient token usage and potential retrieval confusion when multiple similar memories are returned for the same query.\n",
    "\n",
    "## Detecting related memories\n",
    "The first step in consolidation is identifying which memories are related, redundant or overlapping. Semantic similarity using embeddings provides the most effective approach, as it detects memories that convey similar meaning even when worded differently. By embedding each memory and computing similarity scores between pairs, we can identify candidates for consolidation without relying on exact text matching or keyword overlap.\n",
    "\n",
    "We will implement a similarity-based detection system that embeds all memories, computes pairwise similarity scores, and identifies clusters of related memories that should be considered for merging. This provides the foundation for automated consolidation by surfacing memories that likely contain redundant or overlapping information.\n",
    "\n",
    "We will start by creating a function that computes semantic similarity between all memory pairs using embeddings. The function embeds each memory's content into a high-dimensional vector space, then calculates cosine similarity between every pair of vectors. Cosine similarity ranges from 0 to 1, where 1 means the memories are semantically identical and 0 means they are completely unrelated. This approach captures semantic meaning rather than just lexical overlap, so \"User prefers email\" and \"Customer likes email updates\" will show high similarity despite different wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pairwise similarity scores...\n",
      "✓ Computed 36 pairwise similarities for 9 memories\n"
     ]
    }
   ],
   "source": [
    "def compute_memory_similarity(\n",
    "    memories: List[MemoryEntry],\n",
    "    embeddings_model: OpenAIEmbeddings\n",
    ") -> Dict[Tuple[str, str], float]:\n",
    "    \"\"\"\n",
    "    Compute pairwise similarity scores between all memories using embeddings.\n",
    "    This function creates semantic embeddings for each memory and calculates how similar each pair of memories is using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        memories: List of memory entries to compare\n",
    "        embeddings_model: Embedding model for generating semantic vectors\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping memory ID pairs (tuples) to similarity scores (0-1)\n",
    "    \"\"\"\n",
    "    # Step 1: Extract text content from all memories\n",
    "    memory_texts = [mem.content for mem in memories]\n",
    "    \n",
    "    # Step 2: Generate embeddings for all memory texts in one batch - This converts each text into a high-dimensional vector\n",
    "    memory_embeddings = embeddings_model.embed_documents(memory_texts)\n",
    "    \n",
    "    # Step 3: Compute pairwise similarities using cosine similarity\n",
    "    similarity_scores = {}\n",
    "    \n",
    "    # Compare each pair of memories (avoiding duplicate comparisons)\n",
    "    for i in range(len(memories)):\n",
    "        for j in range(i + 1, len(memories)):  # j > i to avoid duplicates\n",
    "            # Convert embeddings to numpy arrays for mathematical operations\n",
    "            emb_i = np.array(memory_embeddings[i])\n",
    "            emb_j = np.array(memory_embeddings[j])\n",
    "            \n",
    "            # Calculate cosine similarity: measures angle between vectors\n",
    "            # Formula: dot(A, B) / (||A|| * ||B||)\n",
    "            # Result ranges from 0 (completely different) to 1 (identical)\n",
    "            similarity = np.dot(emb_i, emb_j) / (np.linalg.norm(emb_i) * np.linalg.norm(emb_j))\n",
    "            \n",
    "            # Store similarity score with memory ID pair as key\n",
    "            pair_key = (memories[i].id, memories[j].id)\n",
    "            similarity_scores[pair_key] = float(similarity)\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# Test the function with our fragmented memories\n",
    "print(\"Computing pairwise similarity scores...\")\n",
    "similarity_scores = compute_memory_similarity(fragmented_memories, embeddings)\n",
    "print(f\"✓ Computed {len(similarity_scores)} pairwise similarities for {len(fragmented_memories)} memories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the top 10 most similar memory pairs to understand what patterns the similarity detection is finding. High similarity scores (above 0.85) indicate memories that are likely redundant or overlapping and should be candidates for consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Similar Memory Pairs:\n",
      "================================================================================\n",
      "\n",
      "Similarity Score: 0.918\n",
      "  [mem_006] User's account is premium tier with annual subscription\n",
      "  [mem_007] Customer has premium membership, annual billing\n",
      "\n",
      "Similarity Score: 0.914\n",
      "  [mem_004] User reported login issues on mobile app\n",
      "  [mem_005] Customer had trouble signing in to the mobile application\n",
      "\n",
      "Similarity Score: 0.888\n",
      "  [mem_001] User prefers email communication over phone calls\n",
      "  [mem_002] Customer likes to receive updates via email\n",
      "\n",
      "Similarity Score: 0.884\n",
      "  [mem_001] User prefers email communication over phone calls\n",
      "  [mem_003] User requested email as primary contact method\n",
      "\n",
      "Similarity Score: 0.863\n",
      "  [mem_002] Customer likes to receive updates via email\n",
      "  [mem_003] User requested email as primary contact method\n",
      "\n",
      "Similarity Score: 0.860\n",
      "  [mem_008] User lives in California, PST timezone\n",
      "  [mem_009] Customer is located in California\n",
      "\n",
      "Similarity Score: 0.829\n",
      "  [mem_002] Customer likes to receive updates via email\n",
      "  [mem_007] Customer has premium membership, annual billing\n",
      "\n",
      "Similarity Score: 0.810\n",
      "  [mem_002] Customer likes to receive updates via email\n",
      "  [mem_005] Customer had trouble signing in to the mobile application\n",
      "\n",
      "Similarity Score: 0.810\n",
      "  [mem_002] Customer likes to receive updates via email\n",
      "  [mem_009] Customer is located in California\n",
      "\n",
      "Similarity Score: 0.810\n",
      "  [mem_003] User requested email as primary contact method\n",
      "  [mem_005] Customer had trouble signing in to the mobile application\n"
     ]
    }
   ],
   "source": [
    "# Sort similarity scores from highest to lowest\n",
    "sorted_pairs = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Create a lookup dictionary for easy memory access by ID\n",
    "memory_lookup = {mem.id: mem for mem in fragmented_memories}\n",
    "\n",
    "print(\"Top 10 Most Similar Memory Pairs:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display each similar pair with its score and content\n",
    "for (id1, id2), score in sorted_pairs:\n",
    "    mem1 = memory_lookup[id1]\n",
    "    mem2 = memory_lookup[id2]\n",
    "    \n",
    "    print(f\"\\nSimilarity Score: {score:.3f}\")\n",
    "    print(f\"  [{id1}] {mem1.content}\")\n",
    "    print(f\"  [{id2}] {mem2.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to group related memories together. Simply looking at pairwise similarities is not enough - if memory A is similar to memory B, and memory B is similar to memory C, then A, B and C should all be in the same group even if A and C are not directly similar. We will use graph-based connected components to find these transitive relationships. This function builds a graph where memories are nodes and edges connect similar memories, then finds all connected groups using depth-first search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding groups of related memories...\n",
      "✓ Found 4 groups of related memories\n"
     ]
    }
   ],
   "source": [
    "def find_related_memories(\n",
    "    memories: List[MemoryEntry],\n",
    "    similarity_scores: Dict[Tuple[str, str], float],\n",
    "    similarity_threshold: float = 0.85\n",
    ") -> List[Set[str]]:\n",
    "    \"\"\"\n",
    "    Group related memories based on similarity threshold using graph connectivity.\n",
    "    \n",
    "    Builds a graph where memories are nodes and edges exist between similar\n",
    "    memories (similarity >= threshold). Then finds all connected components,\n",
    "    which represent groups of transitively related memories.\n",
    "    \n",
    "    Args:\n",
    "        memories: List of memory entries\n",
    "        similarity_scores: Pairwise similarity scores from compute_memory_similarity\n",
    "        similarity_threshold: Minimum similarity to consider memories related (default: 0.85)\n",
    "        \n",
    "    Returns:\n",
    "        List of sets, where each set contains IDs of memories that should be merged\n",
    "    \"\"\"\n",
    "    # Step 1: Build an adjacency list representing the similarity graph\n",
    "    related_graph = defaultdict(set)\n",
    "    \n",
    "    for (mem_id1, mem_id2), score in similarity_scores.items():\n",
    "        if score >= similarity_threshold:\n",
    "            # These memories are similar enough to be related\n",
    "            # Add bidirectional edges in the graph\n",
    "            related_graph[mem_id1].add(mem_id2)\n",
    "            related_graph[mem_id2].add(mem_id1)\n",
    "    \n",
    "    # Step 2: Find connected components using depth-first search\n",
    "    visited = set()  # Track which memories we've already processed\n",
    "    memory_groups = []  # Store the resulting groups\n",
    "    \n",
    "    def dfs(mem_id: str, current_group: Set[str]):\n",
    "        \"\"\"Recursively visit all memories connected to mem_id.\"\"\"\n",
    "        if mem_id in visited:\n",
    "            return  # Already processed this memory\n",
    "        \n",
    "        # Mark as visited and add to current group\n",
    "        visited.add(mem_id)\n",
    "        current_group.add(mem_id)\n",
    "        \n",
    "        # Recursively visit all directly connected memories\n",
    "        for related_id in related_graph[mem_id]:\n",
    "            dfs(related_id, current_group)\n",
    "    \n",
    "    # Step 3: Find all connected components in the graph\n",
    "    for memory in memories:\n",
    "        # Only start DFS from memories that haven't been visited\n",
    "        # and that have at least one connection\n",
    "        if memory.id not in visited and memory.id in related_graph:\n",
    "            group = set()\n",
    "            dfs(memory.id, group)\n",
    "            \n",
    "            # Only include groups with multiple memories (singletons don't need merging)\n",
    "            if len(group) > 1:\n",
    "                memory_groups.append(group)\n",
    "    \n",
    "    return memory_groups\n",
    "\n",
    "# Find groups of related memories\n",
    "print(\"Finding groups of related memories...\")\n",
    "related_groups = find_related_memories(\n",
    "    fragmented_memories, \n",
    "    similarity_scores, \n",
    "    similarity_threshold=0.85  # Memories with 85%+ similarity will be grouped\n",
    ")\n",
    "\n",
    "print(f\"✓ Found {len(related_groups)} groups of related memories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related Memory Groups:\n",
      "================================================================================\n",
      "\n",
      "Group 1: 3 memories that should be merged\n",
      "  [mem_001] (preference) User prefers email communication over phone calls\n",
      "  [mem_003] (preference) User requested email as primary contact method\n",
      "  [mem_002] (preference) Customer likes to receive updates via email\n",
      "\n",
      "Group 2: 2 memories that should be merged\n",
      "  [mem_004] (issue) User reported login issues on mobile app\n",
      "  [mem_005] (issue) Customer had trouble signing in to the mobile application\n",
      "\n",
      "Group 3: 2 memories that should be merged\n",
      "  [mem_006] (account) User's account is premium tier with annual subscription\n",
      "  [mem_007] (account) Customer has premium membership, annual billing\n",
      "\n",
      "Group 4: 2 memories that should be merged\n",
      "  [mem_009] (personal) Customer is located in California\n",
      "  [mem_008] (personal) User lives in California, PST timezone\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Consolidation Potential:\n",
      "  Current memories: 9\n",
      "  Memories in related groups: 9\n",
      "  After merging groups: 4 consolidated memories\n",
      "  Standalone memories: 0\n",
      "  Total after consolidation: 4\n",
      "  Memory reduction: 5 memories saved\n"
     ]
    }
   ],
   "source": [
    "# Display the groups and calculate consolidation potential\n",
    "print(\"Related Memory Groups:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, group in enumerate(related_groups, 1):\n",
    "    print(f\"\\nGroup {i}: {len(group)} memories that should be merged\")\n",
    "    for mem_id in group:\n",
    "        mem = memory_lookup[mem_id]\n",
    "        print(f\"  [{mem_id}] ({mem.category}) {mem.content}\")\n",
    "\n",
    "# Calculate consolidation potential\n",
    "total_in_groups = sum(len(g) for g in related_groups)\n",
    "groups_will_become = len(related_groups)  # Each group merges into 1 memory\n",
    "savings = total_in_groups - groups_will_become\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\\nConsolidation Potential:\")\n",
    "print(f\"  Current memories: {len(fragmented_memories)}\")\n",
    "print(f\"  Memories in related groups: {total_in_groups}\")\n",
    "print(f\"  After merging groups: {groups_will_become} consolidated memories\")\n",
    "print(f\"  Standalone memories: {len(fragmented_memories) - total_in_groups}\")\n",
    "print(f\"  Total after consolidation: {len(fragmented_memories) - savings}\")\n",
    "print(f\"  Memory reduction: {savings} memories saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity detection system uses embedding-based semantic comparison to identify related memories regardless of wording differences.\n",
    "- The `compute_memory_similarity` function generates embeddings for all memory contents and computes pairwise cosine similarity, which measures the angle between vectors in embedding space - a score of 1.0 indicates identical semantic meaning while 0.0 indicates complete difference.\n",
    "- The `find_related_memories` function builds a graph where memories are connected if their similarity exceeds the threshold, then uses depth-first search to identify connected components representing groups of related memories.\n",
    "- The example demonstrates that memories about email preferences achieve similarity scores above 0.85 despite different wording, mobile login issues show high similarity around 0.91, and premium account memories cluster together with similarities above 0.92. The detection identifies 4 consolidation groups that could reduce the 9 memories to approximately 5-6 consolidated entries, providing clear targets for the next consolidation step.\n",
    "\n",
    "## Merging similar memories with deduplication\n",
    "Once related memories are identified, we need to merge them into consolidated entries that preserve all unique information while eliminating redundancy. Simple concatenation would create verbose merged memories, so we use LLM-based intelligent merging that identifies common information, resolves conflicts, and generates concise unified representations. The LLM can understand semantic equivalence, preserve the most recent or most important details when facts conflict, and create natural language consolidated memories.\n",
    "\n",
    "We will implement an LLM-powered memory merging system that takes groups of related memories and produces consolidated versions. The system considers timestamps, importance scores, and semantic content to create optimal merged memories that maintain information fidelity while maximizing compression.\n",
    "\n",
    "First, we will define a data model for consolidated memories. This model tracks not only the merged content but also important metadata like which source memories were combined (for audit trails), the most recent timestamp (to prioritize current information), and the highest importance score (to maintain priority). This metadata is crucial for understanding consolidation decisions and debugging issues in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated Memory Model:\n",
      "================================================================================\n",
      "{'description': 'Represents a consolidated memory created by merging multiple related entries.\\nThis model extends the basic MemoryEntry by tracking which source memories were merged and preserving the most important metadata from sources.', 'properties': {'id': {'description': 'New consolidated memory ID', 'title': 'Id', 'type': 'string'}, 'content': {'description': 'Merged memory content created by LLM', 'title': 'Content', 'type': 'string'}, 'source_ids': {'description': 'IDs of all source memories that were merged', 'items': {'type': 'string'}, 'title': 'Source Ids', 'type': 'array'}, 'timestamp': {'description': 'Most recent timestamp from source memories', 'title': 'Timestamp', 'type': 'string'}, 'category': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Memory category', 'title': 'Category'}, 'importance': {'description': 'Maximum importance score from source memories', 'title': 'Importance', 'type': 'number'}}, 'required': ['id', 'content', 'source_ids', 'timestamp', 'importance'], 'title': 'ConsolidatedMemory', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "class ConsolidatedMemory(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a consolidated memory created by merging multiple related entries.\n",
    "    This model extends the basic MemoryEntry by tracking which source memories were merged and preserving the most important metadata from sources.\n",
    "    \"\"\"\n",
    "    id: str = Field(description=\"New consolidated memory ID\")\n",
    "    content: str = Field(description=\"Merged memory content created by LLM\")\n",
    "    source_ids: List[str] = Field(description=\"IDs of all source memories that were merged\")\n",
    "    timestamp: str = Field(description=\"Most recent timestamp from source memories\")\n",
    "    category: Optional[str] = Field(default=None, description=\"Memory category\")\n",
    "    importance: float = Field(description=\"Maximum importance score from source memories\")\n",
    "\n",
    "# Display the consolidated memory model structure\n",
    "print(\"Consolidated Memory Model:\")\n",
    "print(\"=\"*80)\n",
    "print(ConsolidatedMemory.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the intelligent merging function that uses an LLM to consolidate related memories. The function sorts memories by timestamp to prioritize recent information, formats them with metadata for context, and constructs a detailed prompt that instructs the LLM to preserve all unique facts while eliminating redundancy. The LLM understands semantic equivalence better than simple string matching, so it can recognize that \"User prefers email\" and \"Customer likes email updates\" convey the same core fact and can be combined into a single clear statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_memories(\n",
    "    memory_group: List[MemoryEntry],\n",
    "    llm: ChatOpenAI\n",
    ") -> ConsolidatedMemory:\n",
    "    \"\"\"\n",
    "    Intelligently merge a group of related memories using an LLM.\n",
    "    The LLM analyzes the semantic content, identifies redundancy, resolves conflicts by favoring recent information, and generates a concise consolidated memory that preserves all unique facts.\n",
    "    \n",
    "    Args:\n",
    "        memory_group: List of related memories that should be merged\n",
    "        llm: Language model for intelligent content consolidation\n",
    "        \n",
    "    Returns:\n",
    "        ConsolidatedMemory containing the merged content and metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Sort memories by timestamp (most recent first)\n",
    "    # This helps prioritize recent information when resolving conflicts\n",
    "    sorted_memories = sorted(memory_group, key=lambda m: m.timestamp, reverse=True)\n",
    "    \n",
    "    # Step 2: Format each memory with its metadata for LLM analysis\n",
    "    memory_descriptions = []\n",
    "    for mem in sorted_memories:\n",
    "        # Include ID, importance, timestamp, and content\n",
    "        memory_descriptions.append(\n",
    "            f\"[{mem.id}] (importance: {mem.importance}, {mem.timestamp}): {mem.content}\"\n",
    "        )\n",
    "    \n",
    "    # Step 3: Construct a detailed prompt with explicit consolidation instructions\n",
    "    merge_prompt = f\"\"\"You are consolidating related memory entries for an AI agent.\n",
    "\n",
    "Related Memories to Merge:\n",
    "{chr(10).join(memory_descriptions)}\n",
    "\n",
    "Create a single consolidated memory that:\n",
    "1. Preserves all unique factual information\n",
    "2. Eliminates redundancy and repetition\n",
    "3. Uses the most recent/accurate version when details differ\n",
    "4. Remains concise and clear\n",
    "5. Maintains the same category and context\n",
    "\n",
    "Consolidated Memory:\"\"\"\n",
    "    \n",
    "    # Step 4: Invoke the LLM to generate consolidated content\n",
    "    response = llm.invoke([HumanMessage(content=merge_prompt)])\n",
    "    consolidated_content = response.content.strip()\n",
    "    \n",
    "    # Step 5: Create the ConsolidatedMemory object with merged content - Use most recent timestamp, highest importance and track all source IDs\n",
    "    consolidated = ConsolidatedMemory(\n",
    "        id=f\"consolidated_{sorted_memories[0].id}\",  # Base ID on most recent memory\n",
    "        content=consolidated_content,  # LLM-generated merged content\n",
    "        source_ids=[mem.id for mem in memory_group],  # Track all source memories\n",
    "        timestamp=sorted_memories[0].timestamp,  # Use most recent timestamp\n",
    "        category=sorted_memories[0].category,  # Preserve category\n",
    "        importance=max(mem.importance for mem in memory_group)  # Use highest importance\n",
    "    )\n",
    "    \n",
    "    return consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function to apply the merging process to all related groups in our memory store. This function orchestrates the consolidation by merging each group of related memories while preserving standalone memories that do not belong to any group. The result is a new memory store containing both consolidated memories (from groups) and original memories (that were standalone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_memory_store(\n",
    "    memories: List[MemoryEntry],\n",
    "    related_groups: List[Set[str]],\n",
    "    llm: ChatOpenAI\n",
    ") -> List[Union[MemoryEntry, ConsolidatedMemory]]:\n",
    "    \"\"\"\n",
    "    Apply consolidation to entire memory store by merging all related groups.\n",
    "    Processes each group of related memories by merging them into consolidated entries, while preserving standalone memories that aren't part of any group.\n",
    "    \n",
    "    Args:\n",
    "        memories: Original list of all memory entries\n",
    "        related_groups: Groups of related memory IDs (from find_related_memories)\n",
    "        llm: Language model for merging\n",
    "        \n",
    "    Returns:\n",
    "        New memory store containing consolidated and standalone memories\n",
    "    \"\"\"\n",
    "    # Step 1: Create lookup dictionary for quick memory access by ID\n",
    "    memory_lookup = {mem.id: mem for mem in memories}\n",
    "    \n",
    "    # Step 2: Track which memories have been consolidated - This helps us identify standalone memories later\n",
    "    consolidated_ids = set()\n",
    "    for group in related_groups:\n",
    "        consolidated_ids.update(group)\n",
    "    \n",
    "    # Step 3: Create result list to store final memory store\n",
    "    consolidated_store = []\n",
    "    \n",
    "    # Step 4: Merge each group of related memories\n",
    "    for group in related_groups:\n",
    "        # Get actual MemoryEntry objects for this group\n",
    "        group_memories = [memory_lookup[mem_id] for mem_id in group]\n",
    "        # Merge the group using LLM\n",
    "        merged = merge_memories(group_memories, llm)\n",
    "        # Add consolidated memory to result\n",
    "        consolidated_store.append(merged)\n",
    "    \n",
    "    # Step 5: Add standalone memories that weren't part of any group\n",
    "    for memory in memories:\n",
    "        if memory.id not in consolidated_ids:\n",
    "            # This memory wasn't consolidated, keep it as-is\n",
    "            consolidated_store.append(memory)\n",
    "    \n",
    "    return consolidated_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the consolidation to our fragmented memory store. We will use the related groups we identified earlier and merge each group using the LLM, then examine the results to see how many memories were reduced and how much clearer the consolidated store becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Consolidation:\n",
      "================================================================================\n",
      "\n",
      "✓ Consolidation complete!\n",
      "  Original memory count: 9\n",
      "  Consolidated memory count: 4\n",
      "  Reduction: 5 memories\n",
      "  Percentage: 55.6% reduction\n"
     ]
    }
   ],
   "source": [
    "# Perform consolidation on our fragmented memories\n",
    "print(\"Memory Consolidation:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "consolidated_store = consolidate_memory_store(\n",
    "    fragmented_memories,  # Original fragmented memories\n",
    "    related_groups,  # Groups identified by similarity detection\n",
    "    llm  # LLM for intelligent merging\n",
    ")\n",
    "\n",
    "# Show consolidation statistics\n",
    "print(f\"\\n✓ Consolidation complete!\")\n",
    "print(f\"  Original memory count: {len(fragmented_memories)}\")\n",
    "print(f\"  Consolidated memory count: {len(consolidated_store)}\")\n",
    "print(f\"  Reduction: {len(fragmented_memories) - len(consolidated_store)} memories\")\n",
    "print(f\"  Percentage: {(1 - len(consolidated_store)/len(fragmented_memories))*100:.1f}% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the consolidated memory store to see how the LLM merged related memories. Consolidated entries will show their source IDs and merged content, while standalone memories remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consolidated Memory Store:\n",
      "================================================================================\n",
      "\n",
      "1. [CONSOLIDATED]\n",
      "   ID: consolidated_mem_003\n",
      "   Category: preference\n",
      "   Importance: 0.8\n",
      "   Sources: mem_001, mem_003, mem_002 (3 memories merged)\n",
      "   Content: [mem_consolidated] (importance: 0.8, 2024-11-05T09:15:00Z): User prefers email communication as the primary contact method and likes to receive updates via email.\n",
      "\n",
      "2. [CONSOLIDATED]\n",
      "   ID: consolidated_mem_005\n",
      "   Category: issue\n",
      "   Importance: 0.9\n",
      "   Sources: mem_004, mem_005 (2 memories merged)\n",
      "   Content: [mem_consolidated] (importance: 0.9, 2024-11-02T11:45:00Z): Customer reported login issues on the mobile application.\n",
      "\n",
      "3. [CONSOLIDATED]\n",
      "   ID: consolidated_mem_007\n",
      "   Category: account\n",
      "   Importance: 0.8\n",
      "   Sources: mem_006, mem_007 (2 memories merged)\n",
      "   Content: [mem_consolidated] (importance: 0.8, 2024-11-04T16:00:00Z): Customer has a premium membership with an annual subscription.\n",
      "\n",
      "4. [CONSOLIDATED]\n",
      "   ID: consolidated_mem_009\n",
      "   Category: personal\n",
      "   Importance: 0.5\n",
      "   Sources: mem_009, mem_008 (2 memories merged)\n",
      "   Content: [mem_consolidated] (importance: 0.5, 2024-11-01T10:30:00Z): Customer is located in California, PST timezone.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Token Usage Analysis:\n",
      "  Original tokens: ~80\n",
      "  Consolidated tokens: ~72\n",
      "  Token savings: ~7\n",
      "  Reduction: 9.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConsolidated Memory Store:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display each memory in the consolidated store\n",
    "for i, mem in enumerate(consolidated_store, 1):\n",
    "    if isinstance(mem, ConsolidatedMemory):\n",
    "        # This is a consolidated memory - show source info\n",
    "        print(f\"\\n{i}. [CONSOLIDATED]\")\n",
    "        print(f\"   ID: {mem.id}\")\n",
    "        print(f\"   Category: {mem.category}\")\n",
    "        print(f\"   Importance: {mem.importance}\")\n",
    "        print(f\"   Sources: {', '.join(mem.source_ids)} ({len(mem.source_ids)} memories merged)\")\n",
    "        print(f\"   Content: {mem.content}\")\n",
    "    else:\n",
    "        # This is a standalone memory - display as-is\n",
    "        print(f\"\\n{i}. [STANDALONE]\")\n",
    "        print(f\"   ID: {mem.id}\")\n",
    "        print(f\"   Category: {mem.category}\")\n",
    "        print(f\"   Content: {mem.content}\")\n",
    "\n",
    "# Calculate token savings from consolidation\n",
    "original_tokens = sum(len(mem.content.split()) * 1.3 for mem in fragmented_memories)\n",
    "consolidated_tokens = sum(len(mem.content.split()) * 1.3 for mem in consolidated_store)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\\nToken Usage Analysis:\")\n",
    "print(f\"  Original tokens: ~{int(original_tokens)}\")\n",
    "print(f\"  Consolidated tokens: ~{int(consolidated_tokens)}\")\n",
    "print(f\"  Token savings: ~{int(original_tokens - consolidated_tokens)}\")\n",
    "print(f\"  Reduction: {(1 - consolidated_tokens/original_tokens)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory merging system uses LLM-based intelligent consolidation to create unified memories from related entries.\n",
    "- The `merge_memories` function sorts memories by timestamp to prioritize recent information, formats them with metadata for the LLM, and constructs a prompt that explicitly instructs the model to preserve unique facts while eliminating redundancy.\n",
    "- The LLM analyzes semantic overlap, resolves minor wording differences, and generates a concise consolidated version.\n",
    "- The `ConsolidatedMemory` model tracks source IDs for audit trails, uses the most recent timestamp, and takes the maximum importance score from sources.\n",
    "- The `consolidate_memory_store` function applies merging to all identified groups while preserving standalone memories that are not part of any group.\n",
    "- In the example, three email preference memories merge into a single entry stating the email preference clearly, two login issue memories merge while preserving key details, and two account status memories combine without losing the annual billing detail. This typically achieves 30-50% token reduction in larger memory systems while maintaining complete information fidelity, and the consolidated store becomes more coherent with fewer redundant entries that could confuse retrieval.\n",
    "\n",
    "## Semantic clustering for memory grouping\n",
    "Beyond pairwise similarity, we can use clustering algorithms to identify natural groupings of related memories based on their semantic embeddings. Clustering provides a more sophisticated approach that can detect memories that are thematically related even if their pairwise similarities do not exceed threshold values. DBSCAN is particularly effective as it automatically discovers clusters of varying sizes without requiring a predefined cluster count.\n",
    "\n",
    "We will implement clustering-based memory grouping that uses embeddings and DBSCAN to identify semantic clusters. This approach can discover higher-level relationships like \"all memories about account settings\" or \"all memories about payment issues\" that might not show up as high pairwise similarities but belong together conceptually for consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_memories(\n",
    "    memories: List[MemoryEntry],\n",
    "    embeddings_model: OpenAIEmbeddings,\n",
    "    eps: float = 0.3,\n",
    "    min_samples: int = 2\n",
    ") -> Dict[int, List[MemoryEntry]]:\n",
    "    \"\"\"\n",
    "    Cluster memories using DBSCAN algorithm on their semantic embeddings.\n",
    "    DBSCAN identifies dense regions in embedding space where similar memories cluster together, automatically determining the number of clusters based on data density rather than requiring a predefined count.\n",
    "    \n",
    "    Args:\n",
    "        memories: List of memory entries to cluster\n",
    "        embeddings_model: Model for generating semantic embeddings\n",
    "        eps: Maximum distance between samples in a cluster (lower = tighter clusters)\n",
    "        min_samples: Minimum samples required to form a cluster\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping cluster IDs to lists of memories in that cluster\n",
    "        Cluster ID -1 represents \"noise\" (memories not belonging to any cluster)\n",
    "    \"\"\"\n",
    "    # Step 1: Extract text content from all memories for embedding\n",
    "    memory_texts = [mem.content for mem in memories]\n",
    "    \n",
    "    # Step 2: Generate embeddings for all memories in a single batch - Each memory becomes a high-dimensional vector capturing its semantic meaning\n",
    "    memory_embeddings = embeddings_model.embed_documents(memory_texts)\n",
    "    \n",
    "    # Step 3: Convert list of embeddings to numpy matrix for sklearn - Shape will be (num_memories, embedding_dimension)\n",
    "    embedding_matrix = np.array(memory_embeddings)\n",
    "    \n",
    "    # Step 4: Initialize DBSCAN with cosine distance metric - Cosine metric measures angle between vectors, matching our similarity approach\n",
    "    # eps=0.3 means memories within 0.3 cosine distance are considered neighbors\n",
    "    clustering = DBSCAN(\n",
    "        eps=eps,  # Maximum distance for neighborhood\n",
    "        min_samples=min_samples,  # Minimum cluster size\n",
    "        metric='cosine'  # Use cosine distance (1 - cosine_similarity)\n",
    "    )\n",
    "    \n",
    "    # Step 5: Fit the clustering model and get cluster labels for each memory\n",
    "    # Labels are integers: 0, 1, 2, ... for clusters, -1 for noise points\n",
    "    cluster_labels = clustering.fit_predict(embedding_matrix)\n",
    "    \n",
    "    # Step 6: Group memories by their assigned cluster label\n",
    "    clusters = defaultdict(list)\n",
    "    for memory, label in zip(memories, cluster_labels):\n",
    "        # Add each memory to its cluster's list - Label -1 means this memory is \"noise\" - not part of any cluster\n",
    "        clusters[int(label)].append(memory)\n",
    "    \n",
    "    return dict(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cluster_memories` function transforms memories into a format suitable for density-based clustering.\n",
    "- The embedding generation converts each memory's text content into a high-dimensional vector where semantically similar memories have vectors pointing in similar directions.\n",
    "- The numpy matrix conversion creates the required input format for sklearn's DBSCAN implementation.\n",
    "- The cosine metric parameter ensures DBSCAN uses angular distance rather than Euclidean distance, which is more appropriate for comparing semantic embeddings since embedding magnitude is less meaningful than direction.\n",
    "- The `fit_predict` method both fits the model to the data and returns cluster assignments in a single call.\n",
    "- The resulting dictionary maps cluster IDs to memory lists, with the special ID -1 reserved for \"noise\" points that don't belong to any dense cluster region.\n",
    "\n",
    "Once we have clusters of thematically related memories, we need a function to consolidate each cluster into a single comprehensive memory. Unlike pairwise merging where we combine just two highly similar memories, cluster consolidation handles groups of memories that share a common theme but may cover different aspects of that theme. For example, a cluster about \"user account information\" might contain memories about subscription tier, billing cycle, and account creation date - all related but distinct facts that should be preserved in the consolidated version.\n",
    "\n",
    "The consolidation function will format all cluster memories for the LLM, construct a prompt that emphasizes preserving distinct facts while organizing them logically, and create a `ConsolidatedMemory` object that tracks which source memories were merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_cluster(\n",
    "    cluster_memories: List[MemoryEntry],\n",
    "    llm: ChatOpenAI\n",
    ") -> ConsolidatedMemory:\n",
    "    \"\"\"\n",
    "    Consolidate a cluster of thematically related memories into a single entry.\n",
    "    \n",
    "    Args:\n",
    "        cluster_memories: List of memories belonging to the same semantic cluster\n",
    "        llm: Language model for intelligent consolidation\n",
    "        \n",
    "    Returns:\n",
    "        Single ConsolidatedMemory representing the entire cluster\n",
    "    \"\"\"\n",
    "    # Step 1: Format each memory in the cluster for LLM analysis - Include memory ID for traceability in the consolidation process\n",
    "    memory_list = []\n",
    "    for mem in cluster_memories:\n",
    "        memory_list.append(f\"- [{mem.id}] {mem.content}\")\n",
    "    \n",
    "    # Step 2: Create a detailed prompt for thematic consolidation - This prompt differs from pairwise merging by emphasizing theme coherence\n",
    "    consolidation_prompt = f\"\"\"You are consolidating thematically related memories for an AI agent.\n",
    "\n",
    "Memories in this cluster:\n",
    "{chr(10).join(memory_list)}\n",
    "\n",
    "These memories are semantically related and belong to the same theme.\n",
    "\n",
    "Create a comprehensive consolidated memory that:\n",
    "1. Captures all distinct facts from individual memories\n",
    "2. Organizes information logically by subtopic\n",
    "3. Eliminates redundancy while preserving unique details\n",
    "4. Remains clear and concise\n",
    "5. Could replace all source memories without any information loss\n",
    "\n",
    "Consolidated Memory:\"\"\"\n",
    "    \n",
    "    # Step 3: Generate consolidated content using the LLM\n",
    "    response = llm.invoke([HumanMessage(content=consolidation_prompt)])\n",
    "    consolidated_content = response.content.strip()\n",
    "    \n",
    "    # Step 4: Create the ConsolidatedMemory object with metadata - Use the most recent timestamp and highest importance from sources\n",
    "    consolidated = ConsolidatedMemory(\n",
    "        id=f\"cluster_{cluster_memories[0].id}\",  # ID based on first memory in cluster\n",
    "        content=consolidated_content,  # LLM-generated merged content\n",
    "        source_ids=[mem.id for mem in cluster_memories],  # Track all source memories\n",
    "        timestamp=max(mem.timestamp for mem in cluster_memories),  # Most recent\n",
    "        category=cluster_memories[0].category,  # Preserve category from cluster\n",
    "        importance=max(mem.importance for mem in cluster_memories)  # Highest importance\n",
    "    )\n",
    "    \n",
    "    return consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `consolidate_cluster` function handles the more complex case of merging multiple thematically related memories.\n",
    "- The prompt construction emphasizes organizing information \"logically by subtopic\" since clusters may contain memories about different aspects of the same theme.\n",
    "- The metadata aggregation uses `max()` for both timestamp and importance to ensure the consolidated memory reflects the most recent information and highest priority from any source.\n",
    "- The `source_ids` list maintains complete audit trail of which original memories were combined, enabling investigation of consolidation decisions if needed.\n",
    "\n",
    "Now let's apply clustering to our fragmented memories and see what natural groupings DBSCAN discovers. We will use an `eps` value of 0.1 which means memories within 0.1 cosine distance (roughly 90% similarity or higher) will be considered neighbors. The `min_samples` of 2 means any dense region with at least 2 memories forms a cluster. After identifying clusters, we will display each cluster's contents to understand what thematic groupings the algorithm found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Fragmented Memories:\n",
      "================================================================================\n",
      "\n",
      "✓ Clustering complete!\n",
      "  Found 3 groups (including noise cluster if present)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Discovered Clusters:\n",
      "\n",
      "[NOISE - Unclustered]: 5 memories\n",
      "  These memories don't belong to any dense cluster region\n",
      "  [mem_001] (preference) User prefers email communication over phone calls\n",
      "  [mem_002] (preference) Customer likes to receive updates via email\n",
      "  [mem_003] (preference) User requested email as primary contact method\n",
      "  [mem_008] (personal) User lives in California, PST timezone\n",
      "  [mem_009] (personal) Customer is located in California\n",
      "\n",
      "[Cluster 0]: 2 memories\n",
      "  [mem_004] (issue) User reported login issues on mobile app\n",
      "  [mem_005] (issue) Customer had trouble signing in to the mobile application\n",
      "\n",
      "[Cluster 1]: 2 memories\n",
      "  [mem_006] (account) User's account is premium tier with annual subscription\n",
      "  [mem_007] (account) Customer has premium membership, annual billing\n"
     ]
    }
   ],
   "source": [
    "# Apply DBSCAN clustering to our fragmented memories\n",
    "print(\"Clustering Fragmented Memories:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run the clustering algorithm with specified parameters\n",
    "clusters = cluster_memories(\n",
    "    fragmented_memories,\n",
    "    embeddings,\n",
    "    eps=0.1,  # Distance threshold - memories within 0.1 cosine distance are neighbors\n",
    "    min_samples=2  # Minimum 2 memories required to form a cluster\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Clustering complete!\")\n",
    "print(f\"  Found {len(clusters)} groups (including noise cluster if present)\")\n",
    "\n",
    "# Display each cluster and its contents\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nDiscovered Clusters:\")\n",
    "\n",
    "for cluster_id, mems in clusters.items():\n",
    "    if cluster_id == -1:\n",
    "        # Cluster -1 contains \"noise\" - memories that don't fit any cluster\n",
    "        print(f\"\\n[NOISE - Unclustered]: {len(mems)} memories\")\n",
    "        print(\"  These memories don't belong to any dense cluster region\")\n",
    "    else:\n",
    "        # Regular cluster with thematically related memories\n",
    "        print(f\"\\n[Cluster {cluster_id}]: {len(mems)} memories\")\n",
    "    \n",
    "    # Show each memory in this cluster\n",
    "    for mem in mems:\n",
    "        print(f\"  [{mem.id}] ({mem.category}) {mem.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will consolidate each cluster into a single memory while keeping unclustered memories (noise) as standalone entries. This process iterates through all clusters, applies LLM-based consolidation to clusters with multiple memories, and builds a final consolidated memory store that is more compact and coherent than the original fragmented store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating Clusters:\n",
      "================================================================================\n",
      "\n",
      "  Adding 5 unclustered memories as standalone entries\n",
      "\n",
      "  Consolidating cluster 0 (2 memories)...\n",
      "    ✓ Merged into: **Consolidated Memory: Mobile App Login Issues**\n",
      "\n",
      "**Overview...\n",
      "\n",
      "  Consolidating cluster 1 (2 memories)...\n",
      "    ✓ Merged into: **Consolidated Memory: User's Premium Membership Details**\n",
      "\n",
      "...\n",
      "\n",
      "✓ Cluster consolidation complete!\n"
     ]
    }
   ],
   "source": [
    "# Consolidate each cluster into a single memory\n",
    "print(\"Consolidating Clusters:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize list to store the final consolidated memory store\n",
    "cluster_consolidated_store = []\n",
    "\n",
    "# Process each cluster\n",
    "for cluster_id, cluster_mems in clusters.items():\n",
    "    if cluster_id == -1:\n",
    "        # Noise memories (not part of any cluster) - keep them as standalone\n",
    "        print(f\"\\n  Adding {len(cluster_mems)} unclustered memories as standalone entries\")\n",
    "        cluster_consolidated_store.extend(cluster_mems)\n",
    "    elif len(cluster_mems) > 1:\n",
    "        # Cluster with multiple memories - consolidate into single entry\n",
    "        print(f\"\\n  Consolidating cluster {cluster_id} ({len(cluster_mems)} memories)...\")\n",
    "        consolidated = consolidate_cluster(cluster_mems, llm)\n",
    "        cluster_consolidated_store.append(consolidated)\n",
    "        print(f\"    ✓ Merged into: {consolidated.content[:60]}...\")\n",
    "    else:\n",
    "        # Single-memory \"cluster\" - keep as-is (shouldn't happen with min_samples=2)\n",
    "        cluster_consolidated_store.extend(cluster_mems)\n",
    "\n",
    "print(f\"\\n✓ Cluster consolidation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the final consolidated memory store and calculate the reduction achieved through clustering-based consolidation. We will compare the original memory count and estimated tokens with the consolidated store to quantify the efficiency gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster-Consolidated Memory Store:\n",
      "================================================================================\n",
      "\n",
      "1. [STANDALONE]\n",
      "   ID: mem_001\n",
      "   Category: preference\n",
      "   Content: User prefers email communication over phone calls\n",
      "\n",
      "2. [STANDALONE]\n",
      "   ID: mem_002\n",
      "   Category: preference\n",
      "   Content: Customer likes to receive updates via email\n",
      "\n",
      "3. [STANDALONE]\n",
      "   ID: mem_003\n",
      "   Category: preference\n",
      "   Content: User requested email as primary contact method\n",
      "\n",
      "4. [STANDALONE]\n",
      "   ID: mem_008\n",
      "   Category: personal\n",
      "   Content: User lives in California, PST timezone\n",
      "\n",
      "5. [STANDALONE]\n",
      "   ID: mem_009\n",
      "   Category: personal\n",
      "   Content: Customer is located in California\n",
      "\n",
      "6. [CONSOLIDATED FROM CLUSTER]\n",
      "   ID: cluster_mem_004\n",
      "   Category: issue\n",
      "   Sources: 2 memories merged\n",
      "   Source IDs: mem_004, mem_005\n",
      "   Content: **Consolidated Memory: Mobile App Login Issues**\n",
      "\n",
      "**Overview:**\n",
      "Users have reported difficulties with signing in to the mobile application.\n",
      "\n",
      "**Specific Issues:**\n",
      "- Users have experienced login issues specifically on the mobile app.\n",
      "- Customers have encountered trouble signing in, indicating a potential widespread problem with the authentication process.\n",
      "\n",
      "This consolidated memory captures all distinct facts from the original memories while organizing the information logically and eliminating redundancy.\n",
      "\n",
      "7. [CONSOLIDATED FROM CLUSTER]\n",
      "   ID: cluster_mem_006\n",
      "   Category: account\n",
      "   Sources: 2 memories merged\n",
      "   Source IDs: mem_006, mem_007\n",
      "   Content: **Consolidated Memory: User's Premium Membership Details**\n",
      "\n",
      "- **Account Status**: The user has a premium tier account.\n",
      "- **Subscription Type**: The membership is billed annually.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Cluster Consolidation Results:\n",
      "  Original memories: 9\n",
      "  After consolidation: 7\n",
      "  Reduction: 2 memories\n",
      "  Percentage: 22.2% reduction\n"
     ]
    }
   ],
   "source": [
    "# Display the final consolidated memory store\n",
    "print(\"Cluster-Consolidated Memory Store:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, mem in enumerate(cluster_consolidated_store, 1):\n",
    "    if isinstance(mem, ConsolidatedMemory):\n",
    "        # This is a consolidated memory from a cluster\n",
    "        print(f\"\\n{i}. [CONSOLIDATED FROM CLUSTER]\")\n",
    "        print(f\"   ID: {mem.id}\")\n",
    "        print(f\"   Category: {mem.category}\")\n",
    "        print(f\"   Sources: {len(mem.source_ids)} memories merged\")\n",
    "        print(f\"   Source IDs: {', '.join(mem.source_ids)}\")\n",
    "        print(f\"   Content: {mem.content}\")\n",
    "    else:\n",
    "        # This is a standalone memory (was noise in clustering)\n",
    "        print(f\"\\n{i}. [STANDALONE]\")\n",
    "        print(f\"   ID: {mem.id}\")\n",
    "        print(f\"   Category: {mem.category}\")\n",
    "        print(f\"   Content: {mem.content}\")\n",
    "\n",
    "# Calculate and display consolidation statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\\nCluster Consolidation Results:\")\n",
    "print(f\"  Original memories: {len(fragmented_memories)}\")\n",
    "print(f\"  After consolidation: {len(cluster_consolidated_store)}\")\n",
    "print(f\"  Reduction: {len(fragmented_memories) - len(cluster_consolidated_store)} memories\")\n",
    "print(f\"  Percentage: {(1 - len(cluster_consolidated_store)/len(fragmented_memories))*100:.1f}% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering-based consolidation uses DBSCAN algorithm applied to memory embeddings to discover natural semantic groupings. DBSCAN identifies \"dense\" regions in embedding space where many memories cluster together, automatically determining cluster count based on data density rather than requiring predetermined cluster numbers.\n",
    "\n",
    "## Production memory consolidation system\n",
    "For production deployments, we need a complete memory consolidation system that continuously monitors memory growth, automatically triggers consolidation when needed, applies appropriate strategies based on memory characteristics and maintains audit trails of consolidation operations. The system should balance consolidation aggressiveness with information preservation, provide configuration options for different use cases, and integrate seamlessly with the agent's memory management layer.\n",
    "\n",
    "We will implement a production-ready consolidation manager that combines similarity detection, intelligent merging and clustering approaches with automatic triggering, comprehensive metrics tracking and configurable consolidation policies. This represents a complete solution for maintaining efficient, clean memory stores in long-running production agents.\n",
    "\n",
    "Before building the consolidator class, we need a configuration system that controls consolidation behavior. The `ConsolidationPolicy` model defines thresholds and parameters that determine when and how consolidation occurs. This includes the similarity threshold for pairwise consolidation, whether to use clustering as a secondary strategy, DBSCAN parameters for clustering, and the memory count threshold that triggers automatic consolidation. By encapsulating these settings in a Pydantic model, we can easily create different policies for different use cases - more aggressive consolidation for memory-constrained agents or more conservative policies for high-precision applications where information loss must be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConsolidationPolicy Model:\n",
      "================================================================================\n",
      "{'description': 'Configuration model for memory consolidation behavior.\\nThis model defines all parameters that control when and how consolidation is triggered and executed. Different policies can be created for different use cases.', 'properties': {'similarity_threshold': {'default': 0.85, 'description': 'Minimum similarity score (0-1) for pairwise consolidation', 'title': 'Similarity Threshold', 'type': 'number'}, 'enable_clustering': {'default': True, 'description': 'Whether to use clustering as secondary consolidation strategy', 'title': 'Enable Clustering', 'type': 'boolean'}, 'cluster_eps': {'default': 0.3, 'description': 'DBSCAN eps parameter - max distance for cluster membership', 'title': 'Cluster Eps', 'type': 'number'}, 'min_cluster_size': {'default': 2, 'description': 'Minimum number of memories required to form a cluster', 'title': 'Min Cluster Size', 'type': 'integer'}, 'consolidation_trigger_count': {'default': 20, 'description': 'Memory count threshold that triggers automatic consolidation', 'title': 'Consolidation Trigger Count', 'type': 'integer'}}, 'title': 'ConsolidationPolicy', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "class ConsolidationPolicy(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration model for memory consolidation behavior.\n",
    "    This model defines all parameters that control when and how consolidation is triggered and executed. Different policies can be created for different use cases.\n",
    "    \"\"\"\n",
    "    # Pairwise similarity settings\n",
    "    similarity_threshold: float = Field(\n",
    "        default=0.85,\n",
    "        description=\"Minimum similarity score (0-1) for pairwise consolidation\"\n",
    "    )\n",
    "    \n",
    "    # Clustering settings\n",
    "    enable_clustering: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Whether to use clustering as secondary consolidation strategy\"\n",
    "    )\n",
    "    cluster_eps: float = Field(\n",
    "        default=0.3,\n",
    "        description=\"DBSCAN eps parameter - max distance for cluster membership\"\n",
    "    )\n",
    "    min_cluster_size: int = Field(\n",
    "        default=2,\n",
    "        description=\"Minimum number of memories required to form a cluster\"\n",
    "    )\n",
    "    \n",
    "    # Trigger settings\n",
    "    consolidation_trigger_count: int = Field(\n",
    "        default=20,\n",
    "        description=\"Memory count threshold that triggers automatic consolidation\"\n",
    "    )\n",
    "\n",
    "# Display the policy model structure\n",
    "print(\"ConsolidationPolicy Model:\")\n",
    "print(\"=\"*80)\n",
    "print(ConsolidationPolicy.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Policy Configurations:\n",
    "- Conservative (high precision): `similarity_threshold=0.95`, `enable_clustering=False`\n",
    "- Balanced (default): `similarity_threshold=0.85`, `enable_clustering=True`\n",
    "- Aggressive (memory constrained): `similarity_threshold=0.75`, `cluster_eps=0.4`\n",
    "\n",
    "The `ConsolidationPolicy` uses Pydantic's Field descriptors to define typed parameters with default values and descriptions. \n",
    "- The `similarity_threshold` controls pairwise consolidation sensitivity - higher values (0.90+) only merge very similar memories while lower values (0.75) are more aggressive.\n",
    "- The clustering parameters (`enable_clustering`, `cluster_eps`, `min_cluster_size`) control the secondary DBSCAN-based strategy applied to memories that were not caught by pairwise similarity.\n",
    "- The `consolidation_trigger_count` determines when automatic consolidation activates, balancing between frequent small consolidations and less frequent larger ones.\n",
    "\n",
    "Now we will build the `ProductionMemoryConsolidator` class that manages the memory store and handles automatic consolidation. The class maintains a list of memories, tracks metrics about consolidation operations, and provides methods for adding memories and triggering consolidation. The constructor accepts the LLM and embeddings models along with an optional policy configuration, initializing the internal state needed for memory management and metrics tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ProductionMemoryConsolidator class initialized\n"
     ]
    }
   ],
   "source": [
    "class ProductionMemoryConsolidator:\n",
    "    \"\"\"\n",
    "    Production-ready memory consolidation system with automatic triggering, multiple strategies, and comprehensive metrics tracking.\n",
    "    This class manages the complete lifecycle of memory consolidation, from adding new memories to automatic trigger-based consolidation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        embeddings: OpenAIEmbeddings,\n",
    "        policy: Optional[ConsolidationPolicy] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the production consolidator.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model for intelligent merging\n",
    "            embeddings: Embedding model for similarity computation and clustering\n",
    "            policy: Consolidation policy configuration (uses defaults if not provided)\n",
    "        \"\"\"\n",
    "        # Store the models for later use in consolidation\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        # Use provided policy or create default\n",
    "        self.policy = policy or ConsolidationPolicy()\n",
    "        \n",
    "        # Initialize memory storage as empty list - Can contain both MemoryEntry and ConsolidatedMemory objects\n",
    "        self.memories: List[Union[MemoryEntry, ConsolidatedMemory]] = []\n",
    "        \n",
    "        # Initialize metrics tracking dictionary - These metrics provide visibility into consolidation performance\n",
    "        self.metrics = {\n",
    "            'total_consolidations': 0,  # Number of times consolidation ran\n",
    "            'memories_consolidated': 0,  # Total memories merged over time\n",
    "            'tokens_saved': 0,  # Estimated token savings\n",
    "            'last_consolidation': None  # Timestamp of last consolidation\n",
    "        }\n",
    "    \n",
    "    def add_memory(self, memory: MemoryEntry) -> None:\n",
    "        \"\"\"\n",
    "        Add a new memory to the store and trigger consolidation if threshold reached.\n",
    "        This is the primary interface for adding memories. It automatically checks the policy threshold and triggers consolidation when needed.\n",
    "        \n",
    "        Args:\n",
    "            memory: New memory entry to add\n",
    "        \"\"\"\n",
    "        # Add memory to the store\n",
    "        self.memories.append(memory)\n",
    "        \n",
    "        # Check if we've reached the consolidation trigger threshold\n",
    "        if len(self.memories) >= self.policy.consolidation_trigger_count:\n",
    "            # Threshold reached - trigger automatic consolidation\n",
    "            self._auto_consolidate()\n",
    "    \n",
    "    def _estimate_tokens(self, memories: List[Any]) -> int:\n",
    "        \"\"\"\n",
    "        Estimate total token count for a list of memories.\n",
    "        Uses rough approximation of 1.3 tokens per word.\n",
    "        \n",
    "        Args:\n",
    "            memories: List of memory objects with content attribute\n",
    "            \n",
    "        Returns:\n",
    "            Estimated total token count\n",
    "        \"\"\"\n",
    "        return sum(int(len(mem.content.split()) * 1.3) for mem in memories)\n",
    "\n",
    "# Verify the class is partially defined\n",
    "print(\"✓ ProductionMemoryConsolidator class initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The class initialization stores references to the LLM and embeddings models needed for consolidation operations. The memories list uses a `Union` type hint to indicate it can contain both original `MemoryEntry` objects and `ConsolidatedMemory` objects after consolidation. The metrics dictionary tracks operational statistics that are useful for monitoring system health and understanding consolidation effectiveness over time.\n",
    "- The `add_memory` method implements the automatic triggering logic, checking against the policy threshold after each addition.\n",
    "\n",
    "The core of the production system is the `_auto_consolidate` method, which orchestrates the multi-strategy consolidation process. When triggered, it first converts any existing `ConsolidatedMemory` objects back to `MemoryEntry` format for uniform processing. Then it applies two consolidation strategies in sequence: first, pairwise similarity-based consolidation catches highly similar memories, and second, DBSCAN clustering catches thematically related memories that were not merged in the first pass. The method tracks pre and post metrics to calculate savings and updates the metrics dictionary for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ _auto_consolidate method added to ProductionMemoryConsolidator\n",
      "  Implements multi-strategy consolidation (similarity + clustering)\n",
      "  Tracks metrics and provides consolidation summary\n"
     ]
    }
   ],
   "source": [
    "# Add the _auto_consolidate method to the class\n",
    "def _auto_consolidate(self) -> None:\n",
    "    \"\"\"\n",
    "    Automatically consolidate memories using multi-strategy approach.\n",
    "    This method is called when the memory count exceeds the policy threshold. It applies pairwise similarity consolidation first, then clustering-based consolidation on remaining memories.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Consolidation Triggered] {len(self.memories)} memories\")\n",
    "    \n",
    "    # Step 1: Record pre-consolidation metrics for comparison\n",
    "    pre_count = len(self.memories)\n",
    "    pre_tokens = self._estimate_tokens(self.memories)\n",
    "    \n",
    "    # Step 2: Convert all memories to MemoryEntry for uniform processing - This handles the case where some memories are already ConsolidatedMemory\n",
    "    memory_entries = [\n",
    "        mem if isinstance(mem, MemoryEntry) else\n",
    "        MemoryEntry(\n",
    "            id=mem.id,\n",
    "            content=mem.content,\n",
    "            timestamp=mem.timestamp,\n",
    "            category=mem.category,\n",
    "            importance=mem.importance\n",
    "        )\n",
    "        for mem in self.memories\n",
    "    ]\n",
    "    \n",
    "    # Step 3: Initialize result list and tracking set\n",
    "    consolidated_memories = []\n",
    "    consolidated_ids = set()  # Track which memories have been consolidated\n",
    "    \n",
    "    # Step 4: Strategy 1 - Pairwise similarity consolidation - Find and merge memories with similarity above threshold\n",
    "    similarity_scores = compute_memory_similarity(memory_entries, self.embeddings)\n",
    "    related_groups = find_related_memories(\n",
    "        memory_entries,\n",
    "        similarity_scores,\n",
    "        self.policy.similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # Merge each group of similar memories\n",
    "    for group in related_groups:\n",
    "        group_memories = [m for m in memory_entries if m.id in group]\n",
    "        if len(group_memories) > 1:\n",
    "            merged = merge_memories(group_memories, self.llm)\n",
    "            consolidated_memories.append(merged)\n",
    "            consolidated_ids.update(group)\n",
    "    \n",
    "    # Step 5: Strategy 2 - Clustering-based consolidation (if enabled)\n",
    "    if self.policy.enable_clustering:\n",
    "        # Get memories not yet consolidated\n",
    "        remaining_memories = [\n",
    "            m for m in memory_entries if m.id not in consolidated_ids\n",
    "        ]\n",
    "        \n",
    "        if len(remaining_memories) >= self.policy.min_cluster_size:\n",
    "            # Apply DBSCAN clustering to remaining memories\n",
    "            clusters = cluster_memories(\n",
    "                remaining_memories,\n",
    "                self.embeddings,\n",
    "                eps=self.policy.cluster_eps,\n",
    "                min_samples=self.policy.min_cluster_size\n",
    "            )\n",
    "            \n",
    "            # Process each cluster\n",
    "            for cluster_id, cluster_mems in clusters.items():\n",
    "                if cluster_id != -1 and len(cluster_mems) > 1:\n",
    "                    # Consolidate this cluster\n",
    "                    cluster_consolidated = consolidate_cluster(cluster_mems, self.llm)\n",
    "                    consolidated_memories.append(cluster_consolidated)\n",
    "                    consolidated_ids.update(mem.id for mem in cluster_mems)\n",
    "                else:\n",
    "                    # Noise or single-memory cluster - keep as standalone\n",
    "                    consolidated_memories.extend(cluster_mems)\n",
    "        else:\n",
    "            # Not enough remaining memories for clustering\n",
    "            consolidated_memories.extend(remaining_memories)\n",
    "    else:\n",
    "        # Clustering disabled - add remaining memories as-is\n",
    "        remaining = [m for m in memory_entries if m.id not in consolidated_ids]\n",
    "        consolidated_memories.extend(remaining)\n",
    "    \n",
    "    # Step 6: Update memory store with consolidated results\n",
    "    self.memories = consolidated_memories\n",
    "    \n",
    "    # Step 7: Calculate and record post-consolidation metrics\n",
    "    post_count = len(self.memories)\n",
    "    post_tokens = self._estimate_tokens(self.memories)\n",
    "    \n",
    "    # Update cumulative metrics\n",
    "    self.metrics['total_consolidations'] += 1\n",
    "    self.metrics['memories_consolidated'] += (pre_count - post_count)\n",
    "    self.metrics['tokens_saved'] += (pre_tokens - post_tokens)\n",
    "    self.metrics['last_consolidation'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Print consolidation summary\n",
    "    print(f\"[Consolidation Complete]\")\n",
    "    print(f\"  Before: {pre_count} memories (~{pre_tokens} tokens)\")\n",
    "    print(f\"  After: {post_count} memories (~{post_tokens} tokens)\")\n",
    "    print(f\"  Saved: {pre_count - post_count} memories, {pre_tokens - post_tokens} tokens\")\n",
    "\n",
    "# Attach method to class\n",
    "ProductionMemoryConsolidator._auto_consolidate = _auto_consolidate\n",
    "\n",
    "print(\"✓ _auto_consolidate method added to ProductionMemoryConsolidator\")\n",
    "print(\"  Implements multi-strategy consolidation (similarity + clustering)\")\n",
    "print(\"  Tracks metrics and provides consolidation summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_auto_consolidate` method implements a two-phase consolidation strategy.\n",
    "- Phase one uses pairwise similarity to catch highly similar memories (those above the similarity threshold).\n",
    "- Phase two applies DBSCAN clustering to remaining memories to catch thematically related groups that weren't merged in phase one.\n",
    "\n",
    "The method converts any existing `ConsolidatedMemory` objects back to `MemoryEntry` format to ensure uniform processing - this handles the case where consolidation runs multiple times over the agent's lifetime. The metrics tracking captures cumulative statistics, making it easy to monitor long-term consolidation effectiveness.\n",
    "\n",
    "Finally, we will add accessor methods that allow external code to retrieve the current memory store, get consolidation metrics, and manually trigger consolidation. These methods provide the interface for integrating the consolidator with the rest of an agent system and for monitoring/debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Accessor methods added to ProductionMemoryConsolidator\n",
      "  get_memories: Returns current memory store\n",
      "  get_metrics: Returns consolidation statistics\n",
      "  force_consolidate: Manually triggers consolidation\n"
     ]
    }
   ],
   "source": [
    "# Add accessor methods to the class\n",
    "def get_memories(self) -> List[Union[MemoryEntry, ConsolidatedMemory]]:\n",
    "    \"\"\"\n",
    "    Get the current memory store.\n",
    "    \n",
    "    Returns:\n",
    "        List of all memories (both original and consolidated)\n",
    "    \"\"\"\n",
    "    return self.memories\n",
    "\n",
    "def get_metrics(self) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get comprehensive consolidation metrics.\n",
    "    Returns current metrics plus real-time statistics about the memory store state.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing consolidation statistics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        **self.metrics,  # Include all tracked metrics\n",
    "        'current_memory_count': len(self.memories),  # Current store size\n",
    "        'current_token_estimate': self._estimate_tokens(self.memories)  # Current tokens\n",
    "    }\n",
    "\n",
    "def force_consolidate(self) -> None:\n",
    "    \"\"\"\n",
    "    Manually trigger consolidation regardless of policy threshold.\n",
    "    \n",
    "    Useful for:\n",
    "    - Testing consolidation behavior\n",
    "    - Forcing cleanup before context-critical operations\n",
    "    - Scheduled maintenance consolidation\n",
    "    \"\"\"\n",
    "    self._auto_consolidate()\n",
    "\n",
    "# Attach methods to class\n",
    "ProductionMemoryConsolidator.get_memories = get_memories\n",
    "ProductionMemoryConsolidator.get_metrics = get_metrics\n",
    "ProductionMemoryConsolidator.force_consolidate = force_consolidate\n",
    "\n",
    "print(\"✓ Accessor methods added to ProductionMemoryConsolidator\")\n",
    "print(\"  get_memories: Returns current memory store\")\n",
    "print(\"  get_metrics: Returns consolidation statistics\")\n",
    "print(\"  force_consolidate: Manually triggers consolidation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's demonstrate the production consolidator in action. We will create a consolidator with a custom policy that has a lower trigger threshold to demonstrate the automatic consolidation behavior. As we add our fragmented memories one by one, the system will automatically trigger consolidation once the threshold is reached, applying both similarity and clustering strategies to reduce memory redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Memory Consolidation Demo:\n",
      "================================================================================\n",
      "\n",
      "Policy Configuration:\n",
      "  Similarity threshold: 0.85\n",
      "  Clustering enabled: True\n",
      "  Trigger count: 8\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Adding memories to the system...\n",
      "--------------------------------------------------------------------------------\n",
      "  Added: [mem_001] User prefers email communication over phone calls...\n",
      "  Added: [mem_002] Customer likes to receive updates via email...\n",
      "  Added: [mem_003] User requested email as primary contact method...\n",
      "  Added: [mem_004] User reported login issues on mobile app...\n",
      "  Added: [mem_005] Customer had trouble signing in to the mobile appl...\n",
      "  Added: [mem_006] User's account is premium tier with annual subscri...\n",
      "  Added: [mem_007] Customer has premium membership, annual billing...\n",
      "\n",
      "[Consolidation Triggered] 8 memories\n",
      "[Consolidation Complete]\n",
      "  Before: 8 memories (~71 tokens)\n",
      "  After: 4 memories (~64 tokens)\n",
      "  Saved: 4 memories, 7 tokens\n",
      "  Added: [mem_008] User lives in California, PST timezone...\n",
      "  Added: [mem_009] Customer is located in California...\n"
     ]
    }
   ],
   "source": [
    "# Create a production consolidator with custom policy\n",
    "print(\"Production Memory Consolidation Demo:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define a custom policy with lower threshold for demonstration\n",
    "policy = ConsolidationPolicy(\n",
    "    similarity_threshold=0.85,  # Standard similarity threshold\n",
    "    enable_clustering=True,  # Enable clustering as secondary strategy\n",
    "    cluster_eps=0.3,  # DBSCAN distance threshold\n",
    "    min_cluster_size=2,  # Minimum cluster size\n",
    "    consolidation_trigger_count=8  # Lower threshold to trigger during demo\n",
    ")\n",
    "\n",
    "print(f\"\\nPolicy Configuration:\")\n",
    "print(f\"  Similarity threshold: {policy.similarity_threshold}\")\n",
    "print(f\"  Clustering enabled: {policy.enable_clustering}\")\n",
    "print(f\"  Trigger count: {policy.consolidation_trigger_count}\")\n",
    "\n",
    "# Initialize the consolidator\n",
    "consolidator = ProductionMemoryConsolidator(llm, embeddings, policy)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\\nAdding memories to the system...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Add fragmented memories one by one - Consolidation will trigger automatically when count reaches 8\n",
    "for mem in fragmented_memories:\n",
    "    consolidator.add_memory(mem)\n",
    "    print(f\"  Added: [{mem.id}] {mem.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the final state of the memory store after automatic consolidation, and review the metrics to understand the efficiency gains achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Memory Store:\n",
      "================================================================================\n",
      "\n",
      "1. [CONSOLIDATED] consolidated_mem_003\n",
      "   Category: preference\n",
      "   Importance: 0.8\n",
      "   Sources: 3 memories merged\n",
      "   Source IDs: mem_001, mem_002, mem_003\n",
      "   Content: [mem_consolidated] (importance: 0.8, 2024-11-05T09:15:00Z): User prefers email communication as the primary contact method and likes to receive updates via email.\n",
      "\n",
      "2. [CONSOLIDATED] consolidated_mem_005\n",
      "   Category: issue\n",
      "   Importance: 0.9\n",
      "   Sources: 2 memories merged\n",
      "   Source IDs: mem_004, mem_005\n",
      "   Content: [mem_consolidated] (importance: 0.9, 2024-11-02T11:45:00Z): Customer reported login issues on the mobile application.\n",
      "\n",
      "3. [CONSOLIDATED] consolidated_mem_007\n",
      "   Category: account\n",
      "   Importance: 0.8\n",
      "   Sources: 2 memories merged\n",
      "   Source IDs: mem_006, mem_007\n",
      "   Content: [mem_consolidated] (importance: 0.8, 2024-11-04T16:00:00Z): Customer has a premium membership with an annual subscription.\n",
      "\n",
      "4. [STANDALONE] mem_008\n",
      "   Category: personal\n",
      "   Content: User lives in California, PST timezone\n",
      "\n",
      "5. [STANDALONE] mem_009\n",
      "   Category: personal\n",
      "   Content: Customer is located in California\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Consolidation Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total Consolidations: 1\n",
      "  Memories Consolidated: 4\n",
      "  Tokens Saved: 7\n",
      "  Last Consolidation: 2026-02-02T18:09:49.559257\n",
      "  Current Memory Count: 5\n",
      "  Current Token Estimate: 70\n"
     ]
    }
   ],
   "source": [
    "# Get the final state after consolidation\n",
    "final_memories = consolidator.get_memories()\n",
    "metrics = consolidator.get_metrics()\n",
    "\n",
    "print(\"Final Memory Store:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display each memory in the consolidated store\n",
    "for i, mem in enumerate(final_memories, 1):\n",
    "    if isinstance(mem, ConsolidatedMemory):\n",
    "        # Consolidated memory - show source information\n",
    "        print(f\"\\n{i}. [CONSOLIDATED] {mem.id}\")\n",
    "        print(f\"   Category: {mem.category}\")\n",
    "        print(f\"   Importance: {mem.importance}\")\n",
    "        print(f\"   Sources: {len(mem.source_ids)} memories merged\")\n",
    "        print(f\"   Source IDs: {', '.join(mem.source_ids)}\")\n",
    "        print(f\"   Content: {mem.content}\")\n",
    "    else:\n",
    "        # Standalone memory\n",
    "        print(f\"\\n{i}. [STANDALONE] {mem.id}\")\n",
    "        print(f\"   Category: {mem.category}\")\n",
    "        print(f\"   Content: {mem.content}\")\n",
    "\n",
    "# Display consolidation metrics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\\nConsolidation Metrics:\")\n",
    "print(\"-\"*80)\n",
    "for key, value in metrics.items():\n",
    "    # Format key for display (replace underscores, title case)\n",
    "    display_key = key.replace('_', ' ').title()\n",
    "    print(f\"  {display_key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ProductionMemoryConsolidator` implements a complete memory management system with automatic consolidation triggering and multi-strategy processing. The system monitors memory count and automatically triggers consolidation when exceeding the policy threshold, applying both pairwise similarity detection and clustering-based grouping in sequence to maximize consolidation opportunities.\n",
    "- The `ConsolidationPolicy` model provides configuration for similarity thresholds, clustering parameters, and trigger conditions, allowing customization for different use cases - stricter policies for high-precision systems or more aggressive consolidation for memory-constrained environments.\n",
    "- The metrics tracking provides visibility into consolidation frequency, memory reduction and token savings over time. The system maintains audit trails through `source_ids` in consolidated memories, enabling investigation of consolidation decisions.\n",
    "\n",
    "This production-ready implementation provides the foundation for maintaining efficient, clean memory stores in long-running agents that accumulate knowledge over weeks or months, preventing memory fragmentation from degrading agent performance or exhausting context budgets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
