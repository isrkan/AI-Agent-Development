{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-First Prompt Layout\n",
    "\n",
    "When building AI agents and designing prompts, one simple principle dramatically improves model focus and response quality: **put the task first, then add context.**\n",
    "\n",
    "Traditional prompts often bury the actual question under layers of background information, forcing models to process extensive details before understanding what they are being asked to do. This leads to responses that miss the core task or focus on peripheral details.\n",
    "\n",
    "Task-first prompt layout inverts this structure: state the objective immediately, then provide supporting information. This ensures the model knows what it needs to accomplish before processing how to accomplish it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from typing import List, Optional\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\").strip(), temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem: Details-first vs task-first\n",
    "Let's see the difference empirically. We will create two versions of the same prompt - one that buries the task under context (traditional), and one that states the task first (better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. DETAILS-FIRST PROMPT:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Context: You are analyzing financial data for a technology company operating in cloud \n",
      "services, enterprise software, and hardware segments.\n",
      "\n",
      "Background: Q4 is typically the strongest quarter. R&D ex...\n",
      "[Task appears after 400+ characters]\n",
      "\n",
      "Response preview: To analyze the financial data for the technology company and identify the top three factors that significantly impacted revenue growth in fiscal year 2023, we can consider the following metrics and insights:\n",
      "\n",
      "### 1. **Revenue Breakdown by Segment**\n",
      "   - **Cloud Services**: This segment likely experi...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2. TASK-FIRST PROMPT:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "TASK: Identify the top 3 factors that most significantly impacted revenue growth in 2023.\n",
      "\n",
      "Requirements:\n",
      "- Only use fiscal year 2023 data  \n",
      "- Express...\n",
      "[Task appears immediately]\n",
      "\n",
      "Response preview: To identify the top 3 factors that most significantly impacted revenue growth for the technology company in fiscal year 2023, we will analyze the data from the cloud, enterprise software, and hardware segments, along with the impact of R&D expenses. Here are the findings based on the fiscal year 202...\n"
     ]
    }
   ],
   "source": [
    "# DETAILS-FIRST (traditional - task buried at the end)\n",
    "details_first = \"\"\"\n",
    "Context: You are analyzing financial data for a technology company operating in cloud \n",
    "services, enterprise software, and hardware segments.\n",
    "\n",
    "Background: Q4 is typically the strongest quarter. R&D expenses increased 25% year-over-year.\n",
    "\n",
    "Constraints:\n",
    "- Only use fiscal year 2023 data\n",
    "- Express values in millions USD\n",
    "- Include year-over-year comparisons\n",
    "- No forward-looking predictions\n",
    "\n",
    "Metrics to consider: revenue breakdown, margins, R&D spending, customer acquisition costs\n",
    "\n",
    "Data notes: All figures from audited statements. Some segment data was restated in Q1 2024.\n",
    "\n",
    "Task: Identify the top 3 factors that most significantly impacted revenue growth in 2023.\n",
    "\"\"\"\n",
    "\n",
    "# TASK-FIRST (better - task stated immediately)\n",
    "task_first = \"\"\"\n",
    "TASK: Identify the top 3 factors that most significantly impacted revenue growth in 2023.\n",
    "\n",
    "Requirements:\n",
    "- Only use fiscal year 2023 data  \n",
    "- Express values in millions USD\n",
    "- Include year-over-year comparisons\n",
    "\n",
    "Context: Technology company with cloud, enterprise software, and hardware segments. \n",
    "Q4 typically strongest. R&D expenses increased 25% YoY. All data from audited statements.\n",
    "\"\"\"\n",
    "\n",
    "# Test both\n",
    "print(\"1. DETAILS-FIRST PROMPT:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{details_first[:200]}...\\n[Task appears after 400+ characters]\")\n",
    "\n",
    "response1 = llm.invoke([HumanMessage(content=details_first)])\n",
    "print(f\"\\nResponse preview: {response1.content[:300]}...\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n2. TASK-FIRST PROMPT:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{task_first[:150]}...\\n[Task appears immediately]\")\n",
    "\n",
    "response2 = llm.invoke([HumanMessage(content=task_first)])\n",
    "print(f\"\\nResponse preview: {response2.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-first prompts produce better task adherence. The model focuses on answering the question vs discussing context.\n",
    "\n",
    "**What task-first prompt layout really is:**\n",
    "- Put our task/question/instruction at the beginning of our prompt.\n",
    "- Add supporting details (context, data, examples) after the task.\n",
    "\n",
    "**The basic pattern:**\n",
    "```\n",
    "Task: [What you want the model to do]\n",
    "\n",
    "Requirements: [Any constraints or specifications]\n",
    "\n",
    "Context: [Background information needed to complete the task]\n",
    "```\n",
    "\n",
    "Models process information sequentially and knowing the objective first helps the model interpret context correctly. It is especially important for complex prompts with lots of context. Implementation in:\n",
    "- RAG systems: User question → Retrieved documents.\n",
    "- Chat systems: Task in system/user message → Context follows.\n",
    "\n",
    "## Chat-based systems (system + user messages)\n",
    "For chat models, the pattern follows the same principle: instructions/task in system message or early in user message, context follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat-Based Task-First Pattern:\n",
      "================================================================================\n",
      "SYSTEM: You are a helpful assistant. Follow user instructions carefully.\n",
      "\n",
      "USER: Task: Summarize the key findings from the quarterly report\n",
      "\n",
      "Context: Q3 revenue $42M, down 15% QoQ. Churn up to 12%. Competitor pricing pressure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method: Chat-based pattern\n",
    "def chat_task_first(task: str, context: str) -> list:\n",
    "    \"\"\"\n",
    "    Chat models: task/instructions in system message or start of user message.\n",
    "    Context comes after or in separate message.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Follow user instructions carefully.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Task: {task}\\n\\nContext: {context}\"\n",
    "        }\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "# Example\n",
    "task = \"Summarize the key findings from the quarterly report\"\n",
    "context = \"Q3 revenue $42M, down 15% QoQ. Churn up to 12%. Competitor pricing pressure.\"\n",
    "\n",
    "messages = chat_task_first(task, context)\n",
    "\n",
    "print(\"Chat-Based Task-First Pattern:\")\n",
    "print(\"=\" * 80)\n",
    "for msg in messages:\n",
    "    print(f\"{msg['role'].upper()}: {msg['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How RAG systems use task-first prompts\n",
    "In RAG systems, the pattern is the same: user question first (that is the task), then retrieved context second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Task-First Prompt:\n",
      "================================================================================\n",
      "Question: What caused the revenue decline in Q3?\n",
      "\n",
      "Relevant information:\n",
      "1. Q3 2023 revenue: $42M (down from $49.5M in Q2, a 15% decline)\n",
      "2. Customer churn increased from 5% to 8% in Q3 2023\n",
      "3. Competitor launched similar product at 30% lower price in July 2023\n",
      "\n",
      "Answer based on the provided information.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Method: RAG pattern\n",
    "def rag_prompt(user_question: str, retrieved_docs: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Standard RAG pattern: question first, documents second.\n",
    "    This is what production RAG systems actually look like.\n",
    "    \"\"\"\n",
    "    # Task/question goes first\n",
    "    prompt = f\"Question: {user_question}\\n\\n\"\n",
    "    \n",
    "    # Retrieved context goes second\n",
    "    prompt += \"Relevant information:\\n\"\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        prompt += f\"{i}. {doc}\\n\"\n",
    "    \n",
    "    prompt += \"\\nAnswer based on the provided information.\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Simulate RAG\n",
    "user_question = \"What caused the revenue decline in Q3?\"\n",
    "retrieved_docs = [\n",
    "    \"Q3 2023 revenue: $42M (down from $49.5M in Q2, a 15% decline)\",\n",
    "    \"Customer churn increased from 5% to 8% in Q3 2023\",\n",
    "    \"Competitor launched similar product at 30% lower price in July 2023\"\n",
    "]\n",
    "\n",
    "rag_result = rag_prompt(user_question, retrieved_docs)\n",
    "print(\"RAG Task-First Prompt:\")\n",
    "print(\"=\" * 80)\n",
    "print(rag_result)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
